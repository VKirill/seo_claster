"""–≠—Ç–∞–ø 4: SERP –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ xmlstock"""

import os
from seo_analyzer.analysis.serp_analyzer import SERPAnalyzer
from seo_analyzer.core.config import SERP_CONFIG
from .stage_logger import get_group_prefix, print_stage



def get_api_key(args):
    """
    –ü–æ–ª—É—á–∞–µ—Ç API –∫–ª—é—á –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    
    Args:
        args: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
        
    Returns:
        API –∫–ª—é—á –∏–ª–∏ None
    """
    api_key = args.xmlstock_api_key
    
    # –ï—Å–ª–∏ –Ω–µ—Ç –≤ –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ö, –ø—Ä–æ–±—É–µ–º config_local.py
    if not api_key:
        try:
            import config_local
            api_key = getattr(config_local, 'XMLSTOCK_API_KEY', None)
            if api_key:
                print("‚úì API –∫–ª—é—á –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ config_local.py")
        except ImportError:
            pass
    
    # –ï—Å–ª–∏ –Ω–µ—Ç, –ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
    if not api_key:
        api_key = os.getenv('XMLSTOCK_API_KEY')
        if api_key:
            print("‚úì API –∫–ª—é—á –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è")
    
    return api_key


async def analyze_serp_stage(args, analyzer):
    """
    SERP –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ xmlstock
    
    Args:
        args: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
        analyzer: –≠–∫–∑–µ–º–ø–ª—è—Ä SEOAnalyzer
        
    Returns:
        None (–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ analyzer)
    """
    prefix = get_group_prefix(analyzer)
    print_stage(analyzer, "üîç –≠–¢–ê–ü 3: SERP –∞–Ω–∞–ª–∏–∑ (–±–∞–∑–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)")
    print_stage(analyzer, "-" * 80)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ DataFrame –Ω–µ –ø—É—Å—Ç–æ–π
    if len(analyzer.df) == 0:
        print_stage(analyzer, "‚ö†Ô∏è  DataFrame –ø—É—Å—Ç–æ–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º SERP –∞–Ω–∞–ª–∏–∑")
        print()
        return
    
    # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á
    api_key = get_api_key(args)
    
    if not api_key:
        print_stage(analyzer, "‚ö†Ô∏è  API –∫–ª—é—á xmlstock –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print_stage(analyzer, "   –°–ø–æ—Å–æ–± 1: --xmlstock-api-key user:key")
        print_stage(analyzer, "   –°–ø–æ—Å–æ–± 2: —Å–æ–∑–¥–∞–π—Ç–µ config_local.py (—Å–º. config_local.py.example)")
        print_stage(analyzer, "   –°–ø–æ—Å–æ–± 3: export XMLSTOCK_API_KEY=user:key")
        return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    print_stage(analyzer, f"üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SERP –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞...")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä—É–ø–ø—É –∑–∞–ø—Ä–æ—Å–æ–≤
    query_group = None
    if hasattr(analyzer, 'current_group') and analyzer.current_group:
        query_group = analyzer.current_group.name
        print_stage(analyzer, f"üìÅ –ì—Ä—É–ø–ø–∞ –∑–∞–ø—Ä–æ—Å–æ–≤: {query_group}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ batch_async —Ä–µ–∂–∏–º–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
    use_batch_async = getattr(args, 'serp_batch_async', True)
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã SERP –∏–∑ args
    serp_region = getattr(args, 'serp_region', SERP_CONFIG['api']['lr'])
    serp_device = getattr(args, 'serp_device', 'desktop')
    serp_site = getattr(args, 'serp_site', None)
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–∫—Å–∏ –∏–∑ args –∏–ª–∏ config_local.py
    serp_proxies = getattr(args, 'serp_proxies', None)
    serp_proxy_file = getattr(args, 'serp_proxy_file', None)
    
    # –ï—Å–ª–∏ –ø—Ä–æ–∫—Å–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã –≤ args, –ø—Ä–æ–±—É–µ–º config_local.py
    if not serp_proxies and not serp_proxy_file:
        try:
            import config_local
            serp_proxies = getattr(config_local, 'SERP_PROXIES', None)
            serp_proxy_file = getattr(config_local, 'SERP_PROXY_FILE', None)
            if serp_proxies or serp_proxy_file:
                print("‚úì –ü—Ä–æ–∫—Å–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ config_local.py")
        except ImportError:
            pass
    
    # –ï—Å–ª–∏ –ø—Ä–æ–∫—Å–∏ –≤—Å–µ –µ—â–µ –Ω–µ —É–∫–∞–∑–∞–Ω—ã, –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ socks_working.txt –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    if not serp_proxies and not serp_proxy_file:
        from pathlib import Path
        default_proxy_file = Path('socks_working.txt')
        if default_proxy_file.exists():
            serp_proxy_file = str(default_proxy_file)
            print(f"‚úì –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª –ø—Ä–æ–∫—Å–∏: {serp_proxy_file}")
    
    # –ï—Å–ª–∏ –ø—Ä–æ–∫—Å–∏ —É–∫–∞–∑–∞–Ω—ã –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é), –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫
    if isinstance(serp_proxies, str):
        serp_proxies = [p.strip() for p in serp_proxies.split(',') if p.strip()]
    
    analyzer.serp_analyzer = SERPAnalyzer(
        api_key=api_key,
        lr=serp_region,
        max_retries=SERP_CONFIG['api']['max_retries'],
        retry_delay=SERP_CONFIG['api']['retry_delay'],
        timeout=SERP_CONFIG['api']['timeout'],
        query_group=query_group,
        max_concurrent=SERP_CONFIG['api']['max_concurrent'],  # –ì–ª–æ–±–∞–ª—å–Ω—ã–π –ª–∏–º–∏—Ç –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø
        use_master_db=True,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ Master DB
        use_batch_async=use_batch_async,  # üöÄ –ú–ê–°–°–û–í–´–ô ASYNC –†–ï–ñ–ò–ú (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        device=serp_device,
        site=serp_site,
        proxies=serp_proxies,
        proxy_file=serp_proxy_file
    )
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–∫—Å–∏
    if serp_proxies or serp_proxy_file:
        proxy_count = len(serp_proxies) if serp_proxies else 0
        if serp_proxy_file:
            try:
                from pathlib import Path
                proxy_path = Path(serp_proxy_file)
                if proxy_path.exists():
                    with open(proxy_path, 'r', encoding='utf-8') as f:
                        file_count = len([line.strip() for line in f if line.strip() and not line.strip().startswith('#')])
                    proxy_count += file_count
            except:
                pass
        print_stage(analyzer, f"üåê –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {proxy_count} –ø—Ä–æ–∫—Å–∏ –¥–ª—è —Ä–æ—Ç–∞—Ü–∏–∏ IP")
    
    if use_batch_async:
        print_stage(analyzer, "üöÄ –†–µ–∂–∏–º: BATCH ASYNC (–º–∞—Å—Å–æ–≤–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ ‚Üí –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ)")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
    all_queries = analyzer.df['keyword'].tolist()
    
    # –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ serp_top_urls
    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –ë–î, –Ω–æ serp_top_urls –ø—É—Å—Ç—ã–µ - –Ω—É–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å –∑–∞–Ω–æ–≤–æ
    queries_without_urls = []
    if 'serp_top_urls' in analyzer.df.columns:
        for idx, row in analyzer.df.iterrows():
            keyword = row.get('keyword')
            serp_top_urls = row.get('serp_top_urls')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ serp_top_urls –Ω–µ –ø—É—Å—Ç–æ–π
            has_urls = False
            if serp_top_urls is not None:
                if isinstance(serp_top_urls, list):
                    has_urls = len(serp_top_urls) > 0
                elif isinstance(serp_top_urls, str):
                    serp_top_urls_str = serp_top_urls.strip()
                    if serp_top_urls_str and serp_top_urls_str not in ('', '[]', 'null', 'NULL', 'None'):
                        try:
                            import json
                            parsed = json.loads(serp_top_urls_str)
                            has_urls = isinstance(parsed, list) and len(parsed) > 0
                        except:
                            has_urls = False
            
            if not has_urls and keyword:
                queries_without_urls.append(keyword)
        
        if queries_without_urls:
            print_stage(analyzer, f"‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(queries_without_urls)} –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø—É—Å—Ç—ã–º serp_top_urls")
            print_stage(analyzer, f"   –ë—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ XMLStock API...")
    
    print_stage(analyzer, f"üìä –ê–Ω–∞–ª–∏–∑ SERP –¥–ª—è {len(all_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤ (–∫—ç—à –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)...")
    
    # –ü—Ä–æ–≥—Ä–µ—Å—Å
    def progress_callback(current, total, query, status=None):
        if current % 100 == 0 or current == total:
            status_text = f" {status}" if status else ""
            print_stage(analyzer, f"  [{current}/{total}]{status_text} {query[:60]}...")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞–∫–µ—Ç–æ–º (–∫—ç—à –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ - –º–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö)
    # –í–ê–ñ–ù–û: –ï—Å–ª–∏ –µ—Å—Ç—å –∑–∞–ø—Ä–æ—Å—ã –±–µ–∑ URL, –æ–Ω–∏ –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —á–µ—Ä–µ–∑ API
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å query_to_group_map (–æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –≥—Ä—É–ø–ø), –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
    query_to_group_map = getattr(analyzer, 'query_to_group_map', None)
    
    serp_results = await analyzer.serp_analyzer.analyze_queries_batch(
        all_queries,
        max_concurrent=SERP_CONFIG['api']['max_concurrent'],
        progress_callback=progress_callback,
        query_to_group_map=query_to_group_map
    )
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DataFrame
    serp_dict = {result['query']: result for result in serp_results}
    
    # SERP –º–µ—Ç—Ä–∏–∫–∏
    analyzer.df['serp_docs_count'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('metrics', {}).get('found_docs', 0)
    )
    # –î–æ–±–∞–≤–ª—è–µ–º serp_found_docs –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ë–î
    analyzer.df['serp_found_docs'] = analyzer.df['serp_docs_count']
    
    analyzer.df['serp_main_pages'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('metrics', {}).get('main_pages_count', 0)
    )
    # –ö–æ–ª-–≤–æ –≥–ª–∞–≤–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–¥–ª—è Excel)
    analyzer.df['serp_main_pages_count'] = analyzer.df['serp_main_pages']
    # –ö–æ–ª-–≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–æ–±—â–µ–µ –∫–æ–ª-–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –º–∏–Ω—É—Å –≥–ª–∞–≤–Ω—ã–µ)
    analyzer.df['serp_internal_pages_count'] = (
        analyzer.df['serp_docs_count'] - analyzer.df['serp_main_pages']
    ).clip(lower=0)  # –ù–µ –¥–æ–ø—É—Å–∫–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    
    analyzer.df['serp_titles_count'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('metrics', {}).get('titles_with_keyword', 0)
    )
    # –î–æ–±–∞–≤–ª—è–µ–º serp_titles_with_keyword –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ë–î
    analyzer.df['serp_titles_with_keyword'] = analyzer.df['serp_titles_count']
    
    analyzer.df['serp_commercial_domains'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('metrics', {}).get('commercial_domains', 0)
    )
    analyzer.df['serp_info_domains'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('metrics', {}).get('info_domains', 0)
    )
    
    # LSI —Ñ—Ä–∞–∑—ã
    analyzer.df['lsi_phrases'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('lsi_phrases', [])
    )
    
    # –í–ê–ñ–ù–û: –û–±–Ω–æ–≤–ª—è–µ–º serp_top_urls –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ (–≤–∫–ª—é—á–∞—è –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
    # –≠—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ–∂–µ –ø–æ–ø–∞–ª–∏ –≤ DataFrame
    def update_serp_top_urls(query):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç serp_top_urls –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ SERP –∞–Ω–∞–ª–∏–∑–∞"""
        result = serp_dict.get(query, {})
        documents = result.get('documents', [])
        
        if not documents:
            return None  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º None —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        
        # –ï—Å–ª–∏ documents - —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π (title, snippet, url)
        if isinstance(documents, list) and len(documents) > 0:
            if isinstance(documents[0], dict):
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º TOP-20 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
                return documents[:20]
            elif isinstance(documents[0], str):
                # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ URL —Å—Ç—Ä–æ–∫ - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç —Å dict
                return [{'url': url} for url in documents[:20]]
        
        return None
    
    # –û–±–Ω–æ–≤–ª—è–µ–º serp_top_urls –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–æ–Ω–∫—É –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç, –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é
    updated_top_urls = analyzer.df['keyword'].map(update_serp_top_urls)
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –∑–∞–ø–∏—Å–∏, –≥–¥–µ –µ—Å—Ç—å –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    if 'serp_top_urls' not in analyzer.df.columns:
        analyzer.df['serp_top_urls'] = updated_top_urls
    else:
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, –≥–¥–µ –µ—Å—Ç—å –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (–Ω–µ None)
        mask = updated_top_urls.notna()
        analyzer.df.loc[mask, 'serp_top_urls'] = updated_top_urls[mask]
    
    # –î–æ–º–µ–Ω—ã –∏–∑ SERP (–¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏) - –¢–û–ü-20
    # –°–æ–∑–¥–∞–µ–º serp_urls –∏–∑ serp_top_urls (–∫–æ—Ç–æ—Ä—ã–π —Ç–µ–ø–µ—Ä—å –æ–±–Ω–æ–≤–ª–µ–Ω –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞)
    def extract_urls_from_top_urls(serp_top_urls):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ serp_top_urls –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        if not serp_top_urls or not isinstance(serp_top_urls, list):
            return []
        
        urls = []
        for item in serp_top_urls[:20]:  # TOP-20
            if isinstance(item, dict):
                url = item.get('url', '')
            elif isinstance(item, str):
                url = item
            else:
                continue
            
            if url:
                urls.append(url)
        
        return urls
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π serp_top_urls –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è serp_urls
    if 'serp_top_urls' in analyzer.df.columns:
        analyzer.df['serp_urls'] = analyzer.df['serp_top_urls'].apply(extract_urls_from_top_urls)
        print_stage(analyzer, "   ‚úì serp_urls —Å–æ–∑–¥–∞–Ω—ã –∏–∑ serp_top_urls (–æ–±–Ω–æ–≤–ª–µ–Ω–æ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞)")
    else:
        # –ï—Å–ª–∏ serp_top_urls –Ω–µ—Ç, –∏–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é –∏–∑ serp_dict
        def extract_domains(query):
            result = serp_dict.get(query, {})
            documents = result.get('documents', [])
            if not documents:
                return []
            
            # –ï—Å–ª–∏ documents - —Å—Ç—Ä–æ–∫–∞ (JSON –∏–∑ –∫—ç—à–∞), –ø–∞—Ä—Å–∏–º –µ—ë
            if isinstance(documents, str):
                import json
                try:
                    documents = json.loads(documents)
                except:
                    return []
            
            # –ï—Å–ª–∏ documents - —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
            if isinstance(documents, list) and len(documents) > 0:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
                if isinstance(documents[0], dict):
                    # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ TOP-20 URL –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–¥–∞–∂–µ –µ—Å–ª–∏ –∏—Ö –±–æ–ª—å—à–µ)
                    return [doc.get('url', '') for doc in documents[:20] if doc.get('url')]
                elif isinstance(documents[0], str):
                    # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ —Å–ø–∏—Å–æ–∫ URL
                    return documents[:20]
            
            return []
        
        analyzer.df['serp_urls'] = analyzer.df['keyword'].map(extract_domains)
    
    # –ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ SERP (–¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ —Å title, snippet –∏ URL)
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π serp_top_urls (–∫–æ—Ç–æ—Ä—ã–π —Ç–µ–ø–µ—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞ –∏ API)
    if 'serp_top_urls' in analyzer.df.columns:
        # –î–∞–Ω–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ (–≤–∫–ª—é—á–∞—è –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
        analyzer.df['serp_documents'] = analyzer.df['serp_top_urls']
        print_stage(analyzer, "   ‚úì serp_documents –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ (–≤–∫–ª—é—á–∞—è –∫—ç—à)")
    else:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö API
        def extract_documents(query):
            result = serp_dict.get(query, {})
            documents = result.get('documents', [])
            if not documents:
                return []
            
            # –ï—Å–ª–∏ documents - —Å—Ç—Ä–æ–∫–∞ (JSON –∏–∑ –∫—ç—à–∞), –ø–∞—Ä—Å–∏–º –µ—ë
            if isinstance(documents, str):
                import json
                try:
                    documents = json.loads(documents)
                except:
                    return []
            
            # –ï—Å–ª–∏ documents - —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
            if isinstance(documents, list) and len(documents) > 0:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
                if isinstance(documents[0], dict):
                    # –ë–µ—Ä—ë–º TOP-20 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
                    return documents[:20]
            
            return []
        
        analyzer.df['serp_documents'] = analyzer.df['keyword'].map(extract_documents)
        print_stage(analyzer, "   ‚úì serp_documents –∏–∑–≤–ª–µ—á–µ–Ω—ã –∏–∑ SERP API (—Å title –∏ snippet)")
    
    # XML –æ—Ç–≤–µ—Ç—ã (–¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ offer_info)
    analyzer.df['xml_response'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('xml_response', '')
    )
    
    # SERP —Å—Ç–∞—Ç—É—Å –∏ req_id (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö)
    # –≠—Ç–∏ –ø–æ–ª—è –Ω—É–∂–Ω—ã –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –ë–î
    analyzer.df['serp_status'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('status', 'completed')
    )
    analyzer.df['serp_req_id'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('req_id')
    )
    
    # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    urls_filled = (analyzer.df['serp_urls'].apply(lambda x: isinstance(x, list) and len(x) > 0)).sum()
    urls_empty = (analyzer.df['serp_urls'].apply(lambda x: not isinstance(x, list) or len(x) == 0)).sum()
    
    if len(analyzer.df) > 0:
        print_stage(analyzer, f"  SERP URLs –∑–∞–ø–æ–ª–Ω–µ–Ω–æ: {urls_filled} –∑–∞–ø—Ä–æ—Å–æ–≤ ({urls_filled/len(analyzer.df)*100:.1f}%)")
        print_stage(analyzer, f"  SERP URLs –ø—É—Å—Ç–æ: {urls_empty} –∑–∞–ø—Ä–æ—Å–æ–≤ ({urls_empty/len(analyzer.df)*100:.1f}%)")
        
        # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –∏–º–µ—é—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–π serp_top_urls –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ—Ö–æ–¥–æ–º –∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        if 'serp_top_urls' in analyzer.df.columns:
            queries_without_top_urls = []
            for idx, row in analyzer.df.iterrows():
                keyword = row.get('keyword')
                serp_top_urls = row.get('serp_top_urls')
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ serp_top_urls –Ω–µ –ø—É—Å—Ç–æ–π
                has_urls = False
                if serp_top_urls is not None:
                    if isinstance(serp_top_urls, list):
                        has_urls = len(serp_top_urls) > 0
                    elif isinstance(serp_top_urls, str):
                        serp_top_urls_str = serp_top_urls.strip()
                        if serp_top_urls_str and serp_top_urls_str not in ('', '[]', 'null', 'NULL', 'None'):
                            try:
                                import json
                                parsed = json.loads(serp_top_urls_str)
                                has_urls = isinstance(parsed, list) and len(parsed) > 0
                            except:
                                has_urls = False
                
                if not has_urls and keyword:
                    queries_without_top_urls.append(keyword)
            
            if queries_without_top_urls:
                print_stage(analyzer, "")
                print_stage(analyzer, f"‚ö†Ô∏è  –ö–†–ò–¢–ò–ß–ù–û: –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(queries_without_top_urls)} –∑–∞–ø—Ä–æ—Å–æ–≤ –ë–ï–ó serp_top_urls!")
                print_stage(analyzer, f"   –°–∏—Å—Ç–µ–º–∞ –ù–ï –ø–µ—Ä–µ–π–¥–µ—Ç –∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ–∫–∞ –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–µ –±—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã.")
                print_stage(analyzer, f"   –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ—Ä–µ–∑ API...")
                
                # –ü–æ–≤—Ç–æ—Ä–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –∑–∞–ø—Ä–æ—Å—ã, —É –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç serp_top_urls
                retry_results = await analyzer.serp_analyzer.analyze_queries_batch(
                    queries_without_top_urls,
                    max_concurrent=SERP_CONFIG['api']['max_concurrent'],
                    progress_callback=progress_callback
                )
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                retry_dict = {result['query']: result for result in retry_results}
                for query, result in retry_dict.items():
                    if query in serp_dict:
                        serp_dict[query] = result
                
                # –û–±–Ω–æ–≤–ª—è–µ–º serp_top_urls –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                updated_top_urls_retry = analyzer.df['keyword'].map(update_serp_top_urls)
                mask_retry = updated_top_urls_retry.notna()
                analyzer.df.loc[mask_retry, 'serp_top_urls'] = updated_top_urls_retry[mask_retry]
                
                # –û–±–Ω–æ–≤–ª—è–µ–º serp_urls
                analyzer.df['serp_urls'] = analyzer.df['serp_top_urls'].apply(extract_urls_from_top_urls)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
                final_urls_filled = (analyzer.df['serp_urls'].apply(lambda x: isinstance(x, list) and len(x) > 0)).sum()
                final_urls_empty = (analyzer.df['serp_urls'].apply(lambda x: not isinstance(x, list) or len(x) == 0)).sum()
                
                print_stage(analyzer, f"   –ü–æ—Å–ª–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏:")
                print_stage(analyzer, f"   ‚úì SERP URLs –∑–∞–ø–æ–ª–Ω–µ–Ω–æ: {final_urls_filled} –∑–∞–ø—Ä–æ—Å–æ–≤ ({final_urls_filled/len(analyzer.df)*100:.1f}%)")
                print_stage(analyzer, f"   ‚ö†Ô∏è  SERP URLs –ø—É—Å—Ç–æ: {final_urls_empty} –∑–∞–ø—Ä–æ—Å–æ–≤ ({final_urls_empty/len(analyzer.df)*100:.1f}%)")
                
                if final_urls_empty > 0:
                    print_stage(analyzer, f"   ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: {final_urls_empty} –∑–∞–ø—Ä–æ—Å–æ–≤ –≤—Å–µ –µ—â–µ –±–µ–∑ SERP –¥–∞–Ω–Ω—ã—Ö!")
                    print_stage(analyzer, f"   –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –¥–∞–Ω–Ω—ã–º–∏.")
    else:
        print_stage(analyzer, f"  ‚ö†Ô∏è  DataFrame –ø—É—Å—Ç–æ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ - –Ω–µ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª–∏ –¥–ª—è –Ω–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö
    for col in ['serp_docs_count', 'serp_main_pages', 'serp_main_pages_count', 
                'serp_internal_pages_count', 'serp_titles_count', 
                'serp_commercial_domains', 'serp_info_domains']:
        analyzer.df[col] = analyzer.df[col].fillna(0).astype(int)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = analyzer.serp_analyzer.get_statistics()
    print_stage(analyzer, f"‚úì SERP –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω:")
    print_stage(analyzer, f"  –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['total_queries']}")
    print_stage(analyzer, f"  –ò–∑ –∫—ç—à–∞ (Master DB): {stats['cached_from_master']}")
    print_stage(analyzer, f"  API –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['api_requests']}")
    print_stage(analyzer, f"  –û—à–∏–±–æ–∫: {stats['errors']}")
    if 'cache_hit_rate' in stats:
        print_stage(analyzer, f"  Cache hit rate: {stats['cache_hit_rate']:.1f}%")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    queries_with_serp = len(analyzer.df[analyzer.df['serp_docs_count'] > 0])
    if queries_with_serp > 0:
        print_stage(analyzer, f"‚úì –ü–æ–ª—É—á–µ–Ω—ã SERP –¥–∞–Ω–Ω—ã–µ –¥–ª—è {queries_with_serp} –∑–∞–ø—Ä–æ—Å–æ–≤")
        avg_docs = analyzer.df[analyzer.df['serp_docs_count'] > 0]['serp_docs_count'].mean()
        median_docs = analyzer.df[analyzer.df['serp_docs_count'] > 0]['serp_docs_count'].median()
        print_stage(analyzer, f"  –°—Ä–µ–¥–Ω–µ–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å: {int(avg_docs):,}")
        print_stage(analyzer, f"  –ú–µ–¥–∏–∞–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å: {int(median_docs):,}")
    else:
        print_stage(analyzer, f"‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ù–µ –ø–æ–ª—É—á–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ SERP!")
        print_stage(analyzer, f"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API –∫–ª—é—á –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ xmlstock")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º completed, –Ω–æ –±–µ–∑ URL/LSI
    if query_group:
        print_stage(analyzer, "")
        print_stage(analyzer, "üîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
        try:
            updated_count = await analyzer.serp_analyzer.recover_missing_lsi_from_urls(group_name=query_group)
            if updated_count > 0:
                print_stage(analyzer, f"‚úì –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {updated_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
                # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Master DB –¥–ª—è –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                print_stage(analyzer, "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ DataFrame...")
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –∑–∞–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
                for idx, row in analyzer.df.iterrows():
                    keyword = row['keyword']
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ Master DB
                    if analyzer.serp_analyzer.master_db:
                        updated_data = analyzer.serp_analyzer._get_from_master_db(keyword)
                        if updated_data and updated_data.get('documents'):
                            # –û–±–Ω–æ–≤–ª—è–µ–º LSI —Ñ—Ä–∞–∑—ã
                            if updated_data.get('lsi_phrases'):
                                analyzer.df.at[idx, 'lsi_phrases'] = updated_data['lsi_phrases']
                            # –û–±–Ω–æ–≤–ª—è–µ–º URL
                            if updated_data.get('documents'):
                                analyzer.df.at[idx, 'serp_urls'] = [
                                    doc.get('url', '') for doc in updated_data['documents'][:20] 
                                    if doc.get('url')
                                ]
            else:
                print_stage(analyzer, "‚úì –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã –∏–º–µ—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ")
        except Exception as e:
            print_stage(analyzer, f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            # –ù–µ –∫—Ä–∏—Ç–∏—á–Ω–æ - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É
    
    print()

