"""–≠—Ç–∞–ø 4: SERP –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ xmlstock"""

import os
from seo_analyzer.analysis.serp_analyzer import SERPAnalyzer
from seo_analyzer.core.config import SERP_CONFIG
from .stage_logger import get_group_prefix, print_stage



def get_api_key(args):
    """
    –ü–æ–ª—É—á–∞–µ—Ç API –∫–ª—é—á –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    
    Args:
        args: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
        
    Returns:
        API –∫–ª—é—á –∏–ª–∏ None
    """
    api_key = args.xmlstock_api_key
    
    # –ï—Å–ª–∏ –Ω–µ—Ç –≤ –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ö, –ø—Ä–æ–±—É–µ–º config_local.py
    if not api_key:
        try:
            import config_local
            api_key = getattr(config_local, 'XMLSTOCK_API_KEY', None)
            if api_key:
                print("‚úì API –∫–ª—é—á –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ config_local.py")
        except ImportError:
            pass
    
    # –ï—Å–ª–∏ –Ω–µ—Ç, –ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
    if not api_key:
        api_key = os.getenv('XMLSTOCK_API_KEY')
        if api_key:
            print("‚úì API –∫–ª—é—á –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è")
    
    return api_key


async def analyze_serp_stage(args, analyzer):
    """
    SERP –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ xmlstock
    
    Args:
        args: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
        analyzer: –≠–∫–∑–µ–º–ø–ª—è—Ä SEOAnalyzer
        
    Returns:
        None (–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ analyzer)
    """
    prefix = get_group_prefix(analyzer)
    print_stage(analyzer, "üîç –≠–¢–ê–ü 3: SERP –∞–Ω–∞–ª–∏–∑ (–±–∞–∑–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)")
    print_stage(analyzer, "-" * 80)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ DataFrame –Ω–µ –ø—É—Å—Ç–æ–π
    if len(analyzer.df) == 0:
        print_stage(analyzer, "‚ö†Ô∏è  DataFrame –ø—É—Å—Ç–æ–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º SERP –∞–Ω–∞–ª–∏–∑")
        print()
        return
    
    # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á
    api_key = get_api_key(args)
    
    if not api_key:
        print_stage(analyzer, "‚ö†Ô∏è  API –∫–ª—é—á xmlstock –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print_stage(analyzer, "   –°–ø–æ—Å–æ–± 1: --xmlstock-api-key user:key")
        print_stage(analyzer, "   –°–ø–æ—Å–æ–± 2: —Å–æ–∑–¥–∞–π—Ç–µ config_local.py (—Å–º. config_local.py.example)")
        print_stage(analyzer, "   –°–ø–æ—Å–æ–± 3: export XMLSTOCK_API_KEY=user:key")
        return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    print_stage(analyzer, f"üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SERP –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞...")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä—É–ø–ø—É –∑–∞–ø—Ä–æ—Å–æ–≤
    query_group = None
    if hasattr(analyzer, 'current_group') and analyzer.current_group:
        query_group = analyzer.current_group.name
        print_stage(analyzer, f"üìÅ –ì—Ä—É–ø–ø–∞ –∑–∞–ø—Ä–æ—Å–æ–≤: {query_group}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ batch_async —Ä–µ–∂–∏–º–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
    use_batch_async = getattr(args, 'serp_batch_async', True)
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã SERP –∏–∑ args
    serp_region = getattr(args, 'serp_region', SERP_CONFIG['api']['lr'])
    serp_device = getattr(args, 'serp_device', 'desktop')
    serp_site = getattr(args, 'serp_site', None)
    
    analyzer.serp_analyzer = SERPAnalyzer(
        api_key=api_key,
        lr=serp_region,
        max_retries=SERP_CONFIG['api']['max_retries'],
        retry_delay=SERP_CONFIG['api']['retry_delay'],
        timeout=SERP_CONFIG['api']['timeout'],
        query_group=query_group,
        max_concurrent=SERP_CONFIG['api']['max_concurrent'],  # –ì–ª–æ–±–∞–ª—å–Ω—ã–π –ª–∏–º–∏—Ç –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø
        use_master_db=True,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ Master DB
        use_batch_async=use_batch_async,  # üöÄ –ú–ê–°–°–û–í–´–ô ASYNC –†–ï–ñ–ò–ú (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        device=serp_device,
        site=serp_site
    )
    
    if use_batch_async:
        print_stage(analyzer, "üöÄ –†–µ–∂–∏–º: BATCH ASYNC (–º–∞—Å—Å–æ–≤–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ ‚Üí –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ)")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
    all_queries = analyzer.df['keyword'].tolist()
    
    print_stage(analyzer, f"üìä –ê–Ω–∞–ª–∏–∑ SERP –¥–ª—è {len(all_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤ (–∫—ç—à –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)...")
    
    # –ü—Ä–æ–≥—Ä–µ—Å—Å
    def progress_callback(current, total, query, status=None):
        if current % 100 == 0 or current == total:
            status_text = f" {status}" if status else ""
            print_stage(analyzer, f"  [{current}/{total}]{status_text} {query[:60]}...")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞–∫–µ—Ç–æ–º (–∫—ç—à –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ - –º–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö)
    serp_results = await analyzer.serp_analyzer.analyze_queries_batch(
        all_queries,
        max_concurrent=SERP_CONFIG['api']['max_concurrent'],
        progress_callback=progress_callback
    )
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DataFrame
    serp_dict = {result['query']: result for result in serp_results}
    
    # SERP –º–µ—Ç—Ä–∏–∫–∏
    analyzer.df['serp_docs_count'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('metrics', {}).get('found_docs', 0)
    )
    # –î–æ–±–∞–≤–ª—è–µ–º serp_found_docs –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ë–î
    analyzer.df['serp_found_docs'] = analyzer.df['serp_docs_count']
    
    analyzer.df['serp_main_pages'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('metrics', {}).get('main_pages_count', 0)
    )
    # –ö–æ–ª-–≤–æ –≥–ª–∞–≤–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–¥–ª—è Excel)
    analyzer.df['serp_main_pages_count'] = analyzer.df['serp_main_pages']
    # –ö–æ–ª-–≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–æ–±—â–µ–µ –∫–æ–ª-–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –º–∏–Ω—É—Å –≥–ª–∞–≤–Ω—ã–µ)
    analyzer.df['serp_internal_pages_count'] = (
        analyzer.df['serp_docs_count'] - analyzer.df['serp_main_pages']
    ).clip(lower=0)  # –ù–µ –¥–æ–ø—É—Å–∫–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    
    analyzer.df['serp_titles_count'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('metrics', {}).get('titles_with_keyword', 0)
    )
    # –î–æ–±–∞–≤–ª—è–µ–º serp_titles_with_keyword –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ë–î
    analyzer.df['serp_titles_with_keyword'] = analyzer.df['serp_titles_count']
    
    analyzer.df['serp_commercial_domains'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('metrics', {}).get('commercial_domains', 0)
    )
    analyzer.df['serp_info_domains'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('metrics', {}).get('info_domains', 0)
    )
    
    # LSI —Ñ—Ä–∞–∑—ã
    analyzer.df['lsi_phrases'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('lsi_phrases', [])
    )
    
    # –î–æ–º–µ–Ω—ã –∏–∑ SERP (–¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏) - –¢–û–ü-20
    # –í–ê–ñ–ù–û: –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ serp_top_urls –∏–∑ Master DB
    if 'serp_top_urls' in analyzer.df.columns:
        # –î–∞–Ω–Ω—ã–µ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ Master DB - —Å–æ–∑–¥–∞–µ–º serp_urls –∏–∑ serp_top_urls
        def extract_urls_from_top_urls(serp_top_urls):
            """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ serp_top_urls –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
            if not serp_top_urls or not isinstance(serp_top_urls, list):
                return []
            
            urls = []
            for item in serp_top_urls[:20]:  # TOP-20
                if isinstance(item, dict):
                    url = item.get('url', '')
                elif isinstance(item, str):
                    url = item
                else:
                    continue
                
                if url:
                    urls.append(url)
            
            return urls
        
        analyzer.df['serp_urls'] = analyzer.df['serp_top_urls'].apply(extract_urls_from_top_urls)
        print_stage(analyzer, "   ‚úì serp_urls —Å–æ–∑–¥–∞–Ω—ã –∏–∑ serp_top_urls (Master DB)")
    else:
        # –î–∞–Ω–Ω—ã—Ö –∏–∑ Master DB –Ω–µ—Ç - –∏–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ serp_dict
        def extract_domains(query):
            result = serp_dict.get(query, {})
            documents = result.get('documents', [])
            if not documents:
                return []
            
            # –ï—Å–ª–∏ documents - —Å—Ç—Ä–æ–∫–∞ (JSON –∏–∑ –∫—ç—à–∞), –ø–∞—Ä—Å–∏–º –µ—ë
            if isinstance(documents, str):
                import json
                try:
                    documents = json.loads(documents)
                except:
                    return []
            
            # –ï—Å–ª–∏ documents - —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
            if isinstance(documents, list) and len(documents) > 0:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
                if isinstance(documents[0], dict):
                    # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ TOP-20 URL –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–¥–∞–∂–µ –µ—Å–ª–∏ –∏—Ö –±–æ–ª—å—à–µ)
                    return [doc.get('url', '') for doc in documents[:20] if doc.get('url')]
                elif isinstance(documents[0], str):
                    # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ —Å–ø–∏—Å–æ–∫ URL
                    return documents[:20]
            
            return []
        
        analyzer.df['serp_urls'] = analyzer.df['keyword'].map(extract_domains)
    
    # –ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ SERP (–¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ —Å title, snippet –∏ URL)
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º: –µ—Å–ª–∏ serp_top_urls —É–∂–µ –µ—Å—Ç—å (–∏–∑ Master DB), –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
    if 'serp_top_urls' in analyzer.df.columns:
        # –î–∞–Ω–Ω—ã–µ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ Master DB –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã
        analyzer.df['serp_documents'] = analyzer.df['serp_top_urls']
        print_stage(analyzer, "   ‚úì serp_documents –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ Master DB (—Å title –∏ snippet)")
    else:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö API
        def extract_documents(query):
            result = serp_dict.get(query, {})
            documents = result.get('documents', [])
            if not documents:
                return []
            
            # –ï—Å–ª–∏ documents - —Å—Ç—Ä–æ–∫–∞ (JSON –∏–∑ –∫—ç—à–∞), –ø–∞—Ä—Å–∏–º –µ—ë
            if isinstance(documents, str):
                import json
                try:
                    documents = json.loads(documents)
                except:
                    return []
            
            # –ï—Å–ª–∏ documents - —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
            if isinstance(documents, list) and len(documents) > 0:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
                if isinstance(documents[0], dict):
                    # –ë–µ—Ä—ë–º TOP-20 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
                    return documents[:20]
            
            return []
        
        analyzer.df['serp_documents'] = analyzer.df['keyword'].map(extract_documents)
        print_stage(analyzer, "   ‚úì serp_documents –∏–∑–≤–ª–µ—á–µ–Ω—ã –∏–∑ SERP API (—Å title –∏ snippet)")
    
    # XML –æ—Ç–≤–µ—Ç—ã (–¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ offer_info)
    analyzer.df['xml_response'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('xml_response', '')
    )
    
    # SERP —Å—Ç–∞—Ç—É—Å –∏ req_id (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö)
    # –≠—Ç–∏ –ø–æ–ª—è –Ω—É–∂–Ω—ã –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –ë–î
    analyzer.df['serp_status'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('status', 'completed')
    )
    analyzer.df['serp_req_id'] = analyzer.df['keyword'].map(
        lambda x: serp_dict.get(x, {}).get('req_id')
    )
    
    # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    urls_filled = (analyzer.df['serp_urls'].apply(lambda x: isinstance(x, list) and len(x) > 0)).sum()
    urls_empty = (analyzer.df['serp_urls'].apply(lambda x: not isinstance(x, list) or len(x) == 0)).sum()
    
    if len(analyzer.df) > 0:
        print_stage(analyzer, f"  SERP URLs –∑–∞–ø–æ–ª–Ω–µ–Ω–æ: {urls_filled} –∑–∞–ø—Ä–æ—Å–æ–≤ ({urls_filled/len(analyzer.df)*100:.1f}%)")
        print_stage(analyzer, f"  SERP URLs –ø—É—Å—Ç–æ: {urls_empty} –∑–∞–ø—Ä–æ—Å–æ–≤ ({urls_empty/len(analyzer.df)*100:.1f}%)")
    else:
        print_stage(analyzer, f"  ‚ö†Ô∏è  DataFrame –ø—É—Å—Ç–æ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ - –Ω–µ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª–∏ –¥–ª—è –Ω–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö
    for col in ['serp_docs_count', 'serp_main_pages', 'serp_main_pages_count', 
                'serp_internal_pages_count', 'serp_titles_count', 
                'serp_commercial_domains', 'serp_info_domains']:
        analyzer.df[col] = analyzer.df[col].fillna(0).astype(int)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = analyzer.serp_analyzer.get_statistics()
    print_stage(analyzer, f"‚úì SERP –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω:")
    print_stage(analyzer, f"  –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['total_queries']}")
    print_stage(analyzer, f"  –ò–∑ –∫—ç—à–∞ (Master DB): {stats['cached_from_master']}")
    print_stage(analyzer, f"  API –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['api_requests']}")
    print_stage(analyzer, f"  –û—à–∏–±–æ–∫: {stats['errors']}")
    if 'cache_hit_rate' in stats:
        print_stage(analyzer, f"  Cache hit rate: {stats['cache_hit_rate']:.1f}%")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    queries_with_serp = len(analyzer.df[analyzer.df['serp_docs_count'] > 0])
    if queries_with_serp > 0:
        print_stage(analyzer, f"‚úì –ü–æ–ª—É—á–µ–Ω—ã SERP –¥–∞–Ω–Ω—ã–µ –¥–ª—è {queries_with_serp} –∑–∞–ø—Ä–æ—Å–æ–≤")
        avg_docs = analyzer.df[analyzer.df['serp_docs_count'] > 0]['serp_docs_count'].mean()
        median_docs = analyzer.df[analyzer.df['serp_docs_count'] > 0]['serp_docs_count'].median()
        print_stage(analyzer, f"  –°—Ä–µ–¥–Ω–µ–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å: {int(avg_docs):,}")
        print_stage(analyzer, f"  –ú–µ–¥–∏–∞–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å: {int(median_docs):,}")
    else:
        print_stage(analyzer, f"‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ù–µ –ø–æ–ª—É—á–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ SERP!")
        print_stage(analyzer, f"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API –∫–ª—é—á –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ xmlstock")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º completed, –Ω–æ –±–µ–∑ URL/LSI
    if query_group:
        print_stage(analyzer, "")
        print_stage(analyzer, "üîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
        try:
            updated_count = await analyzer.serp_analyzer.recover_missing_lsi_from_urls(group_name=query_group)
            if updated_count > 0:
                print_stage(analyzer, f"‚úì –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {updated_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
                # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Master DB –¥–ª—è –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                print_stage(analyzer, "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ DataFrame...")
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –∑–∞–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
                for idx, row in analyzer.df.iterrows():
                    keyword = row['keyword']
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ Master DB
                    if analyzer.serp_analyzer.master_db:
                        updated_data = analyzer.serp_analyzer._get_from_master_db(keyword)
                        if updated_data and updated_data.get('documents'):
                            # –û–±–Ω–æ–≤–ª—è–µ–º LSI —Ñ—Ä–∞–∑—ã
                            if updated_data.get('lsi_phrases'):
                                analyzer.df.at[idx, 'lsi_phrases'] = updated_data['lsi_phrases']
                            # –û–±–Ω–æ–≤–ª—è–µ–º URL
                            if updated_data.get('documents'):
                                analyzer.df.at[idx, 'serp_urls'] = [
                                    doc.get('url', '') for doc in updated_data['documents'][:20] 
                                    if doc.get('url')
                                ]
            else:
                print_stage(analyzer, "‚úì –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã –∏–º–µ—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ")
        except Exception as e:
            print_stage(analyzer, f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            # –ù–µ –∫—Ä–∏—Ç–∏—á–Ω–æ - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É
    
    print()

