"""–≠—Ç–∞–ø 2: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"""

from pathlib import Path
from seo_analyzer.core.stopwords import StopwordsFilter, filter_short_queries, filter_low_frequency, filter_by_frequency_ratio
from seo_analyzer.core.normalizer import QueryNormalizer
from seo_analyzer.clustering.deduplicator import QueryDeduplicator
from seo_analyzer.analysis.key_phrases_extractor import KeyPhrasesExtractor
from seo_analyzer.analysis.ner_extractor import NERExtractor
from seo_analyzer.clustering.advanced_deduplicator import AdvancedDeduplicator, load_stopwords_from_file
from seo_analyzer.clustering.geo_processor import AsyncGeoProcessor
from seo_analyzer.export.csv import save_filtered_queries
from .stage_logger import get_group_prefix, print_stage



async def preprocessing_stage(args, analyzer):
    """
    –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
    
    Args:
        args: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
        analyzer: –≠–∫–∑–µ–º–ø–ª—è—Ä SEOAnalyzer
        
    Returns:
        None (–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ analyzer)
    """
    prefix = get_group_prefix(analyzer)
    print_stage(analyzer, "üîß –≠–¢–ê–ü 2: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞")
    print_stage(analyzer, "-" * 80)
    
    # Debug: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    loaded_from_cache = getattr(analyzer, 'loaded_from_cache', False)
    loaded_from_master_db = getattr(analyzer, 'loaded_from_master_db', False)
    print_stage(analyzer, f"üîç DEBUG: loaded_from_cache = {loaded_from_cache}, loaded_from_master_db = {loaded_from_master_db}")
    print_stage(analyzer, f"üîç DEBUG: –ó–∞–ø—Ä–æ—Å–æ–≤ = {len(analyzer.df)}")
    print_stage(analyzer, f"üîç DEBUG: –ö–æ–ª–æ–Ω–∫–∏ –Ω–∞ –≤—Ö–æ–¥–µ: {list(analyzer.df.columns)[:10]}...")
    
    # –ï—Å–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ Master DB - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –í–°–Æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É (—Ç–∞–º —É–∂–µ –≤—Å—ë –µ—Å—Ç—å!)
    if loaded_from_master_db:
        print_stage(analyzer, "‚úÖ –î–∞–Ω–Ω—ã–µ –∏–∑ Master DB - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É (—É–∂–µ –≥–æ—Ç–æ–≤–æ)")
        print_stage(analyzer, f"   –î–æ—Å—Ç—É–ø–Ω–æ –∫–æ–ª–æ–Ω–æ–∫: {len(analyzer.df.columns)}")
        print_stage(analyzer, f"   –ò–Ω—Ç–µ–Ω—Ç: {'‚úì' if 'main_intent' in analyzer.df.columns else '‚úó'}")
        print_stage(analyzer, f"   SERP: {'‚úì' if 'serp_found_docs' in analyzer.df.columns else '‚úó'}")
        print_stage(analyzer, f"   SERP URLs: {'‚úì' if 'serp_top_urls' in analyzer.df.columns else '‚úó'}")
        print()
        return
    
    # –ï—Å–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ query_cache - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∞—Å—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
    if loaded_from_cache:
        print_stage(analyzer, "‚ö° –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞")
        print_stage(analyzer, "‚úì –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞ (normalized, lemmatized, NER, key phrases —É–∂–µ –≤ –∫—ç—à–µ)")
        print_stage(analyzer, f"‚úì –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(analyzer.df)}")
        
        if hasattr(analyzer, 'query_cache') and hasattr(analyzer, 'current_group'):
            stats = analyzer.query_cache.get_group_stats(analyzer.current_group.name)
            if stats:
                print_stage(analyzer, f"‚úì –î—É–±–ª–∏–∫–∞—Ç–æ–≤ —É–¥–∞–ª–µ–Ω–æ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ: {stats['duplicates_removed']}")
        
        # üåç –í–ê–ñ–ù–û: –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏ –¥–∞–∂–µ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!
        # –ì–µ–æ–≥—Ä–∞—Ñ–∏—è –Ω—É–∂–Ω–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏, –∏ –ª—É—á—à–µ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –µ—ë –∑–∞—Ä–∞–Ω–µ–µ –≤ —Ñ–æ–Ω–µ
        print_stage(analyzer, "\nüåç –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ...")
        
        from seo_analyzer.clustering.semantic_checker import SemanticClusterChecker
        semantic_checker = SemanticClusterChecker(geo_dicts=analyzer.geo_dicts)
        
        analyzer.geo_processor = AsyncGeoProcessor(
            semantic_checker=semantic_checker,
            max_workers=4
        )
        
        queries_list = analyzer.df['keyword'].tolist()
        analyzer.geo_processor.start_processing(queries_list)
        
        print_stage(analyzer, f"‚úì –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(queries_list)} –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–∞–ø—É—â–µ–Ω–∞ (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –¥—Ä—É–≥–∏–º–∏ —ç—Ç–∞–ø–∞–º–∏)")
        print_stage(analyzer, "  üí° –ì–µ–æ–≥—Ä–∞—Ñ–∏—è –±—É–¥–µ—Ç –≥–æ—Ç–æ–≤–∞ –∫ –º–æ–º–µ–Ω—Ç—É –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∏ —Å–æ–∑–¥–∞–µ–º –∏—Ö –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç
        # (–º–æ–∂–µ—Ç –±—ã—Ç—å –µ—Å–ª–∏ –∫—ç—à —Å–æ–∑–¥–∞–Ω —Å—Ç–∞—Ä–æ–π –≤–µ—Ä—Å–∏–µ–π)
        missing_critical = []
        if 'normalized' not in analyzer.df.columns:
            missing_critical.append('normalized')
        if 'lemmatized' not in analyzer.df.columns:
            missing_critical.append('lemmatized')
        
        if missing_critical:
            print_stage(analyzer, f"\n‚ö†Ô∏è  –ö—Ä–∏—Ç–∏—á–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç: {', '.join(missing_critical)}")
            print_stage(analyzer, "üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–ª–æ–Ω–æ–∫...")
            
            normalizer = QueryNormalizer()
            normalized_results = normalizer.normalize_batch(analyzer.df['keyword'].tolist())
            
            if 'normalized' not in analyzer.df.columns:
                analyzer.df['normalized'] = [r['normalized'] for r in normalized_results]
            if 'lemmatized' not in analyzer.df.columns:
                analyzer.df['lemmatized'] = [r['lemmatized'] for r in normalized_results]
            
            print_stage(analyzer, "‚úì –ö–æ–ª–æ–Ω–∫–∏ —Å–æ–∑–¥–∞–Ω—ã")
            
            # –ü–µ—Ä–µ—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫—ç—à —Å –Ω–æ–≤—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏
            if hasattr(analyzer, 'query_cache') and hasattr(analyzer, 'current_group'):
                print_stage(analyzer, "üíæ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞...")
                analyzer.query_cache.save_queries(
                    group_name=analyzer.current_group.name,
                    csv_path=analyzer.current_group.input_file,
                    df=analyzer.df,
                    duplicates_removed=stats.get('duplicates_removed', 0) if stats else 0
                )
                print_stage(analyzer, "‚úì –ö—ç—à –æ–±–Ω–æ–≤–ª–µ–Ω")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å CSV –∏–∑ –∫—ç—à–∞
        _sync_csv_from_cache_if_needed(args, analyzer, print_stage)
        
        print()
        return
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤
    stopwords_filter = StopwordsFilter(analyzer.stopwords)
    analyzer.df = stopwords_filter.filter_dataframe(analyzer.df)
    print_stage(analyzer, f"‚úì –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {stopwords_filter.blocked_count} –∑–∞–ø—Ä–æ—Å–æ–≤ —Å–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞–º–∏")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
    analyzer.stopwords_filter = stopwords_filter
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –Ω–∏–∑–∫–æ–π —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å—é
    initial_count = len(analyzer.df)
    min_freq = getattr(args, 'min_frequency', 1)
    analyzer.df = filter_low_frequency(analyzer.df, freq_column='frequency_world', min_freq=min_freq)
    low_freq_removed = initial_count - len(analyzer.df)
    if low_freq_removed > 0:
        if min_freq == 1:
            print_stage(analyzer, f"‚úì –£–¥–∞–ª–µ–Ω–æ {low_freq_removed} –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –Ω—É–ª–µ–≤–æ–π —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å—é")
        else:
            print_stage(analyzer, f"‚úì –£–¥–∞–ª–µ–Ω–æ {low_freq_removed} –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å—é < {min_freq}")
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—é —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–µ–π
    initial_count = len(analyzer.df)
    max_ratio = getattr(args, 'max_frequency_ratio', 51.0)
    analyzer.df = filter_by_frequency_ratio(
        analyzer.df,
        freq_world_column='frequency_world',
        freq_exact_column='frequency_exact',
        max_ratio=max_ratio
    )
    ratio_removed = initial_count - len(analyzer.df)
    if ratio_removed > 0:
        print_stage(analyzer, f"‚úì –£–¥–∞–ª–µ–Ω–æ {ratio_removed} –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–µ–π > {max_ratio}")
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    initial_count = len(analyzer.df)
    analyzer.df = filter_short_queries(analyzer.df, min_length=2)
    print_stage(analyzer, f"‚úì –£–¥–∞–ª–µ–Ω–æ {initial_count - len(analyzer.df)} —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞: –æ—Å—Ç–∞–ª–∏—Å—å –ª–∏ –∑–∞–ø—Ä–æ—Å—ã –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    if len(analyzer.df) == 0:
        print_stage(analyzer, "\n‚ö†Ô∏è  –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã –±—ã–ª–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã!")
        print_stage(analyzer, "‚ö†Ô∏è  –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
        print_stage(analyzer, "    - –£–º–µ–Ω—å—à–∏—Ç—å max_frequency_ratio (--max-frequency-ratio)")
        print_stage(analyzer, "    - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–π CSV —Ñ–∞–π–ª —Å –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏")
        print()
        
        # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –æ—à–∏–±–æ–∫ –≤ –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö —ç—Ç–∞–ø–∞—Ö
        for col in ['normalized', 'lemmatized', 'words_count', 'has_latin', 'has_numbers',
                    'main_words', 'key_phrase', 'ner_entities', 'ner_locations']:
            if col not in analyzer.df.columns:
                analyzer.df[col] = []
        
        return
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
    analyzer.normalizer = QueryNormalizer()
    print_stage(analyzer, "üîÑ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤...")
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
    import asyncio
    normalized_results = await asyncio.to_thread(
        analyzer.normalizer.normalize_batch,
        analyzer.df['keyword'].tolist()
    )
    
    for key in ['normalized', 'lemmatized', 'word_count', 'has_latin', 'has_numbers']:
        analyzer.df[key] = [r[key] for r in normalized_results]
    
    analyzer.df.rename(columns={'word_count': 'words_count'}, inplace=True)
    
    print_stage(analyzer, f"‚úì –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ —Ñ—Ä–∞–∑
    print_stage(analyzer, "üîÑ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑...")
    key_phrases_extractor = KeyPhrasesExtractor()
    
    if key_phrases_extractor.enabled:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–ª–∞–≤–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã (–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ)
        keywords_list = analyzer.df['keyword'].tolist()
        
        main_words_results = await asyncio.to_thread(
            lambda: [key_phrases_extractor.get_main_words_string(kw) for kw in keywords_list]
        )
        key_phrase_results = await asyncio.to_thread(
            lambda: [key_phrases_extractor.get_key_phrase(kw) for kw in keywords_list]
        )
        
        analyzer.df['main_words'] = main_words_results
        analyzer.df['key_phrase'] = key_phrase_results
        
        print_stage(analyzer, f"‚úì –ò–∑–≤–ª–µ—á–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã")
    else:
        print_stage(analyzer, f"‚ö†Ô∏è –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (NER)
    print_stage(analyzer, "üîÑ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (NER)...")
    ner_extractor = NERExtractor()
    
    if ner_extractor.enabled:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ (–±—Ä–µ–Ω–¥—ã, –≥–æ—Ä–æ–¥–∞, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏) –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        keywords_list = analyzer.df['keyword'].tolist()
        
        ner_entities_results = await asyncio.to_thread(
            lambda: [ner_extractor.get_entities_string(kw) for kw in keywords_list]
        )
        ner_locations_results = await asyncio.to_thread(
            lambda: [ner_extractor.get_locations_string(kw) for kw in keywords_list]
        )
        
        analyzer.df['ner_entities'] = ner_entities_results
        analyzer.df['ner_locations'] = ner_locations_results
        
        print_stage(analyzer, f"‚úì –ò–∑–≤–ª–µ—á–µ–Ω—ã –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏")
    else:
        # –°–æ–∑–¥–∞—ë–º –ø—É—Å—Ç—ã–µ —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∫—ç—à–µ–º
        analyzer.df['ner_entities'] = ''
        analyzer.df['ner_locations'] = ''
        print_stage(analyzer, f"‚ö†Ô∏è NER –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ (—Ç—Ä–µ–±—É–µ—Ç natasha)")
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è (–ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ)
    print_stage(analyzer, "üîÑ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è (—Ç–æ—á–Ω—ã–µ –¥—É–±–ª–∏)...")
    analyzer.deduplicator = QueryDeduplicator()
    analyzer.df = analyzer.deduplicator.deduplicate(
        analyzer.df,
        normalized_column='normalized',
        original_column='keyword',
        freq_column='frequency_exact'
    )
    
    stats = analyzer.deduplicator.get_deduplication_stats()
    print_stage(analyzer, f"‚úì –£–¥–∞–ª–µ–Ω–æ —Ç–æ—á–Ω—ã—Ö –¥—É–±–ª–µ–π: {stats['total_duplicates_removed']}")
    
    # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è (–Ω–µ—è–≤–Ω—ã–µ –¥—É–±–ª–∏)
    print_stage(analyzer, "üîÑ –ü–æ–∏—Å–∫ –Ω–µ—è–≤–Ω—ã—Ö –¥—É–±–ª–µ–π...")
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ keywords_settings
    stopwords_file = Path('keywords_settings/stop_keywords.txt')
    dedup_stopwords = load_stopwords_from_file(stopwords_file) if stopwords_file.exists() else set()
    
    advanced_dedup = AdvancedDeduplicator(stopwords=dedup_stopwords)
    analyzer.df, removed_df = advanced_dedup.remove_duplicates(
        analyzer.df,
        keyword_column='keyword',
        freq_column='frequency_world'
    )
    
    adv_stats = advanced_dedup.get_deduplication_stats()
    print_stage(analyzer, f"‚úì –£–¥–∞–ª–µ–Ω–æ –Ω–µ—è–≤–Ω—ã—Ö –¥—É–±–ª–µ–π: {adv_stats['total_duplicates_removed']} ({adv_stats['duplicate_groups']} –≥—Ä—É–ø–ø)")
    print_stage(analyzer, f"‚úì –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {adv_stats['unique_queries']}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
    analyzer.removed_implicit_duplicates = removed_df
    analyzer.advanced_deduplicator = advanced_dedup
    
    # üåç –ó–ê–ü–£–°–ö–ê–ï–ú –ê–°–ò–ù–•–†–û–ù–ù–£–Æ –û–ë–†–ê–ë–û–¢–ö–£ –ì–ï–û–ì–†–ê–§–ò–ò –í –§–û–ù–ï
    # –ü–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –ø–æ–ª—É—á–∏–ª–∏ —á–∏—Å—Ç—ã–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã - —Å–∞–º–æ–µ –≤—Ä–µ–º—è –Ω–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏
    # –ö –º–æ–º–µ–Ω—Ç—É –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (—á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç—Ç–∞–ø–æ–≤) –≥–µ–æ–≥—Ä–∞—Ñ–∏—è —É–∂–µ –±—É–¥–µ—Ç –≥–æ—Ç–æ–≤–∞
    print_stage(analyzer, "\nüåç –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º semantic_checker –µ—Å–ª–∏ –æ–Ω –Ω—É–∂–µ–Ω –¥–ª—è –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏
    from seo_analyzer.clustering.semantic_checker import SemanticClusterChecker
    semantic_checker = SemanticClusterChecker(geo_dicts=analyzer.geo_dicts)
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
    analyzer.geo_processor = AsyncGeoProcessor(
        semantic_checker=semantic_checker,
        max_workers=4  # 4 –ø–æ—Ç–æ–∫–∞ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    )
    
    queries_list = analyzer.df['keyword'].tolist()
    analyzer.geo_processor.start_processing(queries_list)
    
    print_stage(analyzer, f"‚úì –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(queries_list)} –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–∞–ø—É—â–µ–Ω–∞ (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –¥—Ä—É–≥–∏–º–∏ —ç—Ç–∞–ø–∞–º–∏)")
    print_stage(analyzer, "  üí° –ì–µ–æ–≥—Ä–∞—Ñ–∏—è –±—É–¥–µ—Ç –≥–æ—Ç–æ–≤–∞ –∫ –º–æ–º–µ–Ω—Ç—É –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
    
    # –ü–µ—Ä–µ–∑–∞–ø–∏—Å—å –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏ (–ø–æ—Å–ª–µ –≤—Å–µ—Ö —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–π –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏)
    # –ù–û —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –ù–ï –±—ã–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞
    if not getattr(analyzer, 'loaded_from_cache', False):
        print_stage(analyzer, "\nüíæ –ü–µ—Ä–µ–∑–∞–ø–∏—Å—å —Ñ–∞–π–ª–∞ —Å –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏ (–±–µ–∑ –¥—É–±–ª–µ–π)...")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
        input_file = None
        
        if hasattr(analyzer, 'current_group') and analyzer.current_group:
            # –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã —Å –≥—Ä—É–ø–ø–æ–π
            input_file = analyzer.current_group.input_file
        elif hasattr(args, 'input_file') and args.input_file:
            # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
            input_file = Path(args.input_file)
            if not input_file.is_absolute():
                input_file = Path.cwd() / input_file
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        if input_file:
            success = save_filtered_queries(
                analyzer.df,
                input_file,
                backup=True
            )
            
            if success:
                print_stage(analyzer, f"‚úì –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª –æ–±–Ω–æ–≤–ª–µ–Ω (–±–µ–∑ –¥—É–±–ª–µ–π, —Ç–æ–ª—å–∫–æ —Ü–µ–ª–µ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã)")
            else:
                print_stage(analyzer, f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª")
        else:
            print_stage(analyzer, f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ù–ï –∑–∞–≥—Ä—É–∂–∞–ª–∏ –∏–∑ –∫—ç—à–∞)
    if hasattr(analyzer, 'query_cache') and hasattr(analyzer, 'current_group') and analyzer.current_group:
        if not getattr(analyzer, 'loaded_from_cache', False):
            print_stage(analyzer, "\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –∫—ç—à...")
            total_duplicates = stats['total_duplicates_removed'] + adv_stats['total_duplicates_removed']
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –ü–û–°–õ–ï –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏ CSV
            # (—á—Ç–æ–±—ã hash –≤ –∫—ç—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–º—É CSV)
            analyzer.query_cache.save_queries(
                group_name=analyzer.current_group.name,
                csv_path=analyzer.current_group.input_file,
                df=analyzer.df,
                duplicates_removed=total_duplicates
            )
            
            print_stage(analyzer, f"  ‚ö° –°–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—É—Å–∫ –±—É–¥–µ—Ç –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–º (–±–µ–∑ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏)")
            print_stage(analyzer, f"  üí° CSV –æ–±–Ω–æ–≤–ª—ë–Ω, –∫—ç—à —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
            # Debug: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∏ —Å–æ–∑–¥–∞–Ω—ã
            print_stage(analyzer, f"üîç DEBUG: –ö–æ–ª–æ–Ω–∫–∏ –Ω–∞ –≤—ã—Ö–æ–¥–µ: {list(analyzer.df.columns)[:15]}...")
            if 'lemmatized' in analyzer.df.columns and 'normalized' in analyzer.df.columns:
                print_stage(analyzer, f"‚úÖ DEBUG: –ö–æ–ª–æ–Ω–∫–∏ lemmatized –∏ normalized —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ")
            else:
                print_stage(analyzer, f"‚ùå DEBUG: –û–®–ò–ë–ö–ê! –ö–æ–ª–æ–Ω–∫–∏ lemmatized/normalized –ù–ï —Å–æ–∑–¥–∞–Ω—ã!")
        else:
            print_stage(analyzer, f"\n‚ö° –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞ - –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞")
    
    print()


def _sync_csv_from_cache_if_needed(args, analyzer, print_stage):
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç CSV —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –∫—ç—à–∞ –µ—Å–ª–∏ —ç—Ç–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è
    
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç: –µ—Å–ª–∏ CSV —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª—å—à–µ –∑–∞–ø–∏—Å–µ–π —á–µ–º –≤ –∫—ç—à–µ,
    –∑–Ω–∞—á–∏—Ç –≤ CSV –µ—â—ë –µ—Å—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –µ–≥–æ –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å
    """
    # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
    input_file = None
    
    if hasattr(analyzer, 'current_group') and analyzer.current_group:
        input_file = analyzer.current_group.input_file
    elif hasattr(args, 'input_file') and args.input_file:
        input_file = Path(args.input_file)
        if not input_file.is_absolute():
            input_file = Path.cwd() / input_file
    
    if not input_file or not input_file.exists():
        return
    
    # –ß–∏—Ç–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π CSV —á—Ç–æ–±—ã —Å—Ä–∞–≤–Ω–∏—Ç—å —Ä–∞–∑–º–µ—Ä
    try:
        import pandas as pd
        original_df = pd.read_csv(input_file, encoding='utf-8-sig')
        original_count = len(original_df)
        cached_count = len(analyzer.df)
        
        # –ï—Å–ª–∏ –≤ CSV –±–æ–ª—å—à–µ –∑–∞–ø–∏—Å–µ–π —á–µ–º –≤ –∫—ç—à–µ - –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å
        if original_count > cached_count:
            duplicates_in_csv = original_count - cached_count
            
            print_stage(analyzer, f"\n‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ:")
            print_stage(analyzer, f"   üìÑ –í CSV —Ñ–∞–π–ª–µ: {original_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
            print_stage(analyzer, f"   üíæ –í –∫—ç—à–µ: {cached_count} –∑–∞–ø—Ä–æ—Å–æ–≤ (–±–µ–∑ –¥—É–±–ª–µ–π)")
            print_stage(analyzer, f"   üóëÔ∏è  –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ CSV: {duplicates_in_csv}")
            print_stage(analyzer, f"\nüíæ –ü–µ—Ä–µ–∑–∞–ø–∏—Å—å CSV —Å –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏...")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            success = save_filtered_queries(
                analyzer.df,
                input_file,
                backup=True
            )
            
            if success:
                print_stage(analyzer, f"‚úì CSV —Ñ–∞–π–ª —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –∫—ç—à–µ–º")
                print_stage(analyzer, f"‚úì –£–¥–∞–ª–µ–Ω–æ {duplicates_in_csv} –¥—É–±–ª–µ–π –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")
            else:
                print_stage(analyzer, f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å CSV —Ñ–∞–π–ª")
    
    except Exception as e:
        # –¢–∏—Ö–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ CSV
        pass

