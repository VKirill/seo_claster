"""–≠—Ç–∞–ø 2: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"""

from pathlib import Path
from seo_analyzer.core.normalizer import QueryNormalizer
from seo_analyzer.clustering.geo_processor import AsyncGeoProcessor
from .stage_logger import get_group_prefix, print_stage
from .preprocessing.filter_handler import FilterHandler
from .preprocessing.normalization_handler import NormalizationHandler
from .preprocessing.extraction_handler import ExtractionHandler
from .preprocessing.deduplication_handler import DeduplicationHandler
from .preprocessing.cache_handler import CacheHandler


async def preprocessing_stage(args, analyzer):
    """
    –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
    
    Args:
        args: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
        analyzer: –≠–∫–∑–µ–º–ø–ª—è—Ä SEOAnalyzer
        
    Returns:
        None (–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ analyzer)
    """
    prefix = get_group_prefix(analyzer)
    print_stage(analyzer, "üîß –≠–¢–ê–ü 2: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞")
    print_stage(analyzer, "-" * 80)
    
    # Debug: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    loaded_from_cache = getattr(analyzer, 'loaded_from_cache', False)
    loaded_from_master_db = getattr(analyzer, 'loaded_from_master_db', False)
    print_stage(analyzer, f"üîç DEBUG: loaded_from_cache = {loaded_from_cache}, loaded_from_master_db = {loaded_from_master_db}")
    print_stage(analyzer, f"üîç DEBUG: –ó–∞–ø—Ä–æ—Å–æ–≤ = {len(analyzer.df)}")
    print_stage(analyzer, f"üîç DEBUG: –ö–æ–ª–æ–Ω–∫–∏ –Ω–∞ –≤—Ö–æ–¥–µ: {list(analyzer.df.columns)[:10]}...")
    
    # –ï—Å–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ Master DB - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –í–°–Æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É
    if loaded_from_master_db:
        print_stage(analyzer, "‚úÖ –î–∞–Ω–Ω—ã–µ –∏–∑ Master DB - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É (—É–∂–µ –≥–æ—Ç–æ–≤–æ)")
        print_stage(analyzer, f"   –î–æ—Å—Ç—É–ø–Ω–æ –∫–æ–ª–æ–Ω–æ–∫: {len(analyzer.df.columns)}")
        print_stage(analyzer, f"   –ò–Ω—Ç–µ–Ω—Ç: {'‚úì' if 'main_intent' in analyzer.df.columns else '‚úó'}")
        print_stage(analyzer, f"   SERP: {'‚úì' if 'serp_found_docs' in analyzer.df.columns else '‚úó'}")
        print_stage(analyzer, f"   SERP URLs: {'‚úì' if 'serp_top_urls' in analyzer.df.columns else '‚úó'}")
        print()
        return
    
    # –ï—Å–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ query_cache - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∞—Å—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
    if loaded_from_cache:
        _handle_cached_data(args, analyzer, print_stage)
        return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤
    filter_handler = FilterHandler(analyzer.stopwords)
    normalization_handler = NormalizationHandler()
    extraction_handler = ExtractionHandler()
    deduplication_handler = DeduplicationHandler()
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
    analyzer.df = filter_handler.apply_filters(analyzer.df, args, lambda msg: print_stage(analyzer, msg))
    analyzer.stopwords_filter = filter_handler.stopwords_filter
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞: –æ—Å—Ç–∞–ª–∏—Å—å –ª–∏ –∑–∞–ø—Ä–æ—Å—ã –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    if len(analyzer.df) == 0:
        print_stage(analyzer, "\n‚ö†Ô∏è  –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã –±—ã–ª–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã!")
        print_stage(analyzer, "‚ö†Ô∏è  –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
        print_stage(analyzer, "    - –£–º–µ–Ω—å—à–∏—Ç—å max_frequency_ratio (--max-frequency-ratio)")
        print_stage(analyzer, "    - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–π CSV —Ñ–∞–π–ª —Å –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏")
        print()
        
        # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –æ—à–∏–±–æ–∫ –≤ –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö —ç—Ç–∞–ø–∞—Ö
        for col in ['normalized', 'lemmatized', 'words_count', 'has_latin', 'has_numbers',
                    'main_words', 'key_phrase', 'ner_entities', 'ner_locations']:
            if col not in analyzer.df.columns:
                analyzer.df[col] = []
        return
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    queries_list = analyzer.df['keyword'].tolist()
    normalized_results = await normalization_handler.normalize_queries(queries_list, lambda msg: print_stage(analyzer, msg))
    analyzer.df = normalization_handler.apply_normalization_to_df(analyzer.df, normalized_results)
    analyzer.normalizer = normalization_handler.normalizer
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ –∏ NER
    analyzer.df = await extraction_handler.extract_key_phrases(queries_list, analyzer.df, lambda msg: print_stage(analyzer, msg))
    analyzer.df = await extraction_handler.extract_ner(queries_list, analyzer.df, lambda msg: print_stage(analyzer, msg))
    
    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
    analyzer.df, stats = deduplication_handler.deduplicate_exact(analyzer.df, lambda msg: print_stage(analyzer, msg))
    analyzer.deduplicator = deduplication_handler.deduplicator
    
    analyzer.df, adv_stats = deduplication_handler.deduplicate_advanced(analyzer.df, lambda msg: print_stage(analyzer, msg))
    analyzer.removed_implicit_duplicates = deduplication_handler.removed_implicit_duplicates
    analyzer.advanced_deduplicator = deduplication_handler.advanced_deduplicator
    
    # –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏
    _start_geo_processing(analyzer, print_stage)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if not getattr(analyzer, 'loaded_from_cache', False):
        CacheHandler.save_filtered_to_csv(args, analyzer, lambda msg: print_stage(analyzer, msg))
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
    if hasattr(analyzer, 'query_cache') and hasattr(analyzer, 'current_group') and analyzer.current_group:
        if not getattr(analyzer, 'loaded_from_cache', False):
            total_duplicates = stats['total_duplicates_removed'] + adv_stats['total_duplicates_removed']
            CacheHandler.save_to_cache(analyzer, total_duplicates, lambda msg: print_stage(analyzer, msg))
            
            # Debug: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∏ —Å–æ–∑–¥–∞–Ω—ã
            print_stage(analyzer, f"üîç DEBUG: –ö–æ–ª–æ–Ω–∫–∏ –Ω–∞ –≤—ã—Ö–æ–¥–µ: {list(analyzer.df.columns)[:15]}...")
            if 'lemmatized' in analyzer.df.columns and 'normalized' in analyzer.df.columns:
                print_stage(analyzer, f"‚úÖ DEBUG: –ö–æ–ª–æ–Ω–∫–∏ lemmatized –∏ normalized —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ")
            else:
                print_stage(analyzer, f"‚ùå DEBUG: –û–®–ò–ë–ö–ê! –ö–æ–ª–æ–Ω–∫–∏ lemmatized/normalized –ù–ï —Å–æ–∑–¥–∞–Ω—ã!")
        else:
            print_stage(analyzer, f"\n‚ö° –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞ - –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞")
    
    print()


def _handle_cached_data(args, analyzer, print_stage):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞"""
    print_stage(analyzer, "‚ö° –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞")
    print_stage(analyzer, "‚úì –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞ (normalized, lemmatized, NER, key phrases —É–∂–µ –≤ –∫—ç—à–µ)")
    print_stage(analyzer, f"‚úì –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(analyzer.df)}")
    
    if hasattr(analyzer, 'query_cache') and hasattr(analyzer, 'current_group'):
        stats = analyzer.query_cache.get_group_stats(analyzer.current_group.name)
        if stats:
            print_stage(analyzer, f"‚úì –î—É–±–ª–∏–∫–∞—Ç–æ–≤ —É–¥–∞–ª–µ–Ω–æ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ: {stats['duplicates_removed']}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏
    print_stage(analyzer, "\nüåç –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ...")
    _start_geo_processing(analyzer, print_stage)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    missing_critical = []
    if 'normalized' not in analyzer.df.columns:
        missing_critical.append('normalized')
    if 'lemmatized' not in analyzer.df.columns:
        missing_critical.append('lemmatized')
    
    if missing_critical:
        print_stage(analyzer, f"\n‚ö†Ô∏è  –ö—Ä–∏—Ç–∏—á–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç: {', '.join(missing_critical)}")
        print_stage(analyzer, "üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–ª–æ–Ω–æ–∫...")
        
        normalizer = QueryNormalizer()
        normalized_results = normalizer.normalize_batch(analyzer.df['keyword'].tolist())
        
        if 'normalized' not in analyzer.df.columns:
            analyzer.df['normalized'] = [r['normalized'] for r in normalized_results]
        if 'lemmatized' not in analyzer.df.columns:
            analyzer.df['lemmatized'] = [r['lemmatized'] for r in normalized_results]
        
        print_stage(analyzer, "‚úì –ö–æ–ª–æ–Ω–∫–∏ —Å–æ–∑–¥–∞–Ω—ã")
        
        # –ü–µ—Ä–µ—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫—ç—à
        if hasattr(analyzer, 'query_cache') and hasattr(analyzer, 'current_group'):
            print_stage(analyzer, "üíæ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞...")
            analyzer.query_cache.save_queries(
                group_name=analyzer.current_group.name,
                csv_path=analyzer.current_group.input_file,
                df=analyzer.df,
                duplicates_removed=stats.get('duplicates_removed', 0) if stats else 0
            )
            print_stage(analyzer, "‚úì –ö—ç—à –æ–±–Ω–æ–≤–ª–µ–Ω")
    
    # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è CSV
    CacheHandler.sync_csv_from_cache_if_needed(args, analyzer, lambda msg: print_stage(analyzer, msg))
    print()


def _start_geo_processing(analyzer, print_stage):
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ"""
    from seo_analyzer.clustering.semantic_checker import SemanticClusterChecker
    
    semantic_checker = SemanticClusterChecker(geo_dicts=analyzer.geo_dicts)
    analyzer.geo_processor = AsyncGeoProcessor(
        semantic_checker=semantic_checker,
        max_workers=4
    )
    
    queries_list = analyzer.df['keyword'].tolist()
    analyzer.geo_processor.start_processing(queries_list)
    
    print_stage(analyzer, f"‚úì –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(queries_list)} –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–∞–ø—É—â–µ–Ω–∞ (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –¥—Ä—É–≥–∏–º–∏ —ç—Ç–∞–ø–∞–º–∏)")
    print_stage(analyzer, "  üí° –ì–µ–æ–≥—Ä–∞—Ñ–∏—è –±—É–¥–µ—Ç –≥–æ—Ç–æ–≤–∞ –∫ –º–æ–º–µ–Ω—Ç—É –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")


# –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def _sync_csv_from_cache_if_needed(args, analyzer, print_stage):
    """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç CSV —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –∫—ç—à–∞ –µ—Å–ª–∏ —ç—Ç–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è"""
    CacheHandler.sync_csv_from_cache_if_needed(args, analyzer, lambda msg: print_stage(analyzer, msg))
