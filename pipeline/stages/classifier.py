"""–≠—Ç–∞–ø 3: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤"""

import pandas as pd
from seo_analyzer.classification.intent_classifier import IntentClassifier
from seo_analyzer.classification.brand_detector import BrandDetector
from seo_analyzer.classification.funnel_classifier import FunnelClassifier
from seo_analyzer.clustering.structure_clusterer import StructureClusterer
from seo_analyzer.classification.page_mapper import PageMapper
from seo_analyzer.classification.intent.serp_offer_classifier import SERPOfferClassifier
from .stage_logger import get_group_prefix, print_stage, print_stage_header


async def classification_stage(args, analyzer):
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
    
    Args:
        args: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
        analyzer: –≠–∫–∑–µ–º–ø–ª—è—Ä SEOAnalyzer
        
    Returns:
        None (–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ analyzer)
    """
    prefix = get_group_prefix(analyzer)
    print(f"{prefix}üè∑Ô∏è  –≠–¢–ê–ü 6: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (—Å —É—á–µ—Ç–æ–º SERP + –∫–ª–∞—Å—Ç–µ—Ä–æ–≤)")
    print(f"{prefix}{'-' * 80}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ DataFrame –Ω–µ –ø—É—Å—Ç–æ–π
    if len(analyzer.df) == 0:
        print_stage(analyzer, "‚ö†Ô∏è  DataFrame –ø—É—Å—Ç–æ–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é")
        print()
        return
    
    # –í–°–ï–ì–î–ê –≤—ã–ø–æ–ª–Ω—è–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é (–¥–∞–∂–µ –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑ Master DB)
    # –≠—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –ø—Ä–∞–≤–∏–ª
    # –£–±—Ä–∞–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—Å–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑ Master DB
    # loaded_from_master_db = getattr(analyzer, 'loaded_from_master_db', False)
    # if loaded_from_master_db and 'main_intent' in analyzer.df.columns:
    #     # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∞
    #     total = len(analyzer.df)
    #     with_intent = analyzer.df['main_intent'].notna().sum()
    #     
    #     # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 50% –∑–∞–ø—Ä–æ—Å–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑ –∫—ç—à–∞
    #     if with_intent > total * 0.5:
    #         print_stage(analyzer, "‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑ Master DB (—É–∂–µ –≥–æ—Ç–æ–≤–æ)")
    #         
    #         with_brands = analyzer.df['detected_brand'].notna().sum() if 'detected_brand' in analyzer.df.columns else 0
    #         with_funnel = analyzer.df['funnel_stage'].notna().sum() if 'funnel_stage' in analyzer.df.columns else 0
    #         
    #         print_stage(analyzer, f"  ‚úì –ò–Ω—Ç–µ–Ω—Ç: {with_intent}/{total}")
    #         print_stage(analyzer, f"  ‚úì –ë—Ä–µ–Ω–¥—ã: {with_brands}/{total}")
    #         print_stage(analyzer, f"  ‚úì –í–æ—Ä–æ–Ω–∫–∞: {with_funnel}/{total}")
    #         print()
    #         return
    #     else:
    #         print_stage(analyzer, f"‚ö†Ô∏è  –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑ Master DB –Ω–µ–ø–æ–ª–Ω–∞—è ({with_intent}/{total}), –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –∏ —Å–æ–∑–¥–∞–µ–º –∏—Ö –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç
    # (–º–æ–≥—É—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ –∫—ç—à–∞ –∏–ª–∏ –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –ë–î)
    missing_columns = []
    if 'lemmatized' not in analyzer.df.columns:
        missing_columns.append('lemmatized')
    if 'normalized' not in analyzer.df.columns:
        missing_columns.append('normalized')
    
    if missing_columns:
        # –°–æ–∑–¥–∞–µ–º –æ–±–µ –∫–æ–ª–æ–Ω–∫–∏ –æ–¥–Ω–∏–º –ø—Ä–æ—Ö–æ–¥–æ–º (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ)
        from seo_analyzer.core.normalizer import QueryNormalizer
        normalizer = QueryNormalizer()
        print_stage(analyzer, f"üîÑ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ ({', '.join(missing_columns)} –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞–ª–∏)...")
        
        normalized_results = normalizer.normalize_batch(analyzer.df['keyword'].tolist())
        
        if 'normalized' not in analyzer.df.columns:
            analyzer.df['normalized'] = [r['normalized'] for r in normalized_results]
        if 'lemmatized' not in analyzer.df.columns:
            analyzer.df['lemmatized'] = [r['lemmatized'] for r in normalized_results]
        
        print_stage(analyzer, "‚úì –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Ç–∞ (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∞)
    intent_columns = ['main_intent', 'commercial_score', 'informational_score', 'navigational_score']
    has_cached_intent = all(col in analyzer.df.columns and analyzer.df[col].notna().any() for col in intent_columns)
    
    if has_cached_intent:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤ —É–∂–µ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã
        cached_count = analyzer.df['main_intent'].notna().sum()
        total_count = len(analyzer.df)
        
        if cached_count == total_count:
            print_stage(analyzer, f"‚úì –ò–Ω—Ç–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –∫—ç—à–∞ ({cached_count} –∑–∞–ø—Ä–æ—Å–æ–≤)")
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º intent_results –∏–∑ DataFrame –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            # (–Ω—É–∂–Ω–æ –¥–ª—è —Ñ–ª–∞–≥–æ–≤ –∏ –≥–µ–æ-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)
            intent_results = []
            for idx in analyzer.df.index:
                result = {
                    'main_intent': analyzer.df.at[idx, 'main_intent'],
                    'commercial_score': analyzer.df.at[idx, 'commercial_score'],
                    'informational_score': analyzer.df.at[idx, 'informational_score'],
                    'navigational_score': analyzer.df.at[idx, 'navigational_score'],
                    'has_geo': analyzer.df.at[idx, 'has_geo'] if 'has_geo' in analyzer.df.columns else False,
                    'geo_type': analyzer.df.at[idx, 'geo_type'] if 'geo_type' in analyzer.df.columns else None,
                    'geo_country': analyzer.df.at[idx, 'geo_country'] if 'geo_country' in analyzer.df.columns else None,
                    'geo_city': analyzer.df.at[idx, 'geo_city'] if 'geo_city' in analyzer.df.columns else None,
                }
                # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–ª–∞–≥–∏ –∏–∑ keyword_dicts
                for dict_key in analyzer.keyword_dicts.keys():
                    flag_name = analyzer.keyword_dicts[dict_key].get('flag')
                    if flag_name:
                        result[flag_name] = analyzer.df.at[idx, flag_name] if flag_name in analyzer.df.columns else False
                intent_results.append(result)
        else:
            print_stage(analyzer, f"üîÑ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Ç–∞ ({cached_count}/{total_count} –∏–∑ –∫—ç—à–∞)...")
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —É –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –∏–Ω—Ç–µ–Ω—Ç–∞
            analyzer.intent_classifier = IntentClassifier(
                analyzer.keyword_dicts, 
                analyzer.geo_dicts,
                analyzer.intent_weights
            )
            
            # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã –±–µ–∑ –∏–Ω—Ç–µ–Ω—Ç–∞
            missing_intent = analyzer.df['main_intent'].isna()
            if missing_intent.any():
                queries_to_classify = analyzer.df.loc[missing_intent, 'keyword'].tolist()
                lemmatized_queries = None
                if 'lemmatized' in analyzer.df.columns:
                    lemmatized_queries = analyzer.df.loc[missing_intent, 'lemmatized'].tolist()
                
                new_intent_results = analyzer.intent_classifier.classify_batch(
                    queries_to_classify,
                    lemmatized_queries=lemmatized_queries
                )
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏
                for i, idx in enumerate(analyzer.df[missing_intent].index):
                    for key in intent_columns:
                        analyzer.df.at[idx, key] = new_intent_results[i][key]
                
                print_stage(analyzer, f"‚úì –ò–Ω—Ç–µ–Ω—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω ({len(queries_to_classify)} –Ω–æ–≤—ã—Ö)")
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º intent_results –∏–∑ DataFrame (–æ–±—ä–µ–¥–∏–Ω—è–µ–º –∫—ç—à + –Ω–æ–≤—ã–µ)
            intent_results = []
            for idx in analyzer.df.index:
                result = {
                    'main_intent': analyzer.df.at[idx, 'main_intent'],
                    'commercial_score': analyzer.df.at[idx, 'commercial_score'],
                    'informational_score': analyzer.df.at[idx, 'informational_score'],
                    'navigational_score': analyzer.df.at[idx, 'navigational_score'],
                    'has_geo': analyzer.df.at[idx, 'has_geo'] if 'has_geo' in analyzer.df.columns else False,
                    'geo_type': analyzer.df.at[idx, 'geo_type'] if 'geo_type' in analyzer.df.columns else None,
                    'geo_country': analyzer.df.at[idx, 'geo_country'] if 'geo_country' in analyzer.df.columns else None,
                    'geo_city': analyzer.df.at[idx, 'geo_city'] if 'geo_city' in analyzer.df.columns else None,
                }
                # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–ª–∞–≥–∏ –∏–∑ keyword_dicts
                for dict_key in analyzer.keyword_dicts.keys():
                    flag_name = analyzer.keyword_dicts[dict_key].get('flag')
                    if flag_name:
                        result[flag_name] = analyzer.df.at[idx, flag_name] if flag_name in analyzer.df.columns else False
                intent_results.append(result)
    else:
        # –ö—ç—à–∞ –Ω–µ—Ç - –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –≤—Å–µ
        print_stage(analyzer, "üîÑ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Ç–∞...")
        analyzer.intent_classifier = IntentClassifier(
            analyzer.keyword_dicts, 
            analyzer.geo_dicts,
            analyzer.intent_weights
        )
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≥–æ—Ä–æ–¥–æ–≤
        lemmatized_queries = None
        if 'lemmatized' in analyzer.df.columns:
            lemmatized_queries = analyzer.df['lemmatized'].tolist()
        
        intent_results = analyzer.intent_classifier.classify_batch(
            analyzer.df['keyword'].tolist(),
            lemmatized_queries=lemmatized_queries
        )
        
        for key in intent_columns:
            analyzer.df[key] = [r[key] for r in intent_results]
        
        print_stage(analyzer, f"‚úì –ò–Ω—Ç–µ–Ω—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω")
    
    # –§–ª–∞–≥–∏
    for key in analyzer.keyword_dicts.keys():
        flag_name = analyzer.keyword_dicts[key].get('flag')
        if flag_name:
            analyzer.df[flag_name] = [r.get(flag_name, False) for r in intent_results]
    
    # –ì–µ–æ-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    for key in ['has_geo', 'geo_type', 'geo_country', 'geo_city']:
        analyzer.df[key] = [r.get(key) for r in intent_results]
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –≥–æ—Ä–æ–¥–æ–≤ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ —ç–∫—Å–ø–æ—Ä—Ç–∞
    # (—Å–ø–± ‚Üí –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, –º—Å–∫ ‚Üí –ú–æ—Å–∫–≤–∞, –º–æ—Å–∫–≤–∞ ‚Üí –ú–æ—Å–∫–≤–∞)
    from seo_analyzer.core.city_alias_loader import normalize_city_name
    analyzer.df['geo_city'] = analyzer.df['geo_city'].apply(
        lambda city: normalize_city_name(city) if city else city
    )
    
    print_stage(analyzer, f"‚úì –ò–Ω—Ç–µ–Ω—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω")
    
    # –°—á–µ—Ç—á–∏–∫ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∏–Ω—Ç–µ–Ω—Ç–æ–≤ (–¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ë–î –≤ –∫–æ–Ω—Ü–µ)
    changed_count = 0
    
    # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –∏–Ω—Ç–µ–Ω—Ç–∞ –ø–æ SERP offer_info (–ü–†–ò–û–†–ò–¢–ï–¢–ù–´–ô –º–µ—Ç–æ–¥)
    if 'serp_xml' in analyzer.df.columns or 'xml_response' in analyzer.df.columns:
        print_stage(analyzer, "üîÑ –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –∏–Ω—Ç–µ–Ω—Ç–∞ –ø–æ SERP offer_info...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å XML
        xml_column = 'serp_xml' if 'serp_xml' in analyzer.df.columns else 'xml_response'
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–º–µ—é—Ç XML –¥–∞–Ω–Ω—ã–µ
        has_xml = analyzer.df[xml_column].notna().sum()
        
        if has_xml > 0:
            serp_classifier = SERPOfferClassifier(
                top_n=20,
                commercial_threshold=7,  # >= 7 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å offer_info ‚Üí commercial
                commercial_ratio=0.4      # –∏–ª–∏ >= 40% –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            )
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–æ XML
            xml_list = analyzer.df[xml_column].fillna('').tolist()
            queries_list = analyzer.df['keyword'].tolist()
            serp_results = serp_classifier.classify_batch(xml_list, queries_list)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DataFrame
            for key in ['serp_intent', 'serp_confidence', 'serp_docs_with_offers', 
                       'serp_total_docs', 'serp_offer_ratio',
                       'serp_avg_price', 'serp_min_price', 'serp_max_price', 
                       'serp_median_price', 'serp_currency', 'serp_offers_count',
                       'serp_offers_with_discount', 'serp_avg_discount_percent']:
                analyzer.df[key] = [r[key] for r in serp_results]
            
            # –ü–†–ò–û–†–ò–¢–ï–¢: –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ —Å–ª–æ–≤–∞ (–∫—É–ø–∏—Ç—å, –∑–∞–∫–∞–∑–∞—Ç—å) > SERP offer_info
            # –ï—Å–ª–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ –µ—Å—Ç—å —è–≤–Ω–æ–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ —Å–ª–æ–≤–æ - –∏–Ω—Ç–µ–Ω—Ç –í–°–ï–ì–î–ê commercial
            commercial_keywords = analyzer.keyword_dicts.get('commercial', {}).get('words', set())
            
            import re
            # –°–æ–∑–¥–∞–µ–º pattern –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ü–µ–ª—ã—Ö —Å–ª–æ–≤ (—Å –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ —Å–ª–æ–≤)
            commercial_patterns = [
                re.compile(r'\b' + re.escape(word.lower()) + r'\b', re.IGNORECASE)
                for word in commercial_keywords
            ]
            
            for idx in analyzer.df.index:
                if analyzer.df.at[idx, xml_column] and pd.notna(analyzer.df.at[idx, xml_column]):
                    serp_intent = analyzer.df.at[idx, 'serp_intent']
                    current_intent = analyzer.df.at[idx, 'main_intent']
                    query = analyzer.df.at[idx, 'keyword'].lower()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ —Å–ª–æ–≤–æ –≤ –∑–∞–ø—Ä–æ—Å–µ (—Ü–µ–ª–æ–µ —Å–ª–æ–≤–æ, –Ω–µ –ø–æ–¥—Å—Ç—Ä–æ–∫–∞)
                    has_commercial_word = any(pattern.search(query) for pattern in commercial_patterns)
                    
                    # –í–ê–ñ–ù–û: –ï—Å–ª–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ –µ—Å—Ç—å –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ —Å–ª–æ–≤–æ (–∫—É–ø–∏—Ç—å, –∑–∞–∫–∞–∑–∞—Ç—å –∏ —Ç.–¥.),
                    # —Ç–æ –∏–Ω—Ç–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å commercial, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ –∏–Ω—Ç–µ–Ω—Ç–∞ –∏ SERP –¥–∞–Ω–Ω—ã—Ö
                    if has_commercial_word:
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ–Ω—Ç (—Å —É—á–µ—Ç–æ–º –≥–µ–æ)
                        if current_intent in ['informational_geo', 'commercial_geo']:
                            correct_intent = 'commercial_geo'
                        else:
                            correct_intent = 'commercial'
                        
                        # –ï—Å–ª–∏ —Ç–µ–∫—É—â–∏–π –∏–Ω—Ç–µ–Ω—Ç –Ω–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π - –∏—Å–ø—Ä–∞–≤–ª—è–µ–º
                        if current_intent != correct_intent:
                            analyzer.df.at[idx, 'main_intent'] = correct_intent
                            changed_count += 1
                    # –ï—Å–ª–∏ –Ω–µ—Ç –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ —Å–ª–æ–≤–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º SERP –¥–∞–Ω–Ω—ã–µ
                    elif serp_intent != current_intent:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –≥–µ–æ –≤ –∑–∞–ø—Ä–æ—Å–µ
                        has_geo = analyzer.df.at[idx, 'has_geo']
                        
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∏–Ω—Ç–µ–Ω—Ç —Å —É—á–µ—Ç–æ–º –≥–µ–æ
                        if has_geo:
                            # –ï—Å–ª–∏ –µ—Å—Ç—å –≥–µ–æ, –¥–æ–±–∞–≤–ª—è–µ–º —Å—É—Ñ—Ñ–∏–∫—Å _geo
                            if serp_intent == 'commercial':
                                correct_intent = 'commercial_geo'
                            elif serp_intent == 'informational':
                                correct_intent = 'informational_geo'
                            else:
                                correct_intent = serp_intent  # navigational –∏ –¥—Ä—É–≥–∏–µ –±–µ–∑ _geo
                        else:
                            correct_intent = serp_intent
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ç–µ–Ω—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –∏–∑–º–µ–Ω–∏–ª—Å—è
                        if current_intent != correct_intent:
                            analyzer.df.at[idx, 'main_intent'] = correct_intent
                            changed_count += 1
            
            print_stage(analyzer, f"‚úì –ò–Ω—Ç–µ–Ω—Ç —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω –ø–æ SERP offer_info (–∏–∑–º–µ–Ω–µ–Ω–æ: {changed_count} –∏–∑ {has_xml})")
            print_stage(analyzer, f"  üìä –°—Ä–µ–¥–Ω—è—è –¥–æ–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å offer_info: {analyzer.df['serp_offer_ratio'].mean():.1%}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ü–µ–Ω–∞–º (—Ç–æ–ª—å–∫–æ –¥–ª—è –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö)
            commercial_queries = analyzer.df[analyzer.df['serp_intent'] == 'commercial']
            if len(commercial_queries) > 0:
                avg_prices = commercial_queries['serp_avg_price'].dropna()
                if len(avg_prices) > 0:
                    print_stage(analyzer, f"  üí∞ –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —Ü–µ–Ω–∞–º–∏: {len(avg_prices)}")
                    print_stage(analyzer, f"  üí∞ –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ –≤ –≤—ã–¥–∞—á–µ: {avg_prices.mean():.0f} {commercial_queries['serp_currency'].mode().iloc[0] if not commercial_queries['serp_currency'].mode().empty else 'RUR'}")
                    print_stage(analyzer, f"  üí∞ –î–∏–∞–ø–∞–∑–æ–Ω: {avg_prices.min():.0f} - {avg_prices.max():.0f}")
        else:
            print_stage(analyzer, "‚ö†Ô∏è  SERP XML –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞")
    else:
        print_stage(analyzer, "‚ö†Ô∏è  –ö–æ–ª–æ–Ω–∫–∞ —Å SERP XML –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞")
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±—Ä–µ–Ω–¥–æ–≤ (–û–¢–ö–õ–Æ–ß–ï–ù–û - —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ)
    # print_stage(analyzer, "üîÑ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±—Ä–µ–Ω–¥–æ–≤...")
    # analyzer.brand_detector = BrandDetector(analyzer.geo_dicts)
    # brand_results = analyzer.brand_detector.detect_batch(analyzer.df['keyword'].tolist())
    
    # analyzer.df['detected_brand'] = [r['detected_brand'] for r in brand_results]
    # analyzer.df['brand_confidence'] = [r['brand_confidence'] for r in brand_results]
    # analyzer.df['is_brand_query'] = [r['is_brand_query'] for r in brand_results]
    
    # print_stage(analyzer, f"‚úì –ù–∞–π–¥–µ–Ω–æ {sum(analyzer.df['is_brand_query'])} –±—Ä–µ–Ω–¥–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    analyzer.df['detected_brand'] = None
    analyzer.df['brand_confidence'] = 0.0
    analyzer.df['is_brand_query'] = False
    
    print_stage(analyzer, "‚ö†Ô∏è  –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±—Ä–µ–Ω–¥–æ–≤ –æ—Ç–∫–ª—é—á–µ–Ω–æ (–¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)")
    
    # –í–æ—Ä–æ–Ω–∫–∞ –ø—Ä–æ–¥–∞–∂
    print_stage(analyzer, "üîÑ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –≤–æ—Ä–æ–Ω–∫–µ...")
    analyzer.funnel_classifier = FunnelClassifier()
    funnel_results = analyzer.funnel_classifier.classify_batch(analyzer.df['keyword'].tolist())
    
    analyzer.df['funnel_stage'] = [r['funnel_stage'] for r in funnel_results]
    analyzer.df['funnel_priority'] = [r['funnel_priority'] for r in funnel_results]
    
    print_stage(analyzer, f"‚úì –í–æ—Ä–æ–Ω–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–∞")
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    print_stage(analyzer, "üîÑ –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã...")
    analyzer.structure_clusterer = StructureClusterer()
    analyzer.df = analyzer.structure_clusterer.extract_structural_features(analyzer.df)
    
    # –¶–µ–ª–µ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    print_stage(analyzer, "üîÑ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü...")
    analyzer.page_mapper = PageMapper()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ DataFrame –Ω–µ –ø—É—Å—Ç–æ–π
    if len(analyzer.df) == 0:
        print_stage(analyzer, "‚ö†Ô∏è  DataFrame –ø—É—Å—Ç–æ–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü")
        # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        analyzer.df['target_page_type'] = pd.Series(dtype=str)
        analyzer.df['suggested_url'] = pd.Series(dtype=str)
    else:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º apply –≤–º–µ—Å—Ç–æ iterrows (–≤ 5-10 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ)
        def map_row_to_page(row):
            page_info = analyzer.page_mapper.map_query_to_page(
                row['keyword'],
                intent=row.get('main_intent'),
                has_brand=row.get('is_brand_query', False),
                has_geo=row.get('has_geo', False),
                funnel_stage=row.get('funnel_stage'),
                brand=row.get('detected_brand'),
                city=row.get('geo_city')
            )
            return pd.Series({
                'target_page_type': page_info['target_page_type'],
                'suggested_url': page_info['suggested_url']
            })
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ
        page_results = analyzer.df.apply(map_row_to_page, axis=1)
        analyzer.df['target_page_type'] = page_results['target_page_type']
        analyzer.df['suggested_url'] = page_results['suggested_url']
        
        print_stage(analyzer, f"‚úì –¶–µ–ª–µ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ç–µ–Ω—Ç—ã –≤ –ë–î (–µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω—ã)
    if changed_count > 0 and hasattr(analyzer, 'current_group') and analyzer.current_group:
        try:
            from seo_analyzer.core.cache.master_query_db import MasterQueryDatabase
            master_db = MasterQueryDatabase()
            group_name = analyzer.current_group.name
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –∑–∞–ø—Ä–æ—Å—ã, —É –∫–æ—Ç–æ—Ä—ã—Ö –∏–∑–º–µ–Ω–∏–ª—Å—è –∏–Ω—Ç–µ–Ω—Ç
            updated = master_db.update_intents_from_dataframe(group_name, analyzer.df)
            if updated > 0:
                print_stage(analyzer, f"‚úì –û–±–Ω–æ–≤–ª–µ–Ω–æ {updated} –∏–Ω—Ç–µ–Ω—Ç–æ–≤ –≤ –ë–î")
        except Exception as e:
            print_stage(analyzer, f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–Ω—Ç–µ–Ω—Ç–æ–≤ –≤ –ë–î: {e}")
    
    print()

