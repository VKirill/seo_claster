"""–≠—Ç–∞–ø 4.6: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ SERP –¥–∞–Ω–Ω—ã—Ö"""

from seo_analyzer.clustering.serp_clusterer import SERPClusterer
from seo_analyzer.clustering.serp_advanced_clusterer import AdvancedSERPClusterer
from seo_analyzer.clustering.iterative_serp_clusterer import IterativeSERPClusterer
from seo_analyzer.clustering.word_match_clusterer import WordMatchClusterer
from seo_analyzer.core.config import CLUSTERING_CONFIG
from .stage_logger import get_group_prefix, print_stage



async def clustering_stage(args, analyzer):
    """
    –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ SERP –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        args: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
        analyzer: –≠–∫–∑–µ–º–ø–ª—è—Ä SEOAnalyzer
        
    Returns:
        None (–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ analyzer)
    """
    prefix = get_group_prefix(analyzer)
    print_stage(analyzer, "üî¨ –≠–¢–ê–ü 4: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (–Ω–∞ –æ—Å–Ω–æ–≤–µ SERP)")
    print_stage(analyzer, "-" * 80)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ DataFrame –Ω–µ –ø—É—Å—Ç–æ–π
    if len(analyzer.df) == 0:
        print_stage(analyzer, "‚ö†Ô∏è  DataFrame –ø—É—Å—Ç–æ–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é")
        print()
        return
    
    # –í—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    use_legacy = getattr(args, 'use_legacy_serp', False)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π
    use_maxmin = getattr(args, 'maxmin', True)  # –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤–∫–ª—é—á–µ–Ω–æ)
    
    if use_maxmin:
        # –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É –ø–æ—Ä–æ–≥—É
        print_stage(analyzer, "üîÑ –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (–æ—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É –ø–æ—Ä–æ–≥—É)...")
        print_stage(analyzer, f"   –î–∏–∞–ø–∞–∑–æ–Ω –ø–æ—Ä–æ–≥–æ–≤: –æ—Ç 20 –¥–æ 4 –æ–±—â–∏—Ö URL")
        print_stage(analyzer, f"   –ê–ª–≥–æ—Ä–∏—Ç–º: —Å–Ω–∞—á–∞–ª–∞ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –∫–ª–∞—Å—Ç–µ—Ä—ã —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏ (20 URL),")
        print_stage(analyzer, f"              –∑–∞—Ç–µ–º –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è –ø–æ—Ä–æ–≥ –¥–æ 4 URL")
        
        serp_config = CLUSTERING_CONFIG.get('serp_advanced', {})
        
        iterative_clusterer = IterativeSERPClusterer(
            min_threshold=3,
            max_threshold=10,
            top_positions=getattr(args, 'serp_top_positions', serp_config.get('top_positions', 20)),
            max_cluster_size=getattr(args, 'max_cluster_size', serp_config.get('max_cluster_size', 100)),
            semantic_check=True,
            geo_dicts=analyzer.geo_dicts,
            verbose=True
        )
        
        # üåç –ü–µ—Ä–µ–¥–∞–µ–º geo_processor –µ—Å–ª–∏ –æ–Ω –±—ã–ª —Å–æ–∑–¥–∞–Ω —Ä–∞–Ω–µ–µ (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
        geo_processor = getattr(analyzer, 'geo_processor', None)
        
        analyzer.df = await iterative_clusterer.cluster_by_serp(
            analyzer.df,
            serp_column='serp_urls',
            geo_processor=geo_processor
        )
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = iterative_clusterer.get_cluster_stats()
        if stats and 'total_clusters' in stats:
            print_stage(analyzer, f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏:")
            print_stage(analyzer, f"  ‚Ä¢ –í—Å–µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {stats.get('total_clusters', 0)}")
            if 'avg_cluster_size' in stats:
                print_stage(analyzer, f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {stats['avg_cluster_size']:.1f}")
            if 'min_cluster_size' in stats and 'max_cluster_size' in stats:
                print_stage(analyzer, f"  ‚Ä¢ –ú–∏–Ω/–ú–∞–∫—Å: {stats['min_cluster_size']}/{stats['max_cluster_size']}")
            if 'singleton_clusters' in stats:
                print_stage(analyzer, f"  ‚Ä¢ –û–¥–∏–Ω–æ—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['singleton_clusters']}")
            
            # –í—ã–≤–æ–¥–∏–º —Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö URL –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–ø–µ—Ä–≤—ã–µ 5 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞)
            url_overlaps = iterative_clusterer.get_cluster_url_overlaps(analyzer.df, serp_column='serp_urls')
            if url_overlaps:
                print_stage(analyzer, f"\nüìä –¢–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö URL –≤ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö:")
                shown_clusters = 0
                for cluster_id, overlaps in sorted(url_overlaps.items())[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                    if overlaps:
                        cluster_queries = iterative_clusterer.cluster_queries.get(cluster_id, [])
                        print_stage(analyzer, f"  –ö–ª–∞—Å—Ç–µ—Ä {cluster_id + 1} (—Ä–∞–∑–º–µ—Ä: {len(cluster_queries)}):")
                        for overlap_info in overlaps[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –ø–∞—Ä—ã
                            q1 = overlap_info['query1'][:40]
                            q2 = overlap_info['query2'][:40]
                            overlap = overlap_info['overlap']
                            print_stage(analyzer, f"    ‚Ä¢ {q1} ‚Üî {q2}: {overlap} –æ–±—â–∏—Ö URL")
                        if len(overlaps) > 3:
                            print_stage(analyzer, f"    ... –∏ –µ—â–µ {len(overlaps) - 3} –ø–∞—Ä")
                        shown_clusters += 1
                if len(url_overlaps) > shown_clusters:
                    print_stage(analyzer, f"  ... –∏ –µ—â–µ {len(url_overlaps) - shown_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
    
    elif not use_legacy:
        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è SERP –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω–æ—Å—Ç–∏
        serp_config = CLUSTERING_CONFIG.get('serp_advanced', {})
        
        serp_clusterer = AdvancedSERPClusterer(
            min_common_urls=getattr(args, 'serp_similarity_threshold', serp_config.get('min_common_urls', 7)),
            top_positions=getattr(args, 'serp_top_positions', serp_config.get('top_positions', 20)),
            max_cluster_size=getattr(args, 'max_cluster_size', serp_config.get('max_cluster_size', 100)),
            mode=getattr(args, 'serp_mode', serp_config.get('mode', 'balanced')),
            semantic_check=True,  # –í–ö–õ–Æ–ß–ï–ù–û: –î–ª—è –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            min_cluster_cohesion=serp_config.get('min_cluster_cohesion', 0.6),
            geo_dicts=analyzer.geo_dicts  # –ü–µ—Ä–µ–¥–∞–µ–º –≥–µ–æ-—Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
        )
        
        # üåç –ü–µ—Ä–µ–¥–∞–µ–º geo_processor –µ—Å–ª–∏ –æ–Ω –±—ã–ª —Å–æ–∑–¥–∞–Ω —Ä–∞–Ω–µ–µ (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
        geo_processor = getattr(analyzer, 'geo_processor', None)
        
        analyzer.df = await serp_clusterer.cluster_by_serp(
            analyzer.df,
            serp_column='serp_urls',
            geo_processor=geo_processor
        )
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = serp_clusterer.get_cluster_stats()
        if stats and 'total_clusters' in stats:
            print_stage(analyzer, f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏:")
            print_stage(analyzer, f"  ‚Ä¢ –í—Å–µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {stats.get('total_clusters', 0)}")
            if 'avg_cluster_size' in stats:
                print_stage(analyzer, f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {stats['avg_cluster_size']:.1f}")
            if 'min_cluster_size' in stats and 'max_cluster_size' in stats:
                print_stage(analyzer, f"  ‚Ä¢ –ú–∏–Ω/–ú–∞–∫—Å: {stats['min_cluster_size']}/{stats['max_cluster_size']}")
            if 'singleton_clusters' in stats:
                print_stage(analyzer, f"  ‚Ä¢ –û–¥–∏–Ω–æ—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['singleton_clusters']}")
    else:
        # –°—Ç–∞—Ä—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        serp_clusterer = SERPClusterer(
            min_common_urls=args.serp_similarity_threshold,
            top_positions=args.serp_top_positions,
            max_cluster_size=getattr(args, 'max_cluster_size', 50)
        )
        
        # –°—Ç–∞—Ä—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç geo_processor, –Ω–æ –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å - –ø–æ–¥–æ–∂–¥–µ–º –µ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        geo_processor = getattr(analyzer, 'geo_processor', None)
        if geo_processor is not None:
            print_stage(analyzer, "  ‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ñ–æ–Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏...")
            await geo_processor.get_result()
        
        analyzer.df = serp_clusterer.cluster_by_serp(
            analyzer.df,
            serp_column='serp_urls'
        )
    
    # –ì—Ä–∞—Ñ —Å–≤—è–∑–µ–π (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —è–≤–Ω–æ –≤–∫–ª—é—á–µ–Ω)
    if args.enable_graph and not args.skip_embeddings:
        await build_graph_stage(args, analyzer)
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º —Å–ª–æ–≤ (–∞–Ω–∞–ª–æ–≥ KeyCollector)
    if getattr(args, 'enable_word_match', False):
        await word_match_clustering_stage(args, analyzer)
    
    print()


async def build_graph_stage(args, analyzer):
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å–≤—è–∑–µ–π"""
    print_stage(analyzer, "üîÑ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å–≤—è–∑–µ–π...")
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º sentence-transformers –¥–ª—è embeddings
        from sentence_transformers import SentenceTransformer
        
        model_name = CLUSTERING_CONFIG['embeddings']['model_name']
        print_stage(analyzer, f"  –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}...")
        model = SentenceTransformer(model_name)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embeddings
        print_stage(analyzer, "  –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings...")
        queries = analyzer.df['keyword'].tolist()
        embeddings = model.encode(
            queries,
            batch_size=CLUSTERING_CONFIG['embeddings']['batch_size'],
            show_progress_bar=True
        )
        
        # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ
        from seo_analyzer.clustering.graph_builder import GraphBuilder
        analyzer.graph_builder = GraphBuilder(CLUSTERING_CONFIG)
        analyzer.graph_builder.build_graph_from_similarity(embeddings, queries)
        
        # Community detection
        analyzer.graph_builder.detect_communities_louvain()
        
        # PageRank
        analyzer.graph_builder.calculate_pagerank()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≥—Ä–∞—Ñ–æ–≤—ã–µ —Ñ–∏—á–∏ –≤ DataFrame
        analyzer.df = analyzer.graph_builder.add_graph_features_to_dataframe(analyzer.df)
        
        print_stage(analyzer, f"‚úì –ì—Ä–∞—Ñ –ø–æ—Å—Ç—Ä–æ–µ–Ω")
        
    except Exception as e:
        print_stage(analyzer, f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞: {e}")
        print_stage(analyzer, f"  –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –≥—Ä–∞—Ñ–∞...")


async def word_match_clustering_stage(args, analyzer):
    """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º —Å–ª–æ–≤ (–∞–Ω–∞–ª–æ–≥ KeyCollector)"""
    print_stage(analyzer, "üîÑ –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º —Å–ª–æ–≤ (KeyCollector-–ø–æ–¥–æ–±–Ω–∞—è)...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    word_match_config = CLUSTERING_CONFIG.get('word_match', {})
    
    word_match_clusterer = WordMatchClusterer(
        min_match_strength=args.word_match_strength or word_match_config.get('min_match_strength', 2),
        min_group_size=args.word_match_min_size or word_match_config.get('min_group_size', 2),
        strengthen_links=args.word_match_strengthen,
        exclude_stopwords=word_match_config.get('exclude_stopwords', True),
        use_lemmatization=word_match_config.get('use_lemmatization', True)
    )
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    queries = analyzer.df['keyword'].tolist()
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
    frequencies = {}
    if 'frequency_exact' in analyzer.df.columns:
        frequencies = dict(zip(analyzer.df['keyword'], analyzer.df['frequency_exact']))
    elif 'frequency_world' in analyzer.df.columns:
        frequencies = dict(zip(analyzer.df['keyword'], analyzer.df['frequency_world']))
    
    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    clusters = word_match_clusterer.cluster_queries(queries, frequencies)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DataFrame
    analyzer.df = word_match_clusterer.add_to_dataframe(
        analyzer.df,
        query_column='keyword',
        cluster_column='word_match_cluster_id',
        cluster_name_column='word_match_cluster_name'
    )
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = word_match_clusterer.get_cluster_stats()
    print_stage(analyzer, f"‚úì –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
    print_stage(analyzer, f"  ‚Ä¢ –°–æ–∑–¥–∞–Ω–æ –≥—Ä—É–ø–ø: {stats.get('total_clusters', 0)}")
    print_stage(analyzer, f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã: {stats.get('avg_cluster_size', 0):.1f}")
    print_stage(analyzer, f"  ‚Ä¢ –ú–∏–Ω/–ú–∞–∫—Å —Ä–∞–∑–º–µ—Ä: {stats.get('min_cluster_size', 0)}/{stats.get('max_cluster_size', 0)}")

