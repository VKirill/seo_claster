"""
–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏
"""

import asyncio
from seo_analyzer.core.normalizer import QueryNormalizer


class NormalizationHandler:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞"""
        self.normalizer = QueryNormalizer()
    
    async def normalize_queries(self, queries_list, print_stage):
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
        
        Args:
            queries_list: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
            print_stage: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        """
        print_stage("üîÑ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤...")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
        normalized_results = await asyncio.to_thread(
            self.normalizer.normalize_batch,
            queries_list
        )
        
        print_stage(f"‚úì –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        return normalized_results
    
    def apply_normalization_to_df(self, df, normalized_results):
        """
        –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∫ DataFrame
        
        Args:
            df: DataFrame
            normalized_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            
        Returns:
            DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏
        """
        for key in ['normalized', 'lemmatized', 'word_count', 'has_latin', 'has_numbers']:
            df[key] = [r[key] for r in normalized_results]
        
        df.rename(columns={'word_count': 'words_count'}, inplace=True)
        return df

