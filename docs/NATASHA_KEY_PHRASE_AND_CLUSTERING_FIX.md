# üöÄ –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ Natasha –¥–ª—è key_phrase –∏ —É–ø—Ä–æ—â–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏

## –ò–∑–º–µ–Ω–µ–Ω–∏—è

### 1. ‚úÖ key_phrase —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Natasha (–Ω–µ pymorphy3)
### 2. ‚úÖ –£–±—Ä–∞–Ω–æ –ø—Ä–∞–≤–∏–ª–æ 50% –≤ —Ä–µ–∂–∏–º–µ BALANCED –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏

---

## 1. –ü–µ—Ä–µ—Ö–æ–¥ —Å pymorphy3 –Ω–∞ Natasha –¥–ª—è key_phrase

### –ü–æ—á–µ–º—É Natasha –ª—É—á—à–µ?

| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | pymorphy3 | **Natasha** |
|----------------|-----------|-------------|
| **–°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑** | ‚ùå –ù–µ—Ç | ‚úÖ –î–∞ (dependency parsing) |
| **–ò–º–µ–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã** | –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ | ‚úÖ –ß–µ—Ä–µ–∑ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–µ –¥–µ—Ä–µ–≤–æ |
| **–¢–æ—á–Ω–æ—Å—Ç—å** | –°—Ä–µ–¥–Ω—è—è | ‚úÖ –í—ã—Å–æ–∫–∞—è |
| **–ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å–≤—è–∑–µ–π** | –ù–µ—Ç | ‚úÖ –î–∞ (nmod, amod, nsubj, obj) |

### –ß—Ç–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å?

#### –ë–´–õ–û (pymorphy3):
```python
# –ü—Ä–æ—Å—Ç–æ–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
parsed = self.morph.parse(word)[0]
pos = parsed.tag.POS
lemma = parsed.normal_form

# –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –∏—â–µ–º –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏–µ —Å–ª–æ–≤–∞
# "–ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ + —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ"
if word1['pos'] == 'ADJF' and word2['pos'] == 'NOUN':
    phrase = f"{word1['lemma']} {word2['lemma']}"
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
- ‚ùå –ù–µ –ø–æ–Ω–∏–º–∞–µ—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏
- ‚ùå –ú–æ–∂–µ—Ç –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≤–∞–∂–Ω—ã–µ –≥—Ä—É–ø–ø—ã
- ‚ùå –ó–∞–≤–∏—Å–∏—Ç –æ—Ç –ø–æ—Ä—è–¥–∫–∞ —Å–ª–æ–≤

---

#### –°–¢–ê–õ–û (Natasha):
```python
# –ü–æ–ª–Ω—ã–π NLP pipeline —Å —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º
doc = Doc(query.lower())
doc.segment(self.segmenter)
doc.tag_morph(self.morph_tagger)
doc.parse_syntax(self.syntax_parser)  # ‚Üê –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–µ –¥–µ—Ä–µ–≤–æ!

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–Ω—ã—Ö –≥—Ä—É–ø–ø —á–µ—Ä–µ–∑ dependency parsing
for token in doc.tokens:
    if token.pos == 'NOUN':
        # –ò—â–µ–º –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏
        for other_token in doc.tokens:
            if other_token.head_id == token.id:
                # amod: –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ-–º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä
                if other_token.rel == 'amod' and other_token.pos == 'ADJ':
                    chunk_tokens.insert(0, other_token)
                
                # nmod: —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ-–º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä
                elif other_token.rel == 'nmod' and other_token.pos == 'NOUN':
                    chunk_tokens.append(other_token)
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –ü–æ–Ω–∏–º–∞–µ—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ (dependency relations)
- ‚úÖ –ù–∞—Ö–æ–¥–∏—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∏–º–µ–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –ø–æ—Ä—è–¥–∫–∞
- ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç amod (adjectival modifier), nmod (nominal modifier)
- ‚úÖ –ë–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤–æ–π —Ñ—Ä–∞–∑—ã

---

### –ü—Ä–∏–º–µ—Ä—ã —Ä–∞–±–æ—Ç—ã:

#### –ü—Ä–∏–º–µ—Ä 1: "–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä —Å–µ—Ç–µ–≤–æ–π —Å–∫—É–¥"
```
–°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–µ –¥–µ—Ä–µ–≤–æ:
–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä (NOUN, head)
    ‚îú‚îÄ —Å–µ—Ç–µ–≤–æ–π (ADJ, rel=amod) ‚Üê –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    ‚îî‚îÄ —Å–∫—É–¥ (NOUN, rel=nmod) ‚Üê –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä

–†–µ–∑—É–ª—å—Ç–∞—Ç:
key_phrase = "—Å–µ—Ç–µ–≤–æ–π –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä —Å–∫—É–¥"
```

#### –ü—Ä–∏–º–µ—Ä 2: "—Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–æ—Å—Ç—É–ø–∞"
```
–°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–µ –¥–µ—Ä–µ–≤–æ:
—Å–∏—Å—Ç–µ–º–∞ (NOUN, head)
    ‚îî‚îÄ –∫–æ–Ω—Ç—Ä–æ–ª—è (NOUN, rel=nmod)
        ‚îî‚îÄ –¥–æ—Å—Ç—É–ø–∞ (NOUN, rel=nmod)

–†–µ–∑—É–ª—å—Ç–∞—Ç:
key_phrase = "—Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–æ—Å—Ç—É–ø–∞"
```

#### –ü—Ä–∏–º–µ—Ä 3: "–∫—É–ø–∏—Ç—å —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π –∑–∞–º–æ–∫"
```
–°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–µ –¥–µ—Ä–µ–≤–æ:
–∫—É–ø–∏—Ç—å (VERB, head)
    ‚îî‚îÄ –∑–∞–º–æ–∫ (NOUN, rel=obj)
        ‚îî‚îÄ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π (ADJ, rel=amod)

–†–µ–∑—É–ª—å—Ç–∞—Ç:
key_phrase = "—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π –∑–∞–º–æ–∫"
```

---

### –ö–æ–¥ –∏–∑–º–µ–Ω–µ–Ω–∏—è:

**–§–∞–π–ª**: `seo_analyzer/analysis/key_phrases_extractor.py`

#### –ù–æ–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è:
```python
from natasha import (
    Segmenter,
    NewsEmbedding,
    NewsMorphTagger,
    NewsSyntaxParser,
    Doc
)

class KeyPhrasesExtractor:
    def __init__(self):
        self.segmenter = Segmenter()
        self.emb = NewsEmbedding()
        self.morph_tagger = NewsMorphTagger(self.emb)
        self.syntax_parser = NewsSyntaxParser(self.emb)  # ‚Üê –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–µ—Ä
```

#### –ù–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:
```python
def _extract_noun_chunks_from_syntax(self, doc: Doc) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã —á–µ—Ä–µ–∑ dependency parsing"""
    noun_chunks = []
    
    for token in doc.tokens:
        if token.pos == 'NOUN':
            chunk_tokens = [token]
            
            # –ò—â–µ–º –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
            for other_token in doc.tokens:
                if other_token.head_id == token.id:
                    # –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ-–º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä (amod)
                    if other_token.rel == 'amod' and other_token.pos in ['ADJ']:
                        chunk_tokens.insert(0, other_token)
                    
                    # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ-–º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä (nmod)
                    elif other_token.rel == 'nmod' and other_token.pos == 'NOUN':
                        chunk_tokens.append(other_token)
            
            if len(chunk_tokens) >= 2:
                chunk_tokens.sort(key=lambda t: t.start)
                phrase = ' '.join([t.lemma for t in chunk_tokens])
                noun_chunks.append(phrase)
    
    return noun_chunks
```

---

## 2. –£–ø—Ä–æ—â–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞ BALANCED –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏

### –ß—Ç–æ —É–±—Ä–∞–ª–∏?

**–ë–´–õ–û**:
```python
elif self.mode == self.MODE_BALANCED:
    # –î–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ö–æ–∂ —Å min_cluster_cohesion % –∑–∞–ø—Ä–æ—Å–æ–≤
    required_similar = max(1, int(len(cluster_queries) * self.min_cluster_cohesion))
    return similar_count >= required_similar  # –¢—Ä–µ–±–æ–≤–∞–ª–æ—Å—å 50%
```

**–ü—Ä–æ–±–ª–µ–º–∞**: –°–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è –º–µ—à–∞–ª–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–µ.

---

**–°–¢–ê–õ–û**:
```python
elif self.mode == self.MODE_BALANCED:
    # –£–ü–†–û–©–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê: –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ö–æ—Ç—è –±—ã —Å –û–î–ù–ò–ú –∑–∞–ø—Ä–æ—Å–æ–º –∫–ª–∞—Å—Ç–µ—Ä–∞
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—â–∏—Ö URL (>=7) —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –≤ _calculate_weighted_similarity
    # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ø–æ—Ö–æ–∂–∏–π –∑–∞–ø—Ä–æ—Å -> –¥–æ–±–∞–≤–ª—è–µ–º –≤ –∫–ª–∞—Å—Ç–µ—Ä
    return similar_count > 0
```

**–†–µ—à–µ–Ω–∏–µ**: –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ - –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –∏–º–µ–µ—Ç >=7 –æ–±—â–∏—Ö URL —Ö–æ—Ç—è –±—ã —Å –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º –∫–ª–∞—Å—Ç–µ—Ä–∞ ‚Üí –¥–æ–±–∞–≤–ª—è–µ–º!

---

### –ü–æ—á–µ–º—É —ç—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ?

#### –ö—Ä–∏—Ç–µ—Ä–∏–π –ø–æ–ø–∞—Ä–Ω–æ–π –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –£–ñ–ï –∂–µ—Å—Ç–∫–∏–π:
```python
def calculate_weighted_similarity(urls1, urls2):
    common_count = len(set(urls1) & set(urls2))
    
    if common_count >= 7:  # ‚Üê –ñ–ï–°–¢–ö–ò–ô –∫—Ä–∏—Ç–µ—Ä–∏–π!
        return True  # –ó–∞–ø—Ä–æ—Å—ã –ø–æ—Ö–æ–∂–∏
```

**7 –æ–±—â–∏—Ö URL –∏–∑ —Ç–æ–ø-20** - —ç—Ç–æ —É–∂–µ **35% –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ**, —á—Ç–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏.

---

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∂–∏–º–æ–≤ –¢–ï–ü–ï–†–¨:

| –†–µ–∂–∏–º | –£—Å–ª–æ–≤–∏–µ | –ö–ª–∞—Å—Ç–µ—Ä—ã |
|-------|---------|----------|
| **STRICT** üîí | –°—Ö–æ–∂ —Å–æ **–í–°–ï–ú–ò** –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ | –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ |
| **BALANCED** ‚öñÔ∏è | –°—Ö–æ–∂ —Ö–æ—Ç—è –±—ã —Å **–û–î–ù–ò–ú** –∑–∞–ø—Ä–æ—Å–æ–º (>=7 URL) | –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ |
| **SOFT** üåä | –°—Ö–æ–∂ —Ö–æ—Ç—è –±—ã —Å **–û–î–ù–ò–ú** –∑–∞–ø—Ä–æ—Å–æ–º (>=7 URL) | –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ BALANCED |

**BALANCED –∏ SOFT —Ç–µ–ø–µ—Ä—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ**, –Ω–æ BALANCED - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Ä–µ–∂–∏–º.

---

### –ü—Ä–∏–º–µ—Ä —Ä–∞–±–æ—Ç—ã:

```
–ö–ª–∞—Å—Ç–µ—Ä: ["—Å–∫—É–¥ –∫—É–ø–∏—Ç—å", "—Å–∫—É–¥ —Ü–µ–Ω–∞", "—Å–∫—É–¥ –∑–∞–∫–∞–∑–∞—Ç—å"]

–ù–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: "—Å–∫—É–¥ –º–æ—Å–∫–≤–∞"

–ü—Ä–æ–≤–µ—Ä—è–µ–º:
"—Å–∫—É–¥ –º–æ—Å–∫–≤–∞" ‚Üî "—Å–∫—É–¥ –∫—É–ø–∏—Ç—å": 8 –æ–±—â–∏—Ö URL ‚úÖ

–†–µ–∑—É–ª—å—Ç–∞—Ç: –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–ª–∞—Å—Ç–µ—Ä!

–õ–æ–≥–∏–∫–∞: –ï—Å–ª–∏ –µ—Å—Ç—å >=7 –æ–±—â–∏—Ö URL —Ö–æ—Ç—è –±—ã —Å –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º –∫–ª–∞—Å—Ç–µ—Ä–∞,
–∑–∞–ø—Ä–æ—Å—ã —Å–≤—è–∑–∞–Ω—ã –æ–¥–Ω–æ–π —Ç–µ–º–∞—Ç–∏–∫–æ–π SERP ‚Üí –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –≤–º–µ—Å—Ç–µ.
```

---

## –§–∞–π–ª—ã –∏–∑–º–µ–Ω–µ–Ω—ã

1. **`seo_analyzer/analysis/key_phrases_extractor.py`**
   - –ó–∞–º–µ–Ω–µ–Ω pymorphy3 –Ω–∞ Natasha
   - –î–æ–±–∞–≤–ª–µ–Ω —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (syntax parsing)
   - –ù–æ–≤—ã–π –º–µ—Ç–æ–¥ `_extract_noun_chunks_from_syntax()`
   - –ù–æ–≤—ã–π –º–µ—Ç–æ–¥ `_determine_key_phrase_from_syntax()`

2. **`seo_analyzer/clustering/serp_advanced_clusterer.py`**
   - –£–ø—Ä–æ—â–µ–Ω–∞ –ª–æ–≥–∏–∫–∞ —Ä–µ–∂–∏–º–∞ BALANCED
   - –£–±—Ä–∞–Ω–æ –ø—Ä–∞–≤–∏–ª–æ 50% (`min_cluster_cohesion`)
   - –¢–µ–ø–µ—Ä—å: —Å—Ö–æ–∂–µ—Å—Ç—å —Å –ª—é–±—ã–º –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º –∫–ª–∞—Å—Ç–µ—Ä–∞ (>=7 URL)

---

## –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π

### key_phrase (Natasha):
- ‚úÖ –ë–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–ª–∞–≤–Ω–æ–π —Ñ—Ä–∞–∑—ã
- ‚úÖ –ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π
- ‚úÖ –ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –ª—é–±—ã–º –ø–æ—Ä—è–¥–∫–æ–º —Å–ª–æ–≤
- ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ª–æ–∂–Ω—ã—Ö –∏–º–µ–Ω–Ω—ã—Ö –≥—Ä—É–ø–ø

### –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (—É–ø—Ä–æ—â–µ–Ω–∏–µ):
- ‚úÖ –ü—Ä–æ—Å—Ç–∞—è –∏ –ø–æ–Ω—è—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞
- ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ SERP
- ‚úÖ –ù–µ—Ç –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º
- ‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–æ–≤

---

## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑
python main.py

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å key_phrase –≤ JSON
# output/groups/<–≥—Ä—É–ø–ø–∞>/seo_analysis.json
# "key_phrase": "–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä —Å–µ—Ç–µ–≤–æ–π —Å–∫—É–¥"

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ã
# –î–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –±–µ–∑ –ª–∏—à–Ω–µ–π —Ä–∞–∑–¥—Ä–æ–±–ª–µ–Ω–Ω–æ—Å—Ç–∏
```

---

## –ò—Ç–æ–≥

### –î–æ –∏–∑–º–µ–Ω–µ–Ω–∏–π:
- ‚ùå pymorphy3 –Ω–µ –ø–æ–Ω–∏–º–∞–ª —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
- ‚ùå –ü—Ä–∞–≤–∏–ª–æ 50% –º–µ—à–∞–ª–æ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–µ
- ‚ùå key_phrase –±—ã–ª –Ω–µ—Ç–æ—á–Ω—ã–º

### –ü–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π:
- ‚úÖ Natasha –ø–æ–Ω–∏–º–∞–µ—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—Å —á–µ—Ä–µ–∑ dependency parsing
- ‚úÖ –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ –æ–±—â–∏–º URL
- ‚úÖ –¢–æ—á–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤–æ–π —Ñ—Ä–∞–∑—ã

---

‚úÖ **–ì–æ—Ç–æ–≤–æ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!**

