"""
Cluster LSI Aggregator Module
–ê–≥—Ä–µ–≥–∞—Ü–∏—è LSI —Ñ—Ä–∞–∑ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
"""

import pandas as pd
from typing import Dict, List, Any
from collections import Counter


class ClusterLSIAggregator:
    """–ê–≥—Ä–µ–≥–∞—Ç–æ—Ä LSI —Ñ—Ä–∞–∑ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
    
    def __init__(self, top_n_per_cluster: int = 10):
        """
        Args:
            top_n_per_cluster: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø LSI —Ñ—Ä–∞–∑ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä
        """
        self.top_n_per_cluster = top_n_per_cluster
    
    def aggregate_cluster_lsi(
        self,
        df: pd.DataFrame,
        cluster_column: str = 'semantic_cluster_id'
    ) -> Dict[int, List[Dict[str, Any]]]:
        """
        –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å LSI —Ñ—Ä–∞–∑—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        
        Args:
            df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ [cluster_column, 'lsi_phrases']
            cluster_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å ID –∫–ª–∞—Å—Ç–µ—Ä–∞
            
        Returns:
            Dict {cluster_id: [top_lsi_phrases]}
        """
        if cluster_column not in df.columns:
            print(f"‚ö†Ô∏è  –ö–æ–ª–æ–Ω–∫–∞ '{cluster_column}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return {}
        
        if 'lsi_phrases' not in df.columns:
            print("‚ö†Ô∏è  –ö–æ–ª–æ–Ω–∫–∞ 'lsi_phrases' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return {}
        
        cluster_lsi = {}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        clusters_without_lsi = []
        clusters_with_empty_lsi = []
        
        for cluster_id, group in df.groupby(cluster_column):
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ LSI —Ñ—Ä–∞–∑—ã –∏–∑ –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∞
            all_lsi = []
            queries_with_lsi = 0
            queries_without_lsi = 0
            
            for idx, row in group.iterrows():
                lsi_list = row['lsi_phrases']
                
                # –ü–∞—Ä—Å–∏–º JSON —Å—Ç—Ä–æ–∫—É –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if isinstance(lsi_list, str):
                    import json
                    try:
                        lsi_list = json.loads(lsi_list) if lsi_list.strip() else []
                    except (json.JSONDecodeError, TypeError):
                        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        queries_without_lsi += 1
                        continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –∏ –æ–Ω –Ω–µ –ø—É—Å—Ç–æ–π
                if isinstance(lsi_list, list) and len(lsi_list) > 0:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å–ø–∏—Å–∫–∞
                    items_added = 0
                    for item in lsi_list:
                        if isinstance(item, dict):
                            # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–æ–º 'phrase'
                            phrase = item.get('phrase', '')
                            if phrase:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ñ—Ä–∞–∑–∞ –Ω–µ –ø—É—Å—Ç–∞—è
                                all_lsi.append(item)
                                items_added += 1
                            # –ï—Å–ª–∏ —Å–ª–æ–≤–∞—Ä—å –±–µ–∑ –∫–ª—é—á–∞ 'phrase' - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        elif isinstance(item, str):
                            # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ - –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å
                            if item.strip():  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å—Ç—Ä–æ–∫–∞ –Ω–µ –ø—É—Å—Ç–∞—è
                                all_lsi.append({
                                    'phrase': item.strip(),
                                    'frequency': 1,
                                    'source': 'unknown'
                                })
                                items_added += 1
                        # –ò–Ω–∞—á–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
                    
                    if items_added > 0:
                        queries_with_lsi += 1
                    else:
                        # –°–ø–∏—Å–æ–∫ –±—ã–ª –Ω–µ –ø—É—Å—Ç–æ–π, –Ω–æ –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –±—ã–ª–∏ –ø—É—Å—Ç—ã–º–∏ –∏–ª–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
                        queries_without_lsi += 1
                else:
                    queries_without_lsi += 1
            
            # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º
            if all_lsi:
                aggregated = self._aggregate_phrases(all_lsi)
                cluster_lsi[cluster_id] = aggregated
            else:
                cluster_lsi[cluster_id] = []
                if queries_without_lsi > 0:
                    clusters_without_lsi.append((cluster_id, len(group), queries_without_lsi))
                if queries_with_lsi == 0:
                    clusters_with_empty_lsi.append((cluster_id, len(group)))
        
        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        if clusters_without_lsi:
            print(f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {len(clusters_without_lsi)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –≥–¥–µ –Ω–µ —É –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –µ—Å—Ç—å LSI —Ñ—Ä–∞–∑—ã")
            if len(clusters_without_lsi) <= 10:
                for cid, total, without in clusters_without_lsi[:10]:
                    print(f"   –ö–ª–∞—Å—Ç–µ—Ä {cid}: {without}/{total} –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ LSI")
        
        if clusters_with_empty_lsi:
            print(f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {len(clusters_with_empty_lsi)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –±–µ–∑ LSI —Ñ—Ä–∞–∑ (—É –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –ø—É—Å—Ç—ã–µ LSI)")
            if len(clusters_with_empty_lsi) <= 10:
                for cid, total in clusters_with_empty_lsi[:10]:
                    print(f"   –ö–ª–∞—Å—Ç–µ—Ä {cid}: {total} –∑–∞–ø—Ä–æ—Å–æ–≤, –≤—Å–µ –±–µ–∑ LSI")
        
        return cluster_lsi
    
    def _aggregate_phrases(
        self,
        phrases_list: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å —Å–ø–∏—Å–æ–∫ LSI —Ñ—Ä–∞–∑, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Å–∞–º—ã–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ
        
        –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω—É–∂–Ω—ã –ø–æ–ª–Ω—ã–µ —Ñ—Ä–∞–∑—ã, –∞ –Ω–µ —á–∞—Å—Ç–∏.
        –£–¥–∞–ª—è–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—Ö–æ–¥—è—Ç –≤ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ.
        
        Args:
            phrases_list: –°–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑ —Å —á–∞—Å—Ç–æ—Ç–æ–π –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º
            
        Returns:
            –¢–æ–ø –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑ –±–µ–∑ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –ø–æ–¥—Å—Ç—Ä–æ–∫
        """
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–µ–∫—Å—Ç—É —Ñ—Ä–∞–∑—ã
        phrase_data = {}
        
        for item in phrases_list:
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞–Ω–Ω—ã—Ö
            if not isinstance(item, dict):
                # –ï—Å–ª–∏ —ç–ª–µ–º–µ–Ω—Ç –Ω–µ —Å–ª–æ–≤–∞—Ä—å - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º phrase_text —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
            phrase_text = item.get('phrase', '')
            
            # –ï—Å–ª–∏ phrase_text - —Å–ª–æ–≤–∞—Ä—å (–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç), –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Å—Ç—Ä–æ–∫—É
            if isinstance(phrase_text, dict):
                # –í–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ –≤–ª–æ–∂–µ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ - –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å—Ç—Ä–æ–∫—É
                phrase_text = phrase_text.get('phrase', '') if isinstance(phrase_text.get('phrase'), str) else ''
            
            # –ï—Å–ª–∏ phrase_text –≤—Å–µ –µ—â–µ –Ω–µ —Å—Ç—Ä–æ–∫–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if not isinstance(phrase_text, str):
                continue
            
            # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –ø—É—Å—Ç–∞—è - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if not phrase_text.strip():
                continue
            
            frequency = item.get('frequency', 1)
            source = item.get('source', 'unknown')
            
            if phrase_text not in phrase_data:
                phrase_data[phrase_text] = {
                    'total_frequency': 0,
                    'sources': [],
                    'queries_count': 0,
                    'word_count': len(phrase_text.split())
                }
            
            phrase_data[phrase_text]['total_frequency'] += frequency
            phrase_data[phrase_text]['sources'].append(source)
            phrase_data[phrase_text]['queries_count'] += 1
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –ø–æ–¥—Å—Ç—Ä–æ–∫–∏
        phrase_data = self._remove_substring_duplicates(phrase_data)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º: —Å–Ω–∞—á–∞–ª–∞ –ø–æ –¥–ª–∏–Ω–µ (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç–∏), –ø–æ—Ç–æ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ
        sorted_phrases = sorted(
            phrase_data.items(),
            key=lambda x: (x[1]['word_count'], x[1]['total_frequency']),
            reverse=True
        )[:self.top_n_per_cluster]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = []
        for phrase_text, data in sorted_phrases:
            # –û—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫
            source_counter = Counter(data['sources'])
            main_source = source_counter.most_common(1)[0][0]
            
            result.append({
                'phrase': phrase_text,
                'frequency': data['total_frequency'],
                'queries_count': data['queries_count'],
                'main_source': main_source,
                'relevance_score': self._calculate_relevance(
                    data['total_frequency'],
                    data['queries_count'],
                    data['word_count']
                )
            })
        
        return result
    
    def _remove_substring_duplicates(
        self,
        phrase_data: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Dict[str, Any]]:
        """
        –£–¥–∞–ª–∏—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤—Ö–æ–¥—è—Ç –≤ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ
        
        –ü—Ä–∏–º–µ—Ä:
            "—Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å" —É–¥–∞–ª—è–µ—Ç—Å—è, –µ—Å–ª–∏ –µ—Å—Ç—å "—Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø"
        
        –õ–æ–≥–∏–∫–∞: –µ—Å–ª–∏ –∫–æ—Ä–æ—Ç–∫–∞—è —Ñ—Ä–∞–∑–∞ –≤—Ö–æ–¥–∏—Ç –≤ –¥–ª–∏–Ω–Ω—É—é –∏ –µ—ë —á–∞—Å—Ç–æ—Ç–∞ –Ω–µ –±–æ–ª–µ–µ —á–µ–º
        –≤ 2 —Ä–∞–∑–∞ –≤—ã—à–µ, —Ç–æ –∫–æ—Ä–æ—Ç–∫—É—é —É–¥–∞–ª—è–µ–º (–æ–Ω–∞ –Ω–µ –Ω–µ—Å–µ—Ç –¥–æ–ø. –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)
        
        Args:
            phrase_data: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ —Ñ—Ä–∞–∑
            
        Returns:
            –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Ñ—Ä–∞–∑
        """
        phrases_by_length = sorted(
            phrase_data.keys(),
            key=lambda x: len(x.split()),
            reverse=True
        )
        
        to_remove = set()
        
        for long_phrase in phrases_by_length:
            if long_phrase in to_remove:
                continue
                
            long_freq = phrase_data[long_phrase]['total_frequency']
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã
            for short_phrase in phrases_by_length:
                if short_phrase in to_remove:
                    continue
                
                if len(short_phrase.split()) >= len(long_phrase.split()):
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏
                if self._is_substring(short_phrase, long_phrase):
                    short_freq = phrase_data[short_phrase]['total_frequency']
                    
                    # –£–¥–∞–ª—è–µ–º –∫–æ—Ä–æ—Ç–∫—É—é –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ –Ω–∞–º–Ω–æ–≥–æ —á–∞—Å—Ç–æ—Ç–Ω–µ–µ (–Ω–µ –±–æ–ª–µ–µ 2x)
                    if short_freq <= long_freq * 2:
                        to_remove.add(short_phrase)
        
        return {
            phrase: data 
            for phrase, data in phrase_data.items() 
            if phrase not in to_remove
        }
    
    def _is_substring(self, short: str, long: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Ö–æ–¥–∏—Ç –ª–∏ –∫–æ—Ä–æ—Ç–∫–∞—è —Ñ—Ä–∞–∑–∞ –≤ –¥–ª–∏–Ω–Ω—É—é (–≤—Å–µ —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥)
        
        Args:
            short: –ö–æ—Ä–æ—Ç–∫–∞—è —Ñ—Ä–∞–∑–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä "—Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å")
            long: –î–ª–∏–Ω–Ω–∞—è —Ñ—Ä–∞–∑–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä "—Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
        
        Returns:
            True –µ—Å–ª–∏ –∫–æ—Ä–æ—Ç–∫–∞—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤—Ö–æ–¥–∏—Ç –≤ –¥–ª–∏–Ω–Ω—É—é
        """
        short_words = short.split()
        long_words = long.split()
        
        if len(short_words) >= len(long_words):
            return False
        
        # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
        for i in range(len(long_words) - len(short_words) + 1):
            if long_words[i:i + len(short_words)] == short_words:
                return True
        
        return False
    
    def _calculate_relevance(
        self,
        total_frequency: int,
        queries_count: int,
        word_count: int
    ) -> float:
        """
        –†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ñ—Ä–∞–∑—ã –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞ —Å —É—á–µ—Ç–æ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç–∏
        
        –ë–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã –ø–æ–ª—É—á–∞—é—Ç –±–æ–Ω—É—Å –∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        
        Args:
            total_frequency: –û–±—â–∞—è —á–∞—Å—Ç–æ—Ç–∞ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏
            queries_count: –í —Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è
            word_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Ñ—Ä–∞–∑–µ
        
        Returns:
            –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–≤—ã—à–µ = –ª—É—á—à–µ)
        """
        # –ë–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º—É–ª–∞
        base_score = total_frequency * (1 + queries_count * 0.1)
        
        # –ë–æ–Ω—É—Å –∑–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å (–¥–ª–∏–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã)
        # 2-–≥—Ä–∞–º–º—ã: –º–Ω–æ–∂–∏—Ç–µ–ª—å 1.0
        # 3-–≥—Ä–∞–º–º—ã: –º–Ω–æ–∂–∏—Ç–µ–ª—å 1.3
        # 4-–≥—Ä–∞–º–º—ã: –º–Ω–æ–∂–∏—Ç–µ–ª—å 1.6
        specificity_bonus = 1.0 + (word_count - 2) * 0.3
        
        return base_score * specificity_bonus
    
    def add_cluster_lsi_to_dataframe(
        self,
        df: pd.DataFrame,
        cluster_lsi: Dict[int, List[Dict]],
        cluster_column: str = 'semantic_cluster_id'
    ) -> pd.DataFrame:
        """
        –î–æ–±–∞–≤–∏—Ç—å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ LSI —Ñ—Ä–∞–∑—ã –≤ DataFrame
        
        Args:
            df: DataFrame
            cluster_lsi: –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ LSI –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
            cluster_column: –ö–æ–ª–æ–Ω–∫–∞ —Å ID –∫–ª–∞—Å—Ç–µ—Ä–∞
            
        Returns:
            DataFrame —Å –Ω–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–æ–π 'cluster_lsi_phrases'
        """
        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ cluster_id -> LSI phrases
        cluster_to_lsi = {}
        cluster_to_lsi_str = {}
        
        for cluster_id, lsi_list in cluster_lsi.items():
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ø-30 –∫–∞–∫ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π (–¥–ª—è JSON/Excel)
            cluster_to_lsi[cluster_id] = lsi_list[:30] if lsi_list else []
            
            # –¢–∞–∫–∂–µ —Å–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ (–¥–ª—è CSV)
            top_phrases = []
            for item in lsi_list[:30]:
                if isinstance(item, dict):
                    phrase = item.get('phrase', '')
                    if phrase:
                        top_phrases.append(phrase)
                elif isinstance(item, str):
                    if item:
                        top_phrases.append(item)
            cluster_to_lsi_str[cluster_id] = ', '.join(top_phrases) if top_phrases else ''
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (—Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π)
        df['cluster_lsi_phrases'] = df[cluster_column].map(cluster_to_lsi)
        
        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–∫–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–µ–π –ø–æ–ª—É—á–∏–ª–∏ LSI —Ñ—Ä–∞–∑—ã
        mapped_count = df['cluster_lsi_phrases'].notna().sum()
        empty_count = (df['cluster_lsi_phrases'].isna() | (df['cluster_lsi_phrases'] == '')).sum()
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø—É—Å—Ç—ã–º–∏ —Å–ø–∏—Å–∫–∞–º–∏
        df['cluster_lsi_phrases'] = df['cluster_lsi_phrases'].fillna('').apply(
            lambda x: x if isinstance(x, list) else []
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∫–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–µ–π –∏–º–µ—é—Ç –Ω–µ–ø—É—Å—Ç—ã–µ LSI —Ñ—Ä–∞–∑—ã –ø–æ—Å–ª–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
        non_empty_count = df['cluster_lsi_phrases'].apply(lambda x: isinstance(x, list) and len(x) > 0).sum()
        
        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –±–µ–∑ LSI
        clusters_without_lsi = []
        queries_without_cluster = 0
        queries_with_cluster_but_no_lsi = 0
        
        for cluster_id in df[cluster_column].unique():
            cluster_df = df[df[cluster_column] == cluster_id]
            cluster_lsi_data = cluster_df['cluster_lsi_phrases'].iloc[0] if len(cluster_df) > 0 else []
            
            if cluster_id == -1:
                # –ó–∞–ø—Ä–æ—Å—ã –±–µ–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞
                queries_without_cluster += len(cluster_df)
            elif not isinstance(cluster_lsi_data, list) or len(cluster_lsi_data) == 0:
                clusters_without_lsi.append(cluster_id)
                queries_with_cluster_but_no_lsi += len(cluster_df)
        
        if queries_without_cluster > 0:
            print(f"‚ÑπÔ∏è  –ó–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞ (semantic_cluster_id = -1): {queries_without_cluster}")
        
        if clusters_without_lsi:
            print(f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {len(clusters_without_lsi)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –±–µ–∑ LSI —Ñ—Ä–∞–∑ –ø–æ—Å–ª–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏")
            print(f"   –ó–∞–ø—Ä–æ—Å–æ–≤ –≤ —Ç–∞–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö: {queries_with_cluster_but_no_lsi}")
            if len(clusters_without_lsi) <= 20:
                print(f"   ID –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {clusters_without_lsi[:20]}")
        
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ LSI —Ñ—Ä–∞–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:")
        print(f"   –ó–∞–ø–∏—Å–µ–π —Å –º–∞–ø–ø–∏–Ω–≥–æ–º: {mapped_count}/{len(df)}")
        print(f"   –ó–∞–ø–∏—Å–µ–π —Å –Ω–µ–ø—É—Å—Ç—ã–º–∏ LSI: {non_empty_count}/{len(df)}")
        print(f"   –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ –±–µ–∑ LSI: {len(clusters_without_lsi)}")
        if queries_without_cluster > 0 or queries_with_cluster_but_no_lsi > 0:
            print(f"   –ó–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ LSI –∫–ª–∞—Å—Ç–µ—Ä–∞: {queries_without_cluster + queries_with_cluster_but_no_lsi}")
        
        # –°—Ç—Ä–æ–∫–æ–≤–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è CSV
        df['cluster_lsi_phrases_str'] = df[cluster_column].map(cluster_to_lsi_str)
        df['cluster_lsi_phrases_str'] = df['cluster_lsi_phrases_str'].fillna('')
        
        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ (–≤—Å–µ —Ñ—Ä–∞–∑—ã, –Ω–µ —Ç–æ–ª—å–∫–æ —Ç–æ–ø-30)
        df['cluster_lsi_full'] = df[cluster_column].map(
            lambda x: cluster_lsi.get(x, [])
        )
        
        return df
    
    def aggregate_cluster_serp_urls(
        self,
        df: pd.DataFrame,
        cluster_column: str = 'semantic_cluster_id',
        serp_urls_column: str = 'serp_urls',
        top_n: int = 10
    ) -> Dict[int, List[str]]:
        """
        –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å –æ–±—â–∏–µ SERP URL –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        
        Args:
            df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ [cluster_column, serp_urls_column]
            cluster_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å ID –∫–ª–∞—Å—Ç–µ—Ä–∞
            serp_urls_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å SERP URL
            top_n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø URL –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä
            
        Returns:
            Dict {cluster_id: [top_common_urls]}
        """
        if cluster_column not in df.columns:
            print(f"‚ö†Ô∏è  –ö–æ–ª–æ–Ω–∫–∞ '{cluster_column}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return {}
        
        if serp_urls_column not in df.columns:
            print(f"‚ö†Ô∏è  –ö–æ–ª–æ–Ω–∫–∞ '{serp_urls_column}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return {}
        
        cluster_urls = {}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        for cluster_id, group in df.groupby(cluster_column):
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ URL –∏–∑ –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∞
            url_counter = Counter()
            
            for urls_list in group[serp_urls_column]:
                if isinstance(urls_list, list):
                    url_counter.update(urls_list)
            
            # –¢–æ–ø URL –ø–æ —á–∞—Å—Ç–æ—Ç–µ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏
            if url_counter:
                top_urls = [url for url, count in url_counter.most_common(top_n)]
                cluster_urls[cluster_id] = top_urls
            else:
                cluster_urls[cluster_id] = []
        
        return cluster_urls
    
    def add_cluster_serp_urls_to_dataframe(
        self,
        df: pd.DataFrame,
        cluster_urls: Dict[int, List[str]],
        cluster_column: str = 'semantic_cluster_id'
    ) -> pd.DataFrame:
        """
        –î–æ–±–∞–≤–∏—Ç—å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ SERP URL –≤ DataFrame
        
        Args:
            df: DataFrame
            cluster_urls: –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ URL –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
            cluster_column: –ö–æ–ª–æ–Ω–∫–∞ —Å ID –∫–ª–∞—Å—Ç–µ—Ä–∞
            
        Returns:
            DataFrame —Å –Ω–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–æ–π 'cluster_common_urls'
        """
        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ cluster_id -> URL string
        cluster_to_urls = {}
        for cluster_id, urls_list in cluster_urls.items():
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É —Å —Ç–æ–ø URL
            cluster_to_urls[cluster_id] = ', '.join(urls_list)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –æ–±—â–∏–º–∏ URL –∫–ª–∞—Å—Ç–µ—Ä–∞ (–Ω–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ!)
        df['cluster_common_urls'] = df[cluster_column].map(cluster_to_urls)
        df['cluster_common_urls'] = df['cluster_common_urls'].fillna('')
        
        return df
    
    def export_cluster_lsi(
        self,
        cluster_lsi: Dict[int, List[Dict]],
        output_path: str
    ):
        """
        –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å LSI —Ñ—Ä–∞–∑—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ CSV
        
        Args:
            cluster_lsi: –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ LSI
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        rows = []
        
        for cluster_id, phrases in cluster_lsi.items():
            for phrase_data in phrases:
                rows.append({
                    'cluster_id': cluster_id,
                    'phrase': phrase_data['phrase'],
                    'frequency': phrase_data['frequency'],
                    'queries_count': phrase_data['queries_count'],
                    'main_source': phrase_data['main_source'],
                    'relevance_score': round(phrase_data['relevance_score'], 2)
                })
        
        export_df = pd.DataFrame(rows)
        export_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        print(f"‚úì –≠–∫—Å–ø–æ—Ä—Ç LSI —Ñ—Ä–∞–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {output_path}")
        print(f"  –í—Å–µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(cluster_lsi)}")
        print(f"  –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑: {len(export_df)}")
    
    def get_cluster_keywords_for_content(
        self,
        cluster_id: int,
        cluster_lsi: Dict[int, List[Dict]],
        top_n: int = 20
    ) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        
        Args:
            cluster_id: ID –∫–ª–∞—Å—Ç–µ—Ä–∞
            cluster_lsi: –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ LSI
            top_n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–∞–∑
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑
        """
        if cluster_id not in cluster_lsi:
            return []
        
        phrases = cluster_lsi[cluster_id][:top_n]
        return [item['phrase'] for item in phrases]
    
    def get_statistics(
        self,
        cluster_lsi: Dict[int, List[Dict]]
    ) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ LSI —Ñ—Ä–∞–∑–∞–º
        
        Returns:
            Dict —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        total_clusters = len(cluster_lsi)
        total_phrases = sum(len(phrases) for phrases in cluster_lsi.values())
        
        # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        all_frequencies = []
        all_queries_counts = []
        
        for phrases in cluster_lsi.values():
            for phrase in phrases:
                all_frequencies.append(phrase['frequency'])
                all_queries_counts.append(phrase['queries_count'])
        
        stats = {
            'total_clusters': total_clusters,
            'total_unique_phrases': total_phrases,
            'avg_phrases_per_cluster': round(total_phrases / total_clusters, 2) if total_clusters > 0 else 0,
            'avg_phrase_frequency': round(sum(all_frequencies) / len(all_frequencies), 2) if all_frequencies else 0,
            'avg_queries_per_phrase': round(sum(all_queries_counts) / len(all_queries_counts), 2) if all_queries_counts else 0,
        }
        
        return stats

