"""–ê–≥—Ä–µ–≥–∞—Ü–∏—è breadcrumbs –∏–∑ SERP –¥–∞–Ω–Ω—ã—Ö —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ –ë–î"""

from typing import List, Dict
from pathlib import Path
import pandas as pd

from .page_data_extractor import PageDataExtractor
from .breadcrumb_selector import BreadcrumbSelector
from .domain_filter import DomainFilter
from ..core.page_content_database import PageContentDatabase


class BreadcrumbAggregator:
    """–°–æ–±–∏—Ä–∞–µ—Ç breadcrumbs –∏ –∫–æ–Ω—Ç–µ–Ω—Ç, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –ë–î"""
    
    def __init__(
        self, 
        max_urls_per_query: int = 3,
        db_path: Path = None,
        stop_domains_file: Path = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        
        Args:
            max_urls_per_query: –ú–∞–∫—Å URL –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3 –¥–ª—è –¢–û–ü-3)
            db_path: –ü—É—Ç—å –∫ –ë–î
            stop_domains_file: –§–∞–π–ª —Å–æ —Å—Ç–æ–ø-–¥–æ–º–µ–Ω–∞–º–∏
        """
        self.max_urls_per_query = max_urls_per_query
        self.extractor = PageDataExtractor()
        self.selector = BreadcrumbSelector()
        
        # –ë–î –¥–ª—è –∫—ç—à–∞
        if db_path:
            self.database = PageContentDatabase(db_path)
        else:
            self.database = None
        
        # –§–∏–ª—å—Ç—Ä –¥–æ–º–µ–Ω–æ–≤
        if stop_domains_file and stop_domains_file.exists():
            self.domain_filter = DomainFilter(stop_domains_file)
        else:
            self.domain_filter = None
    
    def extract_from_dataframe(self, df: pd.DataFrame) -> Dict[str, List[List[str]]]:
        """
        –ò–∑–≤–ª–µ—á—å breadcrumbs –∏–∑ DataFrame —Å SERP –¥–∞–Ω–Ω—ã–º–∏
        
        Args:
            df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–æ–π 'serp_urls'
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å {query: [[breadcrumbs1], [breadcrumbs2], ...]}
        """
        results = {}
        total = len(df)
        
        print(f"\nüîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ breadcrumbs –∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ {total} –∑–∞–ø—Ä–æ—Å–æ–≤...")
        print(f"  –ú–∞–∫—Å URL –Ω–∞ –∑–∞–ø—Ä–æ—Å: {self.max_urls_per_query} (–¢–û–ü-{self.max_urls_per_query})")
        
        for idx, row in df.iterrows():
            if (idx + 1) % 10 == 0:
                print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {idx + 1}/{total}")
            
            query = row.get('keyword', '')
            serp_urls = row.get('serp_urls', [])
            
            if not isinstance(serp_urls, list) or not serp_urls:
                continue
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Å—Ç–æ–ø-–¥–æ–º–µ–Ω–∞–º
            if self.domain_filter:
                serp_urls = self.domain_filter.filter_urls(serp_urls)
            
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –¢–û–ü-N
            breadcrumbs_list = self._extract_from_urls(
                serp_urls[:self.max_urls_per_query],
                query
            )
            
            if breadcrumbs_list:
                results[query] = breadcrumbs_list
        
        print(f"‚úì –ò–∑–≤–ª–µ—á–µ–Ω–æ breadcrumbs –¥–ª—è {len(results)} –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        if self.database:
            print(f"‚úì –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î: {self.database.db_path}")
        
        return results
    
    def _extract_from_urls(self, urls: List[str], query: str) -> List[List[str]]:
        """
        –ò–∑–≤–ª–µ—á—å breadcrumbs –∏ –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ —Å–ø–∏—Å–∫–∞ URL
        
        Args:
            urls: –°–ø–∏—Å–æ–∫ URL –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –°–ø–∏—Å–æ–∫ breadcrumbs
        """
        breadcrumbs_list = []
        
        for position, url in enumerate(urls, start=1):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ë–î –∫—ç—à
            if self.database:
                cached = self.database.get_page_data(url)
                if cached:
                    if cached['breadcrumbs']:
                        breadcrumbs_list.append(cached['breadcrumbs'])
                    continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            page_data = self.extractor.extract_from_url(url)
            
            if not page_data:
                continue
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
            if self.database:
                self.database.save_page_data(
                    url=page_data['url'],
                    domain=page_data['domain'],
                    query=query,
                    position=position,
                    content_data=page_data['content'],
                    breadcrumbs=page_data['breadcrumbs']
                )
            
            # –î–æ–±–∞–≤–ª—è–µ–º breadcrumbs
            if page_data['breadcrumbs'] and len(page_data['breadcrumbs']) >= 2:
                breadcrumbs_list.append(page_data['breadcrumbs'])
        
        return breadcrumbs_list
    
    def deduplicate_breadcrumbs(self, breadcrumbs_dict: Dict) -> Dict:
        """–£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã breadcrumbs"""
        return self.selector.deduplicate_breadcrumbs(breadcrumbs_dict)
    
    def get_unique_hierarchies(self, deduplicated: Dict) -> set:
        """–ü–æ–ª—É—á–∏—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–µ—Ä–∞—Ä—Ö–∏–∏"""
        return self.selector.get_unique_hierarchies(deduplicated)
    
    def get_hierarchy_stats(self, deduplicated: Dict) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏–µ—Ä–∞—Ä—Ö–∏—è–º"""
        return self.selector.get_hierarchy_stats(deduplicated)
