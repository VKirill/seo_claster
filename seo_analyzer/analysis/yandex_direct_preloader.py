"""
–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö Yandex Direct –≤ —Ñ–æ–Ω–µ.

–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∫—ç—à–µ,
–ø–æ–∫–∞ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –¥—Ä—É–≥–∏–µ —ç—Ç–∞–ø—ã –∞–Ω–∞–ª–∏–∑–∞.
"""

from typing import List, Optional
import logging

from .yandex_direct_client import YandexDirectClient
from .yandex_direct_parser import YandexDirectParser
from ..core.yandex_direct_cache import YandexDirectCache


logger = logging.getLogger(__name__)


class YandexDirectPreloader:
    """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö Direct –≤ —Ñ–æ–Ω–µ."""
    
    def __init__(self, token: str, use_sandbox: bool = False, geo_id: int = 213,
                 minus_words_file: str = "yandex_direct_minus_words.txt",
                 db_path: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è.
        
        Args:
            token: OAuth —Ç–æ–∫–µ–Ω Yandex Direct
            use_sandbox: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å sandbox API
            geo_id: ID —Ä–µ–≥–∏–æ–Ω–∞ (213 = –ú–æ—Å–∫–≤–∞)
            minus_words_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞–º–∏
            db_path: –ü—É—Ç—å –∫ –ë–î (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é output/serp_data.db)
        """
        self.client = YandexDirectClient(token, use_sandbox, geo_id, minus_words_file)
        self.parser = YandexDirectParser()
        
        # –ï—Å–ª–∏ –ø—É—Ç—å –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º output/serp_data.db
        if db_path is None:
            db_path = "output/serp_data.db"
        
        self.cache = YandexDirectCache(db_path=db_path)
        
    def get_missing_queries(self, queries: List[str]) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∫—ç—à–µ.
        
        Args:
            queries: –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        missing = []
        for query in queries:
            if not self.cache.get(query, self.client.geo_id):
                missing.append(query)
        return missing
        
    def preload_queries(self, queries: List[str], show_progress: bool = True) -> dict:
        """
        –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤.
        
        Args:
            queries: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            show_progress: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–≥—Ä—É–∑–∫–∏
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {'loaded': int, 'from_cache': int, 'failed': int, 'skipped_long': int}
        """
        stats = {'loaded': 0, 'from_cache': 0, 'failed': 0, 'skipped_long': 0}
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å –≤ –∫—ç—à–µ
        missing = self.get_missing_queries(queries)
        stats['from_cache'] = len(queries) - len(missing)
        
        if not missing:
            return stats
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã: —Ç–æ–ª—å–∫–æ –¥–æ 6 —Å–ª–æ–≤ (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ API Direct)
        valid_queries = []
        for query in missing:
            if len(query.split()) <= 6:
                valid_queries.append(query)
            else:
                stats['skipped_long'] += 1
        
        if not valid_queries:
            return stats
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –±–∞—Ç—á–∏
        batches = []
        for i in range(0, len(valid_queries), YandexDirectClient.MAX_PHRASES_PER_REQUEST):
            batch = valid_queries[i:i + YandexDirectClient.MAX_PHRASES_PER_REQUEST]
            batches.append(batch)
        
        total_batches = len(batches)
        
        # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏: –∫–æ–Ω–≤–µ–π–µ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        estimated_time_sec = total_batches * 0.1  # ~0.1 —Å–µ–∫ –Ω–∞ –±–∞—Ç—á (–∫–æ–Ω–≤–µ–π–µ—Ä)
        estimated_time_min = estimated_time_sec / 60
        
        if show_progress:
            print(f"  üì• –ù—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å: {len(valid_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤ ({total_batches} –±–∞—Ç—á–µ–π)")
            if stats['skipped_long'] > 0:
                print(f"  ‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ –¥–ª–∏–Ω–Ω—ã—Ö (>6 —Å–ª–æ–≤): {stats['skipped_long']}")
            if estimated_time_min >= 1:
                print(f"  ‚è±Ô∏è  –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è: ~{estimated_time_min:.1f} –º–∏–Ω")
            else:
                print(f"  ‚è±Ô∏è  –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è: ~{estimated_time_sec:.0f} —Å–µ–∫")
        
        # –ö–æ–Ω–≤–µ–π–µ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: –≤—Å–µ–≥–¥–∞ –¥–µ—Ä–∂–∏–º 5 –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
        MAX_PARALLEL = 5
        active_forecasts = []  # [(forecast_id, batch, batch_num), ...]
        batch_idx = 0
        processed = 0
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
        while batch_idx < min(MAX_PARALLEL, total_batches):
            batch = batches[batch_idx]
            try:
                forecast_id = self.client.create_forecast(batch, debug=False, skip_wait=True)
                active_forecasts.append((forecast_id, batch, batch_idx + 1))
            except Exception as e:
                logger.error(f"Failed to create forecast for batch {batch_idx + 1}: {e}")
                stats['failed'] += len(batch)
            batch_idx += 1
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: –ø–æ–ª—É—á–∞–µ–º –≥–æ—Ç–æ–≤—ã–π, —É–¥–∞–ª—è–µ–º, –∑–∞–ø—É—Å–∫–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π
        while active_forecasts:
            # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö (FIFO)
            forecast_id, batch, batch_num = active_forecasts.pop(0)
            
            try:
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                forecast_data = self.client.get_forecast(forecast_id, debug=False, skip_wait=True)
                
                # –ü–∞—Ä—Å–∏–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
                parsed_data = self.parser.parse_forecast_response(forecast_data)
                for data in parsed_data:
                    self.cache.set(data, self.client.geo_id)
                    stats['loaded'] += 1
                
                # –°–†–ê–ó–£ —É–¥–∞–ª—è–µ–º –∏–∑ –Ø–Ω–¥–µ–∫—Å–∞
                self.client.delete_forecast(forecast_id, debug=False, skip_wait=True)
                
                processed += 1
                
                # –°–†–ê–ó–£ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–ª–µ–¥—É—é—â–∏–π –±–∞—Ç—á (–µ—Å–ª–∏ –µ—Å—Ç—å)
                if batch_idx < total_batches:
                    next_batch = batches[batch_idx]
                    try:
                        next_forecast_id = self.client.create_forecast(next_batch, debug=False, skip_wait=True)
                        active_forecasts.append((next_forecast_id, next_batch, batch_idx + 1))
                    except Exception as e:
                        logger.error(f"Failed to create forecast for batch {batch_idx + 1}: {e}")
                        stats['failed'] += len(next_batch)
                    batch_idx += 1
                
            except Exception as e:
                logger.error(f"Failed to process batch {batch_num}: {e}")
                stats['failed'] += len(batch)
                # –ü—ã—Ç–∞–µ–º—Å—è —É–¥–∞–ª–∏—Ç—å –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
                try:
                    self.client.delete_forecast(forecast_id, debug=False, skip_wait=True)
                except:
                    pass
                processed += 1
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 5 –±–∞—Ç—á–µ–π
            if show_progress and processed % 5 == 0:
                progress_percent = (processed / total_batches) * 100
                queries_processed = min(processed * YandexDirectClient.MAX_PHRASES_PER_REQUEST, len(valid_queries))
                print(f"  ‚è≥ –ü—Ä–æ–≥—Ä–µ—Å—Å: {processed}/{total_batches} –±–∞—Ç—á–µ–π ({progress_percent:.1f}%) | {queries_processed}/{len(valid_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        return stats

