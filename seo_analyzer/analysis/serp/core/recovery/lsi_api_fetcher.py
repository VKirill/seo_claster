"""
–ü–æ–ª—É—á–µ–Ω–∏–µ LSI —Ñ—Ä–∞–∑ —á–µ—Ä–µ–∑ API
"""

import asyncio
import json
from typing import List, Tuple
from datetime import datetime

from ...async_batch_client import AsyncBatchSERPClient, PendingRequest
from seo_analyzer.core.serp_data_enricher import SERPDataEnricher
from seo_analyzer.core.lsi_extractor import LSIExtractor


class LSIApiFetcher:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ LSI —á–µ—Ä–µ–∑ API"""
    
    def __init__(self, api_key: str, lr: int, db_path: str):
        """
        Args:
            api_key: API –∫–ª—é—á
            lr: –†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞
            db_path: –ü—É—Ç—å –∫ Master DB
        """
        self.api_key = api_key
        self.lr = lr
        self.db_path = db_path
    
    async def fetch_lsi_for_queries(self, queries_with_req_id: List[Tuple]) -> int:
        """
        –ü–æ–ª—É—á–∏—Ç—å LSI —Ñ—Ä–∞–∑—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ API
        
        Args:
            queries_with_req_id: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å req_id –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ API
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        if not queries_with_req_id:
            print("   ‚ö†Ô∏è  LSIApiFetcher: —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø—É—Å—Ç")
            return 0
        
        print(f"   üîç LSIApiFetcher: –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(queries_with_req_id)} –∑–∞–ø—Ä–æ—Å–æ–≤")
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–º–µ—Ä—ã req_id
        if queries_with_req_id:
            sample_req_id = queries_with_req_id[0][2] if len(queries_with_req_id[0]) > 2 else None
            print(f"   üîç LSIApiFetcher: –ø—Ä–∏–º–µ—Ä req_id: '{sample_req_id}'")
        
        batch_client = AsyncBatchSERPClient(
            api_key=self.api_key,
            lr=self.lr,
            max_concurrent_send=10,
            max_concurrent_fetch=50,
            initial_delay=5,
            retry_delay=5,
            max_attempts=20,
            requests_per_second=90.0
        )
        
        import sqlite3
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            pending_requests = []
            for keyword, _, req_id, _ in queries_with_req_id:
                if not req_id or not req_id.strip():
                    print(f"   ‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω –∑–∞–ø—Ä–æ—Å '{keyword[:50]}...': req_id –ø—É—Å—Ç–æ–π")
                    continue
                pending_requests.append(
                    PendingRequest(query=keyword, req_id=req_id, sent_at=datetime.now())
                )
            
            if not pending_requests:
                print("   ‚ö†Ô∏è  LSIApiFetcher: –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö req_id –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                return 0
            
            print(f"   üîç LSIApiFetcher: —Å–æ–∑–¥–∞–Ω–æ {len(pending_requests)} PendingRequest –æ–±—ä–µ–∫—Ç–æ–≤")
            
            fetch_semaphore = asyncio.Semaphore(50)
            updated_count = 0
            
            enricher = SERPDataEnricher()
            lsi_extractor = LSIExtractor()
            
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –¥–∞–Ω–Ω—ã–º –∑–∞–ø—Ä–æ—Å–∞
            query_data_map = {}
            for keyword, _, req_id, query_group in queries_with_req_id:
                query_data_map[keyword] = (req_id, query_group)
            
            async def process_single_lsi_query(pending: PendingRequest):
                """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è LSI"""
                nonlocal updated_count
                
                async with fetch_semaphore:
                    result = await batch_client.result_fetcher.fetch_result_by_req_id(
                        pending, fetch_semaphore, None
                    )
                    
                    if isinstance(result, Exception):
                        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –¥–ª—è '{pending.query[:50]}...': {result}")
                        return
                    
                    if isinstance(result, dict) and result.get('status') == 'completed':
                        xml_text = result.get('xml_response')
                        if xml_text:
                            enriched = enricher.enrich_from_serp(xml_text, pending.query)
                            lsi_phrases = lsi_extractor.extract_from_serp_documents(
                                enriched['documents'], pending.query
                            )
                            
                            top_urls_new = []
                            for idx, doc in enumerate(enriched['documents'][:20], 1):
                                top_urls_new.append({
                                    'position': idx,
                                    'url': doc.get('url', ''),
                                    'domain': doc.get('domain', ''),
                                    'title': doc.get('title', ''),
                                    'snippet': doc.get('snippet', ''),
                                    'passages': doc.get('passages', ''),
                                    'is_commercial': doc.get('is_commercial', False)
                                })
                            
                            top_urls_json_new = json.dumps(top_urls_new, ensure_ascii=False)
                            lsi_json = json.dumps(lsi_phrases, ensure_ascii=False) if lsi_phrases else '[]'
                            
                            req_id, query_group = query_data_map.get(pending.query, (None, None))
                            if query_group:
                                cursor.execute('''
                                    UPDATE master_queries
                                    SET serp_top_urls = ?, serp_lsi_phrases = ?
                                    WHERE group_name = ? AND keyword = ?
                                ''', (top_urls_json_new, lsi_json, query_group, pending.query))
                                
                                query_short = pending.query[:50] + "..." if len(pending.query) > 50 else pending.query
                                urls_count = len(enriched['documents'])
                                lsi_count = len(lsi_phrases)
                                print(f"     ‚úì '{query_short}': {urls_count} URLs, {lsi_count} LSI —Ñ—Ä–∞–∑")
                                
                                updated_count += 1
                                if updated_count % 10 == 0:
                                    conn.commit()
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ (streaming —Ä–µ–∂–∏–º)
            tasks = [asyncio.create_task(process_single_lsi_query(pending)) for pending in pending_requests]
            await asyncio.gather(*tasks, return_exceptions=True)
            
            conn.commit()
            print(f"   ‚úì LSIApiFetcher: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {updated_count} –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ {len(pending_requests)}")
            if updated_count == 0 and len(pending_requests) > 0:
                print(f"   ‚ö†Ô∏è  LSIApiFetcher: –Ω–∏ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –Ω–µ –±—ã–ª –æ–±–Ω–æ–≤–ª–µ–Ω!")
                print(f"   üîç –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ req_id –≤–∞–ª–∏–¥–Ω—ã –∏ –¥–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ API")
            return updated_count
        except Exception as e:
            print(f"   ‚ùå LSIApiFetcher: –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
            import traceback
            traceback.print_exc()
            return 0
        finally:
            conn.close()
            await batch_client.close()

