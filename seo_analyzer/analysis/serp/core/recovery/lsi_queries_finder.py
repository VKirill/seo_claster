"""
–ü–æ–∏—Å–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ LSI —Ñ—Ä–∞–∑
"""

import sqlite3
import json
from typing import List, Tuple, Dict, Any
from datetime import datetime, timedelta

from .lsi_validator import LSIValidator


class LSIQueriesFinder:
    """–ü–æ–∏—Å–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ LSI"""
    
    def __init__(self, db_path: str):
        """
        Args:
            db_path: –ü—É—Ç—å –∫ Master DB
        """
        self.db_path = db_path
        self.validator = LSIValidator()
    
    def find_queries_without_lsi(self, group_name: str = None) -> Tuple[List[Tuple], Dict[str, int]]:
        """
        –ù–∞–π—Ç–∏ –∑–∞–ø—Ä–æ—Å—ã –±–µ–∑ LSI —Ñ—Ä–∞–∑
        
        Args:
            group_name: –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã (–µ—Å–ª–∏ None, –∏—â–µ—Ç –≤–æ –≤—Å–µ—Ö –≥—Ä—É–ø–ø–∞—Ö)
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (—Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        if group_name:
            cursor.execute('''
                SELECT keyword, serp_top_urls, serp_req_id, group_name, serp_lsi_phrases, serp_updated_at
                FROM master_queries
                WHERE group_name = ?
                  AND serp_status = 'completed'
            ''', (group_name,))
        else:
            cursor.execute('''
                SELECT keyword, serp_top_urls, serp_req_id, group_name, serp_lsi_phrases, serp_updated_at
                FROM master_queries
                WHERE serp_status = 'completed'
            ''')
        
        all_queries = cursor.fetchall()
        conn.close()
        
        queries_to_process = []
        stats = {
            'with_urls_with_lsi': 0,
            'with_urls_no_lsi': 0,
            'no_urls_with_lsi': 0,
            'no_urls_no_lsi': 0
        }
        
        for row in all_queries:
            if len(row) == 6:
                keyword, top_urls_json, req_id, query_group, lsi_json, serp_updated_at = row
            else:
                # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç –±–µ–∑ serp_updated_at
                keyword, top_urls_json, req_id, query_group, lsi_json = row[:5]
                serp_updated_at = None
            
            has_urls = self._has_urls(top_urls_json)
            has_lsi = self.validator.has_valid_lsi(lsi_json)
            
            if has_urls and has_lsi:
                stats['with_urls_with_lsi'] += 1
            elif has_urls and not has_lsi:
                stats['with_urls_no_lsi'] += 1
                queries_to_process.append((keyword, top_urls_json, req_id, query_group, serp_updated_at))
            elif not has_urls and has_lsi:
                stats['no_urls_with_lsi'] += 1
            else:
                stats['no_urls_no_lsi'] += 1
                queries_to_process.append((keyword, top_urls_json, req_id, query_group, serp_updated_at))
        
        return queries_to_process, stats
    
    def _has_urls(self, top_urls_json: Any) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ URL –≤ –¥–∞–Ω–Ω—ã—Ö"""
        if not top_urls_json:
            return False
        try:
            top_urls = json.loads(top_urls_json) if isinstance(top_urls_json, str) else top_urls_json
            if isinstance(top_urls, list) and len(top_urls) > 0:
                first_item = top_urls[0]
                if isinstance(first_item, str):
                    return True
                elif isinstance(first_item, dict):
                    if 'position' in first_item or 'url' in first_item:
                        return True
        except (json.JSONDecodeError, TypeError):
            pass
        return False
    
    def split_queries_by_processing_type(
        self, 
        queries_to_process: List[Tuple]
    ) -> Tuple[List[Tuple], List[Tuple]]:
        """
        –†–∞–∑–¥–µ–ª–∏—Ç—å –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ —Ç–µ, —á—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —á–µ—Ä–µ–∑ API –∏ —Ç–µ, –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –º–æ–∂–Ω–æ –∏–∑–≤–ª–µ—á—å LSI –ª–æ–∫–∞–ª—å–Ω–æ
        
        Args:
            queries_to_process: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–º–æ–∂–µ—Ç –≤–∫–ª—é—á–∞—Ç—å serp_updated_at)
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (–∑–∞–ø—Ä–æ—Å—ã –¥–ª—è API, –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è)
        """
        queries_with_req_id = []
        queries_with_full_data = []
        queries_needing_new_request = []  # –ó–∞–ø—Ä–æ—Å—ã —Å —É—Å—Ç–∞—Ä–µ–≤—à–∏–º req_id
        
        # –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ req_id - 10 –º–∏–Ω—É—Ç
        req_id_max_age = timedelta(minutes=10)
        now = datetime.now()
        
        expired_count = 0
        
        for row in queries_to_process:
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Å—Ç–∞—Ä—ã–π –∏ –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç
            if len(row) == 5:
                keyword, top_urls_json, req_id, query_group, serp_updated_at = row
            else:
                keyword, top_urls_json, req_id, query_group = row[:4]
                serp_updated_at = None
            if isinstance(top_urls_json, str):
                top_urls = json.loads(top_urls_json) if top_urls_json.strip() else []
            else:
                top_urls = top_urls_json if top_urls_json else []
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ URL –¥–∞–Ω–Ω—ã—Ö
            has_urls = len(top_urls) > 0
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –Ω—É–∂–Ω—ã –ª–∏ –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ API
            needs_api = False
            if not has_urls:
                # –ù–µ—Ç URL –¥–∞–Ω–Ω—ã—Ö - –Ω—É–∂–µ–Ω –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–Ω–µ –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –ø–æ req_id)
                needs_api = True
            elif len(top_urls) > 0:
                if isinstance(top_urls[0], str):
                    # –¢–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ URL –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö - –Ω—É–∂–µ–Ω API
                    needs_api = True
                elif isinstance(top_urls[0], dict):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–Ω–æ—Ç—É –¥–∞–Ω–Ω—ã—Ö
                    has_complete = 'snippet' in top_urls[0] and 'passages' in top_urls[0]
                    if not has_complete:
                        needs_api = True
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º req_id –±–æ–ª–µ–µ —è–≤–Ω–æ (–º–æ–∂–µ—Ç –±—ã—Ç—å None, –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏)
            has_valid_req_id = req_id and isinstance(req_id, str) and req_id.strip()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —É—Å—Ç–∞—Ä–µ–ª –ª–∏ req_id (–±–æ–ª—å—à–µ 10 –º–∏–Ω—É—Ç)
            req_id_expired = False
            if has_valid_req_id and serp_updated_at:
                try:
                    if isinstance(serp_updated_at, str):
                        updated_time = datetime.fromisoformat(serp_updated_at.replace('Z', '+00:00'))
                    else:
                        updated_time = serp_updated_at
                    
                    age = now - updated_time.replace(tzinfo=None) if updated_time.tzinfo else now - updated_time
                    if age > req_id_max_age:
                        req_id_expired = True
                        expired_count += 1
                except (ValueError, TypeError, AttributeError):
                    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –≤—Ä–µ–º—è, —Å—á–∏—Ç–∞–µ–º req_id –≤–∞–ª–∏–¥–Ω—ã–º
                    pass
            
            if needs_api:
                # –ï—Å–ª–∏ –Ω–µ—Ç URL –¥–∞–Ω–Ω—ã—Ö - —Å—Ä–∞–∑—É –Ω—É–∂–µ–Ω –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–Ω–µ –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –ø–æ req_id)
                if not has_urls:
                    queries_needing_new_request.append((keyword, top_urls_json, None, query_group))
                # –ï—Å–ª–∏ –µ—Å—Ç—å URL, –Ω–æ –Ω–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø–æ–ª—É—á–∏—Ç—å –ø–æ req_id
                elif has_valid_req_id and not req_id_expired:
                    # –ï—Å—Ç—å –≤–∞–ª–∏–¥–Ω—ã–π –∏ –Ω–µ —É—Å—Ç–∞—Ä–µ–≤—à–∏–π req_id - –ø–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ API
                    queries_with_req_id.append((keyword, top_urls_json, req_id, query_group))
                else:
                    # –ù–µ—Ç req_id –∏–ª–∏ –æ–Ω —É—Å—Ç–∞—Ä–µ–ª - –Ω—É–∂–µ–Ω –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
                    queries_needing_new_request.append((keyword, top_urls_json, None, query_group))
            else:
                # –ï—Å—Ç—å –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –º–æ–∂–Ω–æ –∏–∑–≤–ª–µ—á—å LSI –ª–æ–∫–∞–ª—å–Ω–æ
                queries_with_full_data.append((keyword, top_urls_json, req_id, query_group))
        
        # –ó–∞–ø—Ä–æ—Å—ã –±–µ–∑ URL –∏–ª–∏ —Å —É—Å—Ç–∞—Ä–µ–≤—à–∏–º req_id –¥–æ–±–∞–≤–ª—è–µ–º –≤ queries_with_full_data —Å req_id = None
        # —á—Ç–æ–±—ã recovery_handler –º–æ–≥ –∏—Ö –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ (—Å–±—Ä–æ—Å–∏—Ç—å –∏ —Å–¥–µ–ª–∞—Ç—å –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å)
        no_url_count = sum(1 for q in queries_needing_new_request if not self._has_urls(q[1]))
        if expired_count > 0 or no_url_count > 0:
            if expired_count > 0:
                print(f"   ‚è∞ –ù–∞–π–¥–µ–Ω–æ {expired_count} –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —É—Å—Ç–∞—Ä–µ–≤—à–∏–º req_id (>10 –º–∏–Ω—É—Ç)")
            if no_url_count > 0:
                print(f"   üì≠ –ù–∞–π–¥–µ–Ω–æ {no_url_count} –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ URL –¥–∞–Ω–Ω—ã—Ö - –Ω—É–∂–µ–Ω –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º req_id)")
        
        return queries_with_req_id, queries_with_full_data + queries_needing_new_request

