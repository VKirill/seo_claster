"""
–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ LSI —Ñ—Ä–∞–∑
"""

import asyncio
import sqlite3
import json
import aiohttp
import re
from typing import Dict, List, Any, Optional
from datetime import datetime

from .master_db_handler import MasterDBHandler
from ..async_batch_client import AsyncBatchSERPClient, PendingRequest
from seo_analyzer.core.serp_data_enricher import SERPDataEnricher
from seo_analyzer.core.lsi_extractor import LSIExtractor
from seo_analyzer.analysis.serp.api_semaphore import get_api_semaphore


class RecoveryHandler:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    def __init__(
        self,
        api_key: str,
        lr: int,
        master_db_handler: MasterDBHandler,
        query_group: str
    ):
        """
        Args:
            api_key: API –∫–ª—é—á
            lr: –†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞
            master_db_handler: –û–±—Ä–∞–±–æ—Ç—á–∏–∫ Master DB
            query_group: –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        self.api_key = api_key
        self.lr = lr
        self.master_db_handler = master_db_handler
        self.query_group = query_group
    
    async def recover_pending_requests(self) -> int:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ –≤—Å–µ—Ö –≥—Ä—É–ø–ø
        
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        if not self.master_db_handler.master_db:
            return 0
        
        print(f"\n{'='*80}")
        print(f"üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"{'='*80}")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –≥—Ä—É–ø–ø—ã —Å –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏
        conn = sqlite3.connect(self.master_db_handler.master_db.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT DISTINCT group_name 
            FROM master_queries
            WHERE (
                (serp_status = 'processing' AND serp_req_id IS NOT NULL)
                OR serp_status = 'pending'
                OR serp_status = 'error'
                OR serp_status = 'failed'
                OR (serp_status = 'completed' AND (serp_found_docs IS NULL OR serp_found_docs = 0))
            )
            ORDER BY group_name
        ''')
        groups_with_pending = [row[0] for row in cursor.fetchall()]
        
        if not groups_with_pending:
            print("‚úì –ù–µ—Ç –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è")
            conn.close()
            return 0
        
        print(f"üì¶ –ù–∞–π–¥–µ–Ω–æ –≥—Ä—É–ø–ø —Å –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏: {len(groups_with_pending)}")
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        all_pending = []
        for group_name in groups_with_pending:
            # –ó–∞–ø—Ä–æ—Å—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º processing —Å req_id
            cursor.execute('''
                SELECT keyword, serp_req_id, group_name, serp_status
                FROM master_queries
                WHERE group_name = ? AND serp_status = 'processing' AND serp_req_id IS NOT NULL
                ORDER BY serp_updated_at
            ''', (group_name,))
            
            for row in cursor.fetchall():
                all_pending.append({
                    'query': row[0],
                    'req_id': row[1],
                    'group': row[2],
                    'status': row[3],
                    'needs_new_request': False
                })
        
        conn.close()
        
        if not all_pending:
            print("‚úì –ù–µ—Ç –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è")
            return 0
        
        print(f"üìã –ù–∞–π–¥–µ–Ω–æ –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(all_pending)}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º AsyncBatchSERPClient –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
        batch_client = AsyncBatchSERPClient(
            api_key=self.api_key,
            lr=self.lr,
            max_concurrent_send=10,
            max_concurrent_fetch=20,
            initial_delay=5,
            retry_delay=10,
            max_attempts=50,
            requests_per_second=90.0
        )
        
        try:
            recoverable_by_req_id = [
                item for item in all_pending 
                if item.get('req_id') and not item.get('needs_new_request', False)
            ]
            
            pending_requests = [
                PendingRequest(query=item['query'], req_id=item['req_id'], sent_at=datetime.now())
                for item in recoverable_by_req_id
            ]
            
            if not pending_requests:
                return 0
            
            print(f"üîÑ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ {len(pending_requests)} –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ req_id...")
            
            fetch_semaphore = asyncio.Semaphore(50)
            recovered_count = 0
            batch_size = 500
            
            for i in range(0, len(pending_requests), batch_size):
                batch = pending_requests[i:i + batch_size]
                print(f"\n   –ë–∞—Ç—á –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è {i//batch_size + 1}: {len(batch)} –∑–∞–ø—Ä–æ—Å–æ–≤...")
                
                fetch_tasks = [
                    batch_client._fetch_result_by_req_id(pending, fetch_semaphore, None)
                    for pending in batch
                ]
                
                fetch_results = await asyncio.gather(*fetch_tasks, return_exceptions=True)
                
                enricher = SERPDataEnricher()
                lsi_extractor = LSIExtractor()
                
                for j, result in enumerate(fetch_results):
                    pending = batch[j]
                    original_group = recoverable_by_req_id[i + j]['group']
                    
                    if isinstance(result, dict) and result.get('status') == 'completed':
                        xml_text = result.get('xml_response')
                        if xml_text:
                            enriched = enricher.enrich_from_serp(xml_text, pending.query)
                            lsi_phrases = lsi_extractor.extract_from_serp_documents(
                                enriched['documents'], pending.query
                            )
                            
                            if self.master_db_handler.master_db:
                                try:
                                    self.master_db_handler.master_db.update_serp_status(
                                        group_name=original_group,
                                        keyword=pending.query,
                                        status='completed',
                                        req_id=pending.req_id
                                    )
                                    self.master_db_handler.master_db.update_serp_metrics(
                                        group_name=original_group,
                                        keyword=pending.query,
                                        metrics=enriched['metrics'],
                                        documents=enriched['documents'],
                                        lsi_phrases=lsi_phrases
                                    )
                                    
                                    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (–∫–∞–∫ –≤ batch_processor)
                                    query_short = pending.query[:50] + "..." if len(pending.query) > 50 else pending.query
                                    urls_count = len(enriched['documents'])
                                    lsi_count = len(lsi_phrases)
                                    print(f"     ‚úì '{query_short}': {urls_count} URLs, {lsi_count} LSI —Ñ—Ä–∞–∑")
                                    
                                    recovered_count += 1
                                except Exception as e:
                                    print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–ª—è '{pending.query[:50]}...': {e}")
                
                if i + batch_size < len(pending_requests):
                    await asyncio.sleep(2)
            
            print(f"\n‚úì –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ req_id: {recovered_count}")
            print(f"{'='*80}\n")
            
            return recovered_count
        finally:
            await batch_client.close()
    
    async def recover_missing_lsi_from_urls(self, group_name: str = None) -> int:
        """
        –î–æ—Å–æ–±—Ä–∞—Ç—å LSI —Ñ—Ä–∞–∑—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, —É –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å URL, –Ω–æ –Ω–µ—Ç LSI
        
        Args:
            group_name: –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è self.query_group)
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        if not self.master_db_handler.master_db:
            print("‚ö†Ô∏è  Master DB –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return 0
        
        conn = sqlite3.connect(self.master_db_handler.master_db.db_path)
        cursor = conn.cursor()
        
        group = group_name or self.query_group
        
        # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        if group:
            cursor.execute('''
                SELECT keyword, serp_top_urls, serp_req_id, group_name, serp_lsi_phrases
                FROM master_queries
                WHERE group_name = ?
                  AND serp_status = 'completed'
                  AND serp_req_id IS NOT NULL
                  AND serp_req_id != ''
            ''', (group,))
        else:
            cursor.execute('''
                SELECT keyword, serp_top_urls, serp_req_id, group_name, serp_lsi_phrases
                FROM master_queries
                WHERE serp_status = 'completed'
                  AND serp_req_id IS NOT NULL
                  AND serp_req_id != ''
            ''')
        
        all_queries = cursor.fetchall()
        
        # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–ª–∏—á–∏—è –≤–∞–ª–∏–¥–Ω—ã—Ö LSI –¥–∞–Ω–Ω—ã—Ö
        def has_valid_lsi(lsi_json_str):
            """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ –≤–∞–ª–∏–¥–Ω—ã–µ LSI –¥–∞–Ω–Ω—ã–µ –≤ JSON —Å—Ç—Ä–æ–∫–µ"""
            if not lsi_json_str or lsi_json_str.strip() == '' or lsi_json_str.strip() == '[]':
                return False
            try:
                lsi_data = json.loads(lsi_json_str)
                if not isinstance(lsi_data, list):
                    return False
                if len(lsi_data) == 0:
                    return False
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç —Å –ø–æ–ª–µ–º "phrase"
                for item in lsi_data:
                    if isinstance(item, dict) and 'phrase' in item:
                        return True
                return False
            except (json.JSONDecodeError, TypeError):
                return False
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã: –Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ —Ç–µ, —É –∫–æ—Ç–æ—Ä—ã—Ö –ù–ï–¢ –≤–∞–ª–∏–¥–Ω—ã—Ö LSI
        queries_to_process = []
        stats = {
            'with_urls_with_lsi': 0,
            'with_urls_no_lsi': 0,
            'no_urls_with_lsi': 0,
            'no_urls_no_lsi': 0
        }
        
        for keyword, top_urls_json, req_id, query_group, lsi_json in all_queries:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ URL (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –æ–±–∞ —Ñ–æ—Ä–º–∞—Ç–∞: –º–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫ –∏ –º–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤)
            has_urls = False
            if top_urls_json:
                try:
                    top_urls = json.loads(top_urls_json) if isinstance(top_urls_json, str) else top_urls_json
                    if isinstance(top_urls, list) and len(top_urls) > 0:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç: –º–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫ –∏–ª–∏ –º–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤
                        first_item = top_urls[0]
                        if isinstance(first_item, str):
                            # –ú–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫ - –µ—Å—Ç—å URL
                            has_urls = True
                        elif isinstance(first_item, dict):
                            # –ú–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤ - –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–æ–ª—è "position" –∏–ª–∏ "url"
                            if 'position' in first_item or 'url' in first_item:
                                has_urls = True
                except (json.JSONDecodeError, TypeError):
                    pass
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ LSI
            has_lsi = has_valid_lsi(lsi_json)
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            if has_urls and has_lsi:
                stats['with_urls_with_lsi'] += 1
            elif has_urls and not has_lsi:
                stats['with_urls_no_lsi'] += 1
                queries_to_process.append((keyword, top_urls_json, req_id, query_group))
            elif not has_urls and has_lsi:
                stats['no_urls_with_lsi'] += 1
                # –ù–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º - LSI —É–∂–µ –µ—Å—Ç—å, –ø—Ä–æ—Å—Ç–æ –Ω–µ—Ç URL
            else:  # not has_urls and not has_lsi
                stats['no_urls_no_lsi'] += 1
                queries_to_process.append((keyword, top_urls_json, req_id, query_group))
        
        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        total = len(all_queries)
        print(f"   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º:")
        print(f"      –° URL –∏ LSI: {stats['with_urls_with_lsi']} ({stats['with_urls_with_lsi']/total*100:.1f}%)")
        print(f"      –° URL –±–µ–∑ LSI: {stats['with_urls_no_lsi']} ({stats['with_urls_no_lsi']/total*100:.1f}%)")
        print(f"      –ë–µ–∑ URL —Å LSI: {stats['no_urls_with_lsi']} ({stats['no_urls_with_lsi']/total*100:.1f}%)")
        print(f"      –ë–µ–∑ URL –±–µ–∑ LSI: {stats['no_urls_no_lsi']} ({stats['no_urls_no_lsi']/total*100:.1f}%)")
        
        if not queries_to_process:
            print("‚úì –ù–µ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            conn.close()
            return 0
        
        print(f"   –ù–∞–π–¥–µ–Ω–æ {len(queries_to_process)} –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ LSI")
        
        lsi_extractor = LSIExtractor()
        enricher = SERPDataEnricher()
        updated_count = 0
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –∑–∞–ø—Ä–æ—Å—ã: —Ç–µ —á—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —á–µ—Ä–µ–∑ API (–µ—Å—Ç—å req_id) –∏ —Ç–µ –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –º–æ–∂–Ω–æ –∏–∑–≤–ª–µ—á—å LSI
        queries_with_req_id = []  # –ù—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —á–µ—Ä–µ–∑ API
        queries_with_full_data = []  # –ú–æ–∂–Ω–æ –∏–∑–≤–ª–µ—á—å LSI –∏–∑ –∏–º–µ—é—â–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö
        
        for keyword, top_urls_json, req_id, query_group in queries_to_process:
            if isinstance(top_urls_json, str):
                top_urls = json.loads(top_urls_json) if top_urls_json.strip() else []
            else:
                top_urls = top_urls_json if top_urls_json else []
            
            # –ï—Å–ª–∏ –Ω–µ—Ç URL –∏–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ–ø–æ–ª–Ω—ã–µ - –Ω—É–∂–µ–Ω API –∑–∞–ø—Ä–æ—Å
            needs_api = False
            if not top_urls:
                needs_api = True
            elif len(top_urls) > 0:
                if isinstance(top_urls[0], str):
                    # –ú–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫ - –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è LSI
                    needs_api = True
                elif isinstance(top_urls[0], dict):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–Ω–æ—Ç—É –¥–∞–Ω–Ω—ã—Ö
                    has_complete = 'snippet' in top_urls[0] and 'passages' in top_urls[0]
                    if not has_complete:
                        needs_api = True
            
            if needs_api and req_id:
                queries_with_req_id.append((keyword, top_urls_json, req_id, query_group))
            else:
                queries_with_full_data.append((keyword, top_urls_json, req_id, query_group))
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å req_id —á–µ—Ä–µ–∑ –±–∞—Ç—á–∏–Ω–≥ (–∫–∞–∫ –≤ recover_pending_requests)
        if queries_with_req_id:
            print(f"   üì§ –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ API –¥–ª—è {len(queries_with_req_id)} –∑–∞–ø—Ä–æ—Å–æ–≤...")
            
            batch_client = AsyncBatchSERPClient(
                api_key=self.api_key,
                lr=self.lr,
                max_concurrent_send=10,
                max_concurrent_fetch=50,  # –î–æ 50 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                initial_delay=5,
                retry_delay=5,
                max_attempts=20,
                requests_per_second=90.0
            )
            
            try:
                # –°–æ–∑–¥–∞–µ–º PendingRequest –æ–±—ä–µ–∫—Ç—ã
                pending_requests = [
                    PendingRequest(query=keyword, req_id=req_id, sent_at=datetime.now())
                    for keyword, _, req_id, _ in queries_with_req_id
                ]
                
                fetch_semaphore = asyncio.Semaphore(50)  # –î–æ 50 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                batch_size = 500
                
                for i in range(0, len(pending_requests), batch_size):
                    batch = pending_requests[i:i + batch_size]
                    
                    fetch_tasks = [
                        batch_client.result_fetcher.fetch_result_by_req_id(pending, fetch_semaphore, None)
                        for pending in batch
                    ]
                    
                    fetch_results = await asyncio.gather(*fetch_tasks, return_exceptions=True)
                    
                    for j, result in enumerate(fetch_results):
                        keyword, _, req_id, query_group = queries_with_req_id[i + j]
                        pending = batch[j]
                        
                        if isinstance(result, dict) and result.get('status') == 'completed':
                            xml_text = result.get('xml_response')
                            if xml_text:
                                enriched = enricher.enrich_from_serp(xml_text, keyword)
                                lsi_phrases = lsi_extractor.extract_from_serp_documents(
                                    enriched['documents'], keyword
                                )
                                
                                top_urls_new = []
                                for idx, doc in enumerate(enriched['documents'][:20], 1):
                                    top_urls_new.append({
                                        'position': idx,
                                        'url': doc.get('url', ''),
                                        'domain': doc.get('domain', ''),
                                        'title': doc.get('title', ''),
                                        'snippet': doc.get('snippet', ''),
                                        'passages': doc.get('passages', ''),
                                        'is_commercial': doc.get('is_commercial', False)
                                    })
                                
                                top_urls_json_new = json.dumps(top_urls_new, ensure_ascii=False)
                                lsi_json = json.dumps(lsi_phrases, ensure_ascii=False) if lsi_phrases else '[]'
                                
                                cursor.execute('''
                                    UPDATE master_queries
                                    SET serp_top_urls = ?, serp_lsi_phrases = ?
                                    WHERE group_name = ? AND keyword = ?
                                ''', (top_urls_json_new, lsi_json, query_group, keyword))
                                
                                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (–∫–∞–∫ –≤ batch_processor)
                                query_short = keyword[:50] + "..." if len(keyword) > 50 else keyword
                                urls_count = len(enriched['documents'])
                                lsi_count = len(lsi_phrases)
                                print(f"     ‚úì '{query_short}': {urls_count} URLs, {lsi_count} LSI —Ñ—Ä–∞–∑")
                                
                                updated_count += 1
                                if updated_count % 10 == 0:
                                    conn.commit()
                    
                    if i + batch_size < len(pending_requests):
                        await asyncio.sleep(2)
                
                conn.commit()
            finally:
                await batch_client.close()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–∏–∑–≤–ª–µ–∫–∞–µ–º LSI –∏–∑ –∏–º–µ—é—â–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ API)
        if queries_with_full_data:
            print(f"   üìù –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ LSI –∏–∑ –∏–º–µ—é—â–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {len(queries_with_full_data)} –∑–∞–ø—Ä–æ—Å–æ–≤...")
            
            for keyword, top_urls_json, req_id, query_group in queries_with_full_data:
                try:
                    if isinstance(top_urls_json, str):
                        top_urls = json.loads(top_urls_json) if top_urls_json.strip() else []
                    else:
                        top_urls = top_urls_json if top_urls_json else []
                    
                    if not top_urls:
                        continue
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º LSI –∏–∑ –∏–º–µ—é—â–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö
                    documents = []
                    has_title_data = False
                    
                    for item in top_urls:
                        if isinstance(item, dict):
                            if item.get('title'):
                                has_title_data = True
                            documents.append({
                                'title': item.get('title', ''),
                                'snippet': item.get('snippet', ''),
                                'passages': item.get('passages', ''),
                                'url': item.get('url', ''),
                                'domain': item.get('domain', ''),
                                'is_commercial': item.get('is_commercial', False)
                            })
                    
                    if has_title_data and documents:
                        lsi_phrases = lsi_extractor.extract_from_serp_documents(documents, keyword)
                        
                        if lsi_phrases:
                            top_urls_updated = []
                            for i, doc in enumerate(documents[:20], 1):
                                top_urls_updated.append({
                                    'position': i,
                                    'url': doc.get('url', ''),
                                    'domain': doc.get('domain', ''),
                                    'title': doc.get('title', ''),
                                    'snippet': doc.get('snippet', ''),
                                    'passages': doc.get('passages', ''),
                                    'is_commercial': doc.get('is_commercial', False)
                                })
                            
                            top_urls_json_updated = json.dumps(top_urls_updated, ensure_ascii=False)
                            lsi_json = json.dumps(lsi_phrases, ensure_ascii=False)
                            
                            cursor.execute('''
                                UPDATE master_queries
                                SET serp_top_urls = ?, serp_lsi_phrases = ?
                                WHERE group_name = ? AND keyword = ?
                            ''', (top_urls_json_updated, lsi_json, query_group, keyword))
                            
                            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (–∫–∞–∫ –≤ batch_processor)
                            query_short = keyword[:50] + "..." if len(keyword) > 50 else keyword
                            urls_count = len(documents)
                            lsi_count = len(lsi_phrases)
                            print(f"     ‚úì '{query_short}': {urls_count} URLs, {lsi_count} LSI —Ñ—Ä–∞–∑")
                            
                            updated_count += 1
                            if updated_count % 10 == 0:
                                conn.commit()
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ '{keyword[:50]}...': {e}")
                    continue
        
        conn.commit()
        conn.close()
        
        print(f"‚úì –î–æ—Å–æ–±–æ—Ä LSI –∑–∞–≤–µ—Ä—à–µ–Ω: –æ–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
        return updated_count
    

