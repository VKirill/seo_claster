"""
–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ
"""

from typing import List, Dict, Any, Optional, Callable

from .master_db_handler import MasterDBHandler
from .result_formatter import ResultFormatter
from ..sync_batch_client import SyncBatchSERPClient
from seo_analyzer.core.serp_data_enricher import SERPDataEnricher
from seo_analyzer.core.lsi_extractor import LSIExtractor
from ..utils.error_handler import create_error_result


class BatchProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –±–∞—Ç—á–µ–π –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    def __init__(
        self,
        api_key: str,
        lr: int,
        master_db_handler: MasterDBHandler,
        result_formatter: ResultFormatter,
        stats: Dict[str, int],
        recovery_handler=None,
        device: str = 'desktop',
        site: str = None,
        proxies: List[str] = None,
        proxy_file: str = None
    ):
        """
        Args:
            api_key: API –∫–ª—é—á
            lr: –†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞
            master_db_handler: –û–±—Ä–∞–±–æ—Ç—á–∏–∫ Master DB
            result_formatter: –§–æ—Ä–º–∞—Ç—Ç–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            stats: –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
            recovery_handler: –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (desktop, mobile, tablet, iphone, android)
            site: –î–æ–º–µ–Ω –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (site:domain.ru)
            proxies: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ ['http://user:pass@ip:port', ...]
            proxy_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –ø—Ä–æ–∫—Å–∏ (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É)
        """
        self.api_key = api_key
        self.lr = lr
        self.device = device
        self.site = site
        self.master_db_handler = master_db_handler
        self.result_formatter = result_formatter
        self.stats = stats
        self.recovery_handler = recovery_handler
        self.proxies = proxies
        self.proxy_file = proxy_file
    
    async def analyze_queries_batch_async_mode(
        self,
        queries: List[str],
        progress_callback: Optional[Callable] = None,
        auto_recover: bool = True,
        query_to_group_map: Dict[str, str] = None
    ) -> List[Dict[str, Any]]:
        """
        –ú–∞—Å—Å–æ–≤–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ SERP (streaming mode)
        
        Args:
            queries: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
            progress_callback: Callback –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            auto_recover: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ SERP
        """
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        if auto_recover and self.recovery_handler:
            await self.recovery_handler.recover_pending_requests()
            print(f"\n‚è≠Ô∏è  –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–π –≥—Ä—É–ø–ø—ã '{self.master_db_handler.query_group}'...")
        
        total = len(queries)
        self.stats['total_queries'] += total
        print(f"\nüöÄ BATCH ASYNC MODE: {total} –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        # –°—Ç—Ä–∏–º–∏–Ω–≥ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞ (–±–∞—Ç—á–∞–º–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
        print(f"üì¶ –°—Ç—Ä–∏–º–∏–Ω–≥ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞...")
        cached_results = {}
        uncached_queries = []
        empty_urls_count = 0  # –°—á–µ—Ç—á–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø—É—Å—Ç—ã–º serp_top_urls
        
        # –£–ë–†–ê–ù–û –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –ø–æ –í–°–ï–ú –≥—Ä—É–ø–ø–∞–º, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—É—â–µ–π
        # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à –∏–∑ –¥—Ä—É–≥–∏—Ö –≥—Ä—É–ø–ø –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ
        if self.master_db_handler.master_db:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –±–∞—Ç—á–∞–º–∏ (–ø–æ 1000 –∑–∞–ø—Ä–æ—Å–æ–≤) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            cache_batch_size = 1000
            total_batches = (total + cache_batch_size - 1) // cache_batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * cache_batch_size
                end_idx = min(start_idx + cache_batch_size, total)
                batch_queries = queries[start_idx:end_idx]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞ (–ø–æ –≤—Å–µ–º –≥—Ä—É–ø–ø–∞–º)
                batch_results = self.master_db_handler.batch_get_from_master_db(batch_queries)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–∞—Ç—á–∞
                for query in batch_queries:
                    master_cached = batch_results.get(query)
                    if master_cached:
                        self.stats['cached_from_master'] += 1
                        cached_results[query] = master_cached
                    else:
                        # –î–∞–Ω–Ω—ã—Ö –Ω–µ—Ç –∏–ª–∏ serp_top_urls –ø—É—Å—Ç–æ–π
                        empty_urls_count += 1
                        uncached_queries.append(query)
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫—ç—à–∞
                checked = end_idx
                cached_count = len(cached_results)
                uncached_count = len(uncached_queries)
                print(f"   –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: {checked}/{total} | –ö—ç—à: {cached_count} | –ó–∞–≥—Ä—É–∑–∏—Ç—å: {uncached_count}")
        else:
            # –ù–µ—Ç Master DB - –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å
            uncached_queries = queries.copy()
        
        print(f"‚úì –ó–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–æ: {len(cached_results)}/{total}")
        if empty_urls_count > 0:
            print(f"‚ö†Ô∏è  –ó–∞–ø—Ä–æ—Å–æ–≤ —Å –ø—É—Å—Ç—ã–º serp_top_urls: {empty_urls_count} (–±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∑–∞–Ω–æ–≤–æ)")
        print(f"üì§ –ù—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å: {len(uncached_queries)}")
        
        if not uncached_queries:
            return [cached_results.get(q, self._create_error_result(q, "Not found")) for q in queries]
        
        # –î–æ–±–∞–≤–ª—è–µ–º site: –∫ –∑–∞–ø—Ä–æ—Å–∞–º –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –¥–æ–º–µ–Ω
        actual_queries = []
        query_mapping = {}  # actual_query -> original_query
        for query in uncached_queries:
            if self.site:
                actual_query = f"{query} site:{self.site}"
            else:
                actual_query = query
            actual_queries.append(actual_query)
            query_mapping[actual_query] = query
        
        # Callback –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ req_id
        req_id_saved_count = [0]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∑–∞–º—ã–∫–∞–Ω–∏–∏
        data_saved_count = [0]  # –°—á–µ—Ç—á–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        def on_req_id_received(actual_query: str, req_id: str):
            """–°–æ—Ö—Ä–∞–Ω—è–µ–º req_id –≤ Master DB –°–†–ê–ó–£ –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è"""
            original_query = query_mapping.get(actual_query, actual_query)
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º group_name –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            target_group = None
            if query_to_group_map:
                target_group = query_to_group_map.get(original_query)
            if not target_group:
                target_group = self.master_db_handler.query_group
            
            if self.master_db_handler.master_db and target_group:
                self.master_db_handler.update_master_status(
                    original_query, 'processing', req_id=req_id, group_name=target_group
                )
                req_id_saved_count[0] += 1
                # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 50 –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ª—É—á—à–µ–π –≤–∏–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                if req_id_saved_count[0] % 50 == 0:
                    print(f"   üì§ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {req_id_saved_count[0]}/{len(uncached_queries)}")
                elif req_id_saved_count[0] % 100 == 0:
                    print(f"   ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ req_id: {req_id_saved_count[0]} –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±–æ–≥–∞—Ç–∏—Ç–µ–ª–∏ –æ–¥–∏–Ω —Ä–∞–∑
        enricher = SERPDataEnricher()
        lsi_extractor = LSIExtractor()
        
        # Callback –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –°–†–ê–ó–£
        def on_result_completed(raw_result: Dict[str, Any]):
            """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –°–†–ê–ó–£ –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è"""
            actual_query = raw_result.get('query', '')
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∏–∑ –º–∞–ø–ø–∏–Ω–≥–∞
            query = query_mapping.get(actual_query, actual_query)
            # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–æ–¥–µ—Ä–∂–∏—Ç site:, —É–¥–∞–ª—è–µ–º –µ–≥–æ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ
            if self.site and query.endswith(f" site:{self.site}"):
                query = query[:-len(f" site:{self.site}")]
            
            if not query or query in cached_results:
                return
            
            req_id = raw_result.get('req_id')
            
            if raw_result.get('status') == 'completed':
                xml_text = raw_result.get('xml_response')
                if xml_text:
                    enriched = enricher.enrich_from_serp(xml_text, query)
                    lsi_phrases = lsi_extractor.extract_from_serp_documents(enriched['documents'], query)
                    
                    result = {
                        'query': query,
                        'lr': self.lr,
                        'source': 'api_batch_async',
                        'cached_at': None,
                        'error': None,
                        'status': 'completed',
                        'req_id': req_id,
                        'xml_response': xml_text,
                        'metrics': enriched['metrics'],
                        'documents': enriched['documents'],
                        'lsi_phrases': lsi_phrases
                    }
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Master DB –°–†–ê–ó–£
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º group_name –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
                    target_group = None
                    if query_to_group_map:
                        target_group = query_to_group_map.get(query)
                    if not target_group:
                        target_group = self.master_db_handler.query_group
                    
                    if self.master_db_handler.master_db and target_group:
                        self.master_db_handler.update_master_status(
                            query, 'completed', req_id=req_id, group_name=target_group
                        )
                        self.master_db_handler.master_db.update_serp_metrics(
                            group_name=target_group,
                            keyword=query,
                            metrics=enriched['metrics'],
                            documents=enriched['documents'],
                            lsi_phrases=lsi_phrases
                        )
                        
                        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                        data_saved_count[0] += 1
                        
                        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–∫–∞–∂–¥—ã–µ 100 –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏)
                        if data_saved_count[0] % 100 == 0:
                            urls_count = len(enriched['documents'])
                            lsi_count = len(lsi_phrases)
                            print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î: {data_saved_count[0]} –∑–∞–ø—Ä–æ—Å–æ–≤ (–ø–æ—Å–ª–µ–¥–Ω–∏–π: {urls_count} URLs, {lsi_count} LSI —Ñ—Ä–∞–∑)")
                        else:
                            # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ - —Ç–æ–ª—å–∫–æ –∫—Ä–∞—Ç–∫–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 10
                            if data_saved_count[0] % 10 == 0:
                                query_short = query[:50] + "..." if len(query) > 50 else query
                                urls_count = len(enriched['documents'])
                                lsi_count = len(lsi_phrases)
                                print(f"     ‚úì '{query_short}': {urls_count} URLs, {lsi_count} LSI —Ñ—Ä–∞–∑")
                    
                    cached_results[query] = result
                    self.stats['api_requests'] += 1
                else:
                    # –ù–µ—Ç XML –æ—Ç–≤–µ—Ç–∞
                    result = {
                        'query': query,
                        'lr': self.lr,
                        'source': 'error',
                        'cached_at': None,
                        'error': 'No XML response',
                        'status': 'error',
                        'req_id': req_id,
                        'metrics': SERPDataEnricher()._get_empty_metrics(),
                        'documents': [],
                        'lsi_phrases': []
                    }
                    cached_results[query] = result
                    self.stats['errors'] += 1
            else:
                # –û—à–∏–±–∫–∞ –∏–ª–∏ –¥—Ä—É–≥–æ–π —Å—Ç–∞—Ç—É—Å
                error_msg = raw_result.get('error', 'Unknown error')
                
                result = {
                    'query': query,
                    'lr': self.lr,
                    'source': 'error',
                    'cached_at': None,
                    'error': error_msg,
                    'status': raw_result.get('status', 'error'),
                    'req_id': req_id,
                    'metrics': SERPDataEnricher()._get_empty_metrics(),
                    'documents': [],
                    'lsi_phrases': []
                }
                
                if self.master_db_handler.master_db and self.master_db_handler.query_group:
                    self.master_db_handler.update_master_status(
                        query, 'error', req_id=req_id, error_message=error_msg
                    )
                
                cached_results[query] = result
                self.stats['errors'] += 1
        
        # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –û–±—â–∞—è –æ—á–µ—Ä–µ–¥—å –∑–∞–ø—Ä–æ—Å–æ–≤, –≤—Å–µ –ø—Ä–æ–∫—Å–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        # –ö–∞–∂–¥—ã–π –ø—Ä–æ–∫—Å–∏ –±–µ—Ä–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –∏–∑ –æ–±—â–µ–π –æ—á–µ—Ä–µ–¥–∏ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é: –æ—Ç–ø—Ä–∞–≤–∏–ª ‚Üí –ø–æ–ª—É—á–∏–ª ‚Üí –ø–æ—à–µ–ª –¥–∞–ª—å—à–µ
        from ..batch.proxy_manager import ProxyManager
        import asyncio
        
        temp_proxy_manager = ProxyManager(proxies=self.proxies, proxy_file=self.proxy_file)
        proxy_count = temp_proxy_manager.get_proxy_count()
        
        if proxy_count > 0:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç —Å –æ–±—â–µ–π –æ—á–µ—Ä–µ–¥—å—é
            from ..async_queue_client import AsyncQueueSERPClient
            
            client = AsyncQueueSERPClient(
                api_key=self.api_key,
                lr=self.lr,
                requests_per_second=40.0,  # 40 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É –Ω–∞ IP (—É–≤–µ–ª–∏—á–µ–Ω–æ)
                initial_delay=0.5,  # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ 0.5 —Å–µ–∫ –ø–æ—Å–ª–µ –æ—Ç–ø—Ä–∞–≤–∫–∏ (—É–º–µ–Ω—å—à–µ–Ω–æ)
                retry_delay=0.5,  # 0.5 —Å–µ–∫ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ (—É–º–µ–Ω—å—à–µ–Ω–æ)
                max_attempts=50,
                device=self.device,
                proxies=self.proxies,
                proxy_file=self.proxy_file,
                silent=False
            )
            
            try:
                batch_result = await client.process_queries_batch(
                    queries=actual_queries,
                    progress_callback=progress_callback,
                    on_req_id_received=on_req_id_received,
                    on_result_completed=on_result_completed
                )
            finally:
                await client.close()
        else:
            # –ù–µ—Ç –ø—Ä–æ–∫—Å–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–∏–Ω –∫–ª–∏–µ–Ω—Ç —Å–æ —Å—Ç—Ä–æ–≥–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
            print(f"‚ö° –ë–µ–∑ –ø—Ä–æ–∫—Å–∏ - —Å—Ç—Ä–æ–≥–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞")
            print(f"   –û–±—Ä–∞–±–æ—Ç–∫–∞: –æ—Ç–ø—Ä–∞–≤–∏–ª ‚Üí —á–µ—Ä–µ–∑ {2}—Å –ø—Ä–æ–≤–µ—Ä–∏–ª ‚Üí –∑–∞–ø–∏—Å–∞–ª ‚Üí —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—Ä–æ—Å")
            print(f"   Rate limit: –º–∞–∫—Å–∏–º—É–º 50 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É")
            print(f"   –°—Ç—Ä–æ–≥–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ: –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∑–∞ —Ä–∞–∑")
            
            from ..sync_batch_client import SyncBatchSERPClient
            
            client = SyncBatchSERPClient(
                api_key=self.api_key,
                lr=self.lr,
                max_concurrent_send=1,  # –°—Ç—Ä–æ–≥–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ: –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∑–∞ —Ä–∞–∑
                max_concurrent_fetch=1,  # –¢–æ –∂–µ —Å–∞–º–æ–µ
                initial_delay=2,
                retry_delay=2,
                max_attempts=50,
                requests_per_second=50.0,  # –õ–∏–º–∏—Ç 50 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
                device=self.device,
                proxies=None,
                proxy_file=None,
                silent=False
            )
            
            try:
                batch_result = await client.process_queries_batch(
                    queries=actual_queries,
                    progress_callback=progress_callback,
                    on_req_id_received=on_req_id_received,
                    on_result_completed=on_result_completed
                )
            finally:
                await client.close()
        
        # batch_result —É–∂–µ –ø–æ–ª—É—á–µ–Ω –≤—ã—à–µ (–≤ –±–ª–æ–∫–µ —Å –ø—Ä–æ–∫—Å–∏ –∏–ª–∏ –±–µ–∑ –ø—Ä–æ–∫—Å–∏)
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –£–ñ–ï –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã —á–µ—Ä–µ–∑ callback on_result_completed
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–ø–∞–ª–∏ –≤ callback
        if 'batch_result' in locals():
            for raw_result in batch_result.get('results', []):
                query = raw_result.get('query')
                if query and query not in cached_results:
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –Ω–µ –±—ã–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –≤ callback
                    if raw_result.get('status') != 'completed':
                        error_msg = raw_result.get('error', 'Unknown error')
                        result = {
                            'query': query,
                            'lr': self.lr,
                            'source': 'error',
                            'cached_at': None,
                            'error': error_msg,
                            'status': raw_result.get('status', 'error'),
                            'req_id': raw_result.get('req_id'),
                            'metrics': SERPDataEnricher()._get_empty_metrics(),
                            'documents': [],
                            'lsi_phrases': []
                        }
                        cached_results[query] = result
                        self.stats['errors'] += 1
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        return [cached_results.get(q, self._create_error_result(q, "Not processed")) for q in queries]
    
    def _create_error_result(self, query: str, error: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –æ—à–∏–±–∫–æ–π"""
        empty_metrics = SERPDataEnricher()._get_empty_metrics()
        return create_error_result(query, error, self.lr, empty_metrics)

