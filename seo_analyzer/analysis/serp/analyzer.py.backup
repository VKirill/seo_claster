"""
SERP Analyzer
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä SERP —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤ SQLite –∏ —Ñ–∞–π–ª–æ–≤–æ–º –∫—ç—à–µ
"""

import asyncio
import json
import re
import requests
from typing import Optional, Dict, List, Any
from datetime import datetime

from .api_client import SERPAPIClient
from .async_batch_client import AsyncBatchSERPClient
from .sync_batch_client import SyncBatchSERPClient  # –ì–∏–±—Ä–∏–¥–Ω—ã–π –∫–ª–∏–µ–Ω—Ç (requests + async)
# from .cache_manager import SERPCacheManager  # –£–°–¢–ê–†–ï–õ–û: –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è (Master DB —Ç–æ–ª—å–∫–æ)
from seo_analyzer.core.serp_data_enricher import SERPDataEnricher
from seo_analyzer.core.lsi_extractor import LSIExtractor


class SERPAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä SERP —á–µ—Ä–µ–∑ xmlstock API"""
    
    def __init__(
        self,
        api_key: str,
        lr: int = 213,
        max_retries: int = 5,
        retry_delay: int = 5,
        timeout: int = 10,
        query_group: str = None,
        max_concurrent: int = 50,
        use_master_db: bool = True,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Master DB (–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫)
        use_batch_async: bool = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞—Å—Å–æ–≤—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ SERP
        
        Args:
            api_key: API –∫–ª—é—á xmlstock
            lr: –†–µ–≥–∏–æ–Ω (213 = –ú–æ—Å–∫–≤–∞)
            max_retries: –ú–∞–∫—Å –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
            retry_delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞
            query_group: –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã –∑–∞–ø—Ä–æ—Å–æ–≤
            max_concurrent: –ú–∞–∫—Å–∏–º—É–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–≥–ª–æ–±–∞–ª—å–Ω–æ –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø)
            use_master_db: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Master DB (–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö)
            use_batch_async: –ú–∞—Å—Å–æ–≤—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º (–æ—Ç–ø—Ä–∞–≤–∫–∞ –≤—Å–µ—Ö —Å—Ä–∞–∑—É, –ø–æ—Ç–æ–º –ø–æ–ª—É—á–µ–Ω–∏–µ)
        """
        self.api_key = api_key
        self.lr = lr
        self.query_group = query_group
        self.use_master_db = use_master_db
        self.use_batch_async = use_batch_async
        
        # Master DB - –ï–î–ò–ù–°–¢–í–ï–ù–ù–´–ô –∏—Å—Ç–æ—á–Ω–∏–∫ SERP –¥–∞–Ω–Ω—ã—Ö
        self.master_db = None
        if use_master_db:
            try:
                from seo_analyzer.core.cache.master_query_db import MasterQueryDatabase
                self.master_db = MasterQueryDatabase()
            except Exception as e:
                print(f"‚ö†Ô∏è  Master DB –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                self.master_db = None
        
        # API –∫–ª–∏–µ–Ω—Ç —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º —Å–µ–º–∞—Ñ–æ—Ä–æ–º
        self.api_client = SERPAPIClient(
            api_key=api_key,
            lr=lr,
            max_retries=max_retries,
            retry_delay=retry_delay,
            timeout=timeout,
            max_concurrent=max_concurrent
        )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_queries': 0,
            'cached_from_master': 0,
            'api_requests': 0,
            'errors': 0,
            'status_updated': 0,
        }
    
    async def analyze_query(
        self,
        query: str,
        force_refresh: bool = False
    ) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º —Å—Ç–∞—Ç—É—Å–∞"""
        self.stats['total_queries'] += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Master DB (–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫)
        if not force_refresh:
            if self.master_db and self.query_group:
                master_cached = self._get_from_master_db(query)
                if master_cached:
                    self.stats['cached_from_master'] += 1
                    return master_cached
        
        # –ù–µ—Ç –≤ Master DB - –¥–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ API
        return await self._fetch_from_api(query)
    
    async def analyze_queries_batch(
        self,
        queries: List[str],
        max_concurrent: int = None,  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞ —É—Ä–æ–≤–Ω–µ API –∫–ª–∏–µ–Ω—Ç–∞
        progress_callback: Optional[callable] = None,
        batch_size: int = 500  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (500 –∑–∞–¥–∞—á –Ω–∞ –±–∞—Ç—á, —Å–µ–º–∞—Ñ–æ—Ä –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å)
    ) -> List[Dict[str, Any]]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–∞–∫–µ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –±–∞—Ç—á–∏–Ω–≥–æ–º
        
        –†–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç—ã:
        - use_batch_async=True: –ú–∞—Å—Å–æ–≤–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –∑–∞—Ç–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ (–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø!)
        - use_batch_async=False: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏ (—Å—Ç–∞—Ä—ã–π —Ä–µ–∂–∏–º)
        
        Args:
            queries: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
            max_concurrent: –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
            progress_callback: Callback –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 500)
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ SERP –∞–Ω–∞–ª–∏–∑–∞
        """
        if not queries:
            return []
        
        # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω batch async —Ä–µ–∂–∏–º - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ!
        if self.use_batch_async:
            return await self.analyze_queries_batch_async_mode(queries, progress_callback)
        
        # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π —Ä–µ–∂–∏–º (–±–∞—Ç—á–∞–º–∏)
        total_queries = len(queries)
        self.stats['total_queries'] += total_queries
        
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ú–∞—Å—Å–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ Master DB
        cached_data = {}
        queries_need_api = []
        
        for query in queries:
            if self.master_db and self.query_group:
                master_cached = self._get_from_master_db(query)
                if master_cached:
                    self.stats['cached_from_master'] += 1
                    cached_data[query] = master_cached
                    continue
            queries_need_api.append(query)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ —Ç—Ä–µ–±—É—é—â–∏–µ API
        cached_queries = list(cached_data.keys())
        
        all_results = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–º–≥–Ω–æ–≤–µ–Ω–Ω–æ)
        for query in cached_queries:
            all_results.append(cached_data[query])
            if progress_callback:
                progress_callback(len(all_results), total_queries, query)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã —Ç—Ä–µ–±—É—é—â–∏–µ API (–±–∞—Ç—á–∞–º–∏)
        if queries_need_api:
            async def fetch_with_progress(query: str, index: int):
                result = await self._fetch_from_api(query)
                if progress_callback:
                    progress_callback(len(cached_queries) + index + 1, total_queries, query)
                return result
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º API –∑–∞–ø—Ä–æ—Å—ã –±–∞—Ç—á–∞–º–∏
            for batch_start in range(0, len(queries_need_api), batch_size):
                batch_end = min(batch_start + batch_size, len(queries_need_api))
                batch_queries = queries_need_api[batch_start:batch_end]
                
                # –°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞
                tasks = [
                    fetch_with_progress(q, batch_start + i) 
                    for i, q in enumerate(batch_queries)
                ]
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ–∫—É—â–∏–π –±–∞—Ç—á
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
                for i, result in enumerate(batch_results):
                    if isinstance(result, Exception):
                        self.stats['errors'] += 1
                        all_results.append(
                            self._create_error_result(batch_queries[i], str(result))
                        )
                    else:
                        all_results.append(result)
        
        return all_results
    
    def _create_error_result(self, query: str, error: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –æ—à–∏–±–∫–æ–π"""
        return {
            'query': query,
            'lr': self.lr,
            'source': 'error',
            'error': error,
            'metrics': self.api_client.enricher._get_empty_metrics(),
            'documents': [],
            'lsi_phrases': []
        }
    
    async def _fetch_from_api(self, query: str) -> Dict[str, Any]:
        """–ó–∞–ø—Ä–æ—Å –∫ API —á–µ—Ä–µ–∑ –∫–ª–∏–µ–Ω—Ç —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º —Å—Ç–∞—Ç—É—Å–∞"""
        self.stats['api_requests'] += 1
        
        # –û—Ç–º–µ—á–∞–µ–º –∫–∞–∫ processing –≤ Master DB
        if self.master_db and self.query_group:
            self._update_master_status(query, 'processing')
        
        api_result = await self.api_client.fetch_serp_data(query)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º req_id –µ—Å–ª–∏ –µ—Å—Ç—å
        req_id = api_result.get('req_id')
        
        if api_result.get('error'):
            self.stats['errors'] += 1
            error_text = api_result['error']
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –æ—à–∏–±–∫–∏
            is_temporary_error = any(keyword in error_text.lower() for keyword in [
                'timeout',           # Timeout –æ—à–∏–±–∫–∏
                'code="202"',        # –ó–∞–ø—Ä–æ—Å –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
                'code="210"',        # –ó–∞–ø—Ä–æ—Å –≤ –æ—á–µ—Ä–µ–¥–∏
                '–Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω',      # –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –æ—à–∏–±–∫–∏ 202
                '–≤ –æ—á–µ—Ä–µ–¥—å',         # –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –æ—à–∏–±–∫–∏ 210
                'network error',     # –°–µ—Ç–µ–≤—ã–µ –æ—à–∏–±–∫–∏
            ])
            
            result = self._create_result(query, api_result, 'error', error_text)
            
            # –î–æ–±–∞–≤–ª—è–µ–º req_id –∏ status –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result['req_id'] = req_id
            result['status'] = 'processing' if is_temporary_error else 'error'
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≤ Master DB
            if self.master_db and self.query_group:
                if is_temporary_error:
                    # –í—Ä–µ–º–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ - –æ—Å—Ç–∞–≤–ª—è–µ–º processing
                    self._update_master_status(query, 'processing', req_id=req_id, error_message=error_text)
                else:
                    # –ü–æ—Å—Ç–æ—è–Ω–Ω–∞—è –æ—à–∏–±–∫–∞
                    self._update_master_status(query, 'error', req_id=req_id, error_message=error_text)
            
            return result
        
        result = self._create_result(query, api_result)
        
        # –î–æ–±–∞–≤–ª—è–µ–º req_id –∏ status –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result['req_id'] = req_id
        result['status'] = 'completed'
        
        # –û—Ç–º–µ—á–∞–µ–º –∫–∞–∫ completed –≤ Master DB + —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        if self.master_db and self.query_group:
            self._update_master_status(query, 'completed', req_id=req_id)
        
        return result
    
    def _create_result(
        self,
        query: str,
        api_result: Dict[str, Any],
        source: str = 'api',
        error: Optional[str] = None
    ) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ API –æ—Ç–≤–µ—Ç–∞"""
        return {
            'query': query,
            'lr': self.lr,
            'source': source,
            'cached_at': None,
            'error': error,
            'metrics': api_result['metrics'],
            'documents': api_result['documents'],
            'lsi_phrases': api_result['lsi_phrases']
        }
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        return {
            'total_queries': self.stats['total_queries'],
            'cached_from_master': self.stats['cached_from_master'],
            'api_requests': self.stats['api_requests'],
            'errors': self.stats['errors'],
            'cache_hit_rate': (
                self.stats['cached_from_master'] / self.stats['total_queries'] * 100
                if self.stats['total_queries'] > 0 else 0
            )
        }
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –∏ –æ—Å–≤–æ–±–æ–¥–∏—Ç—å —Ä–µ—Å—É—Ä—Å—ã"""
        await self.api_client.close()
    
    def _get_from_master_db(self, query: str) -> Optional[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ Master DB
        –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â—É—é –≥—Ä—É–ø–ø—É, –∑–∞—Ç–µ–º –∏—â–µ–º –ø–æ –≤—Å–µ–º –≥—Ä—É–ø–ø–∞–º
        (–ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å completed –∏ –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö)
        
        –í–ê–ñ–ù–û: –ï—Å–ª–∏ —Å—Ç–∞—Ç—É—Å completed, –Ω–æ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç (found_docs = 0 –∏–ª–∏ NULL, –Ω–µ—Ç URL) -
        –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None —á—Ç–æ–±—ã –∑–∞–≥—Ä—É–∑–∏—Ç—å –∑–∞–Ω–æ–≤–æ
        """
        if not self.master_db:
            return None
        
        try:
            import sqlite3
            import json
            conn = sqlite3.connect(self.master_db.db_path)
            cursor = conn.cursor()
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â—É—é –≥—Ä—É–ø–ø—É (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞)
            if self.query_group:
                cursor.execute('''
                    SELECT 
                        serp_status,
                        serp_req_id,
                        serp_found_docs,
                        serp_main_pages_count,
                        serp_titles_with_keyword,
                        serp_commercial_domains,
                        serp_info_domains,
                        serp_top_urls,
                        serp_lsi_phrases
                    FROM master_queries
                    WHERE group_name = ? AND keyword = ?
                      AND serp_status = 'completed'
                ''', (self.query_group, query))
                
                row = cursor.fetchone()
                if row:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –µ—Å—Ç—å
                    found_docs = row[2]  # serp_found_docs
                    top_urls_json = row[7]  # serp_top_urls
                    
                    # –ï—Å–ª–∏ –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ - –¥–∞–Ω–Ω—ã–µ –Ω–µ–ø–æ–ª–Ω—ã–µ, –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∑–∞–Ω–æ–≤–æ
                    if found_docs is None or found_docs == 0:
                        conn.close()
                        return None  # –î–∞–Ω–Ω—ã—Ö –Ω–µ—Ç - –∑–∞–≥—Ä—É–∑–∏–º –∑–∞–Ω–æ–≤–æ
                    
                    # –ï—Å–ª–∏ found_docs > 0, –∑–Ω–∞—á–∏—Ç –¥–∞–Ω–Ω—ã–µ –µ—Å—Ç—å - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
                    # –î–∞–∂–µ –µ—Å–ª–∏ top_urls_json –ø—É—Å—Ç–æ–π –∏–ª–∏ None - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                    # –ì–ª–∞–≤–Ω–æ–µ —á—Ç–æ —Å—Ç–∞—Ç—É—Å 'completed' –∏ found_docs > 0
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ top_urls_json (–º–æ–∂–µ—Ç –±—ã—Ç—å NULL –∏–ª–∏ –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π)
                    if top_urls_json:
                        # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ - –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ–Ω–∞ –Ω–µ –ø—É—Å—Ç–∞—è
                        if isinstance(top_urls_json, str):
                            if top_urls_json.strip() == '':
                                # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
                                top_urls_json = '[]'
                        # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ —Å—Ç—Ä–æ–∫–∞ –∏ –Ω–µ None - –ø—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ –µ—Å—Ç—å
                    else:
                        # top_urls_json = None –∏–ª–∏ –ø—É—Å—Ç–æ–π - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
                        top_urls_json = '[]'
                    
                    # –ü–∞—Ä—Å–∏–º JSON (–¥–∞–∂–µ –µ—Å–ª–∏ —ç—Ç–æ –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤)
                    try:
                        top_urls = json.loads(top_urls_json) if isinstance(top_urls_json, str) else top_urls_json
                        # –ù–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø—É—Å—Ç–æ—Ç—É - –¥–∞–∂–µ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —ç—Ç–æ –≤–∞–ª–∏–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                        # –µ—Å–ª–∏ found_docs > 0 –∏ —Å—Ç–∞—Ç—É—Å 'completed'
                    except (json.JSONDecodeError, TypeError) as e:
                        # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ - –¥–∞–Ω–Ω—ã–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω—ã, –Ω–æ –µ—Å–ª–∏ found_docs > 0
                        # –ø–æ–ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
                        top_urls = []
                    
                    conn.close()
                    return self._format_serp_result(query, row)
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–µ–∫—É—â–µ–π –≥—Ä—É–ø–ø–µ - –∏—â–µ–º –ø–æ –≤—Å–µ–º –≥—Ä—É–ø–ø–∞–º
            cursor.execute('''
                SELECT 
                    serp_status,
                    serp_req_id,
                    serp_found_docs,
                    serp_main_pages_count,
                    serp_titles_with_keyword,
                    serp_commercial_domains,
                    serp_info_domains,
                    serp_top_urls,
                    serp_lsi_phrases
                FROM master_queries
                WHERE keyword = ?
                  AND serp_status = 'completed'
                LIMIT 1
            ''', (query,))
            
            row = cursor.fetchone()
            
            if row:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –µ—Å—Ç—å
                found_docs = row[2]  # serp_found_docs
                top_urls_json = row[7]  # serp_top_urls
                
                # –ï—Å–ª–∏ –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ - –¥–∞–Ω–Ω—ã–µ –Ω–µ–ø–æ–ª–Ω—ã–µ, –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∑–∞–Ω–æ–≤–æ
                if found_docs is None or found_docs == 0:
                    conn.close()
                    return None  # –î–∞–Ω–Ω—ã—Ö –Ω–µ—Ç - –∑–∞–≥—Ä—É–∑–∏–º –∑–∞–Ω–æ–≤–æ
                
                # –ï—Å–ª–∏ found_docs > 0, –∑–Ω–∞—á–∏—Ç –¥–∞–Ω–Ω—ã–µ –µ—Å—Ç—å - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
                # –î–∞–∂–µ –µ—Å–ª–∏ top_urls_json –ø—É—Å—Ç–æ–π –∏–ª–∏ None - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                # –ì–ª–∞–≤–Ω–æ–µ —á—Ç–æ —Å—Ç–∞—Ç—É—Å 'completed' –∏ found_docs > 0
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ top_urls_json (–º–æ–∂–µ—Ç –±—ã—Ç—å NULL –∏–ª–∏ –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π)
                if top_urls_json:
                    # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ - –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ–Ω–∞ –Ω–µ –ø—É—Å—Ç–∞—è
                    if isinstance(top_urls_json, str):
                        if top_urls_json.strip() == '':
                            # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
                            top_urls_json = '[]'
                    # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ —Å—Ç—Ä–æ–∫–∞ –∏ –Ω–µ None - –ø—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ –µ—Å—Ç—å
                else:
                    # top_urls_json = None –∏–ª–∏ –ø—É—Å—Ç–æ–π - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
                    top_urls_json = '[]'
                
                # –ü–∞—Ä—Å–∏–º JSON (–¥–∞–∂–µ –µ—Å–ª–∏ —ç—Ç–æ –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤)
                try:
                    top_urls = json.loads(top_urls_json) if isinstance(top_urls_json, str) else top_urls_json
                    # –ù–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø—É—Å—Ç–æ—Ç—É - –¥–∞–∂–µ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —ç—Ç–æ –≤–∞–ª–∏–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    # –µ—Å–ª–∏ found_docs > 0 –∏ —Å—Ç–∞—Ç—É—Å 'completed'
                except (json.JSONDecodeError, TypeError) as e:
                    # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ - –¥–∞–Ω–Ω—ã–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω—ã, –Ω–æ –µ—Å–ª–∏ found_docs > 0
                    # –ø–æ–ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
                    top_urls = []
                
                conn.close()
                # –ù–∞–π–¥–µ–Ω–æ –≤ –¥—Ä—É–≥–æ–π –≥—Ä—É–ø–ø–µ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
                return self._format_serp_result(query, row)
            
            conn.close()
            return None
        
        except Exception as e:
            # –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è - –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º, –∑–∞–≥—Ä—É–∑–∏–º –∏–∑ API
            return None
    
    def _format_serp_result(self, query: str, row: tuple) -> Dict[str, Any]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç SERP –∏–∑ –ë–î"""
        import json
        
        # –ü–∞—Ä—Å–∏–º JSON –¥–∞–Ω–Ω—ã–µ (–∏–Ω–¥–µ–∫—Å—ã —Å–¥–≤–∏–Ω—É–ª–∏—Å—å –∏–∑-–∑–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è serp_req_id)
        # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        top_urls = []
        if row[7]:
            try:
                if isinstance(row[7], str):
                    if row[7].strip():
                        top_urls = json.loads(row[7])
                    else:
                        top_urls = []
                else:
                    top_urls = row[7] if row[7] else []
            except (json.JSONDecodeError, TypeError):
                top_urls = []
        
        lsi_phrases = []
        if row[8]:
            try:
                if isinstance(row[8], str):
                    if row[8].strip():
                        lsi_phrases = json.loads(row[8])
                    else:
                        lsi_phrases = []
                else:
                    lsi_phrases = row[8] if row[8] else []
            except (json.JSONDecodeError, TypeError):
                lsi_phrases = []
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç LSI —Ñ—Ä–∞–∑: –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
        normalized_lsi = []
        if lsi_phrases:
            for item in lsi_phrases:
                if isinstance(item, dict):
                    # –£–∂–µ —Å–ª–æ–≤–∞—Ä—å - –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–∞ 'phrase'
                    if 'phrase' in item:
                        normalized_lsi.append(item)
                    elif 'phrase' not in item and len(item) > 0:
                        # –°–ª–æ–≤–∞—Ä—å –±–µ–∑ –∫–ª—é—á–∞ 'phrase' - –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å—Ç—Ä–æ–∫—É
                        # –í–æ–∑–º–æ–∂–Ω–æ —ç—Ç–æ —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        continue
                elif isinstance(item, str):
                    # –°—Ç—Ä–æ–∫–∞ - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å
                    normalized_lsi.append({
                        'phrase': item,
                        'frequency': 1,
                        'source': 'unknown'
                    })
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç URL: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä–∏, –∞ –Ω–µ —Å—Ç—Ä–æ–∫–∏
        normalized_urls = []
        if top_urls:
            for item in top_urls:
                if isinstance(item, dict):
                    # –£–∂–µ —Å–ª–æ–≤–∞—Ä—å - –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    normalized_urls.append(item)
                elif isinstance(item, str):
                    # –°—Ç—Ä–æ–∫–∞ URL - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
                    normalized_urls.append({
                        'url': item,
                        'domain': '',
                        'title': '',
                        'snippet': '',
                        'passages': '',
                        'is_commercial': False
                    })
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–º —Å –æ—Å—Ç–∞–ª—å–Ω—ã–º –∫–æ–¥–æ–º
        return {
            'query': query,
            'source': 'master_db',
            'status': row[0],  # serp_status
            'req_id': row[1],  # serp_req_id
            'metrics': {
                'found_docs': row[2],
                'main_pages_count': row[3],
                'titles_with_keyword': row[4],
                'commercial_domains': row[5],
                'info_domains': row[6]
            },
            'documents': normalized_urls,  # TOP-20 URL (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ)
            'lsi_phrases': normalized_lsi,
            'xml_response': None  # –í Master DB –Ω–µ —Ö—Ä–∞–Ω–∏–º XML
        }
    
    def _update_master_status(
        self,
        query: str,
        status: str,
        req_id: str = None,
        error_message: str = None
    ):
        """–û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å SERP –∑–∞–ø—Ä–æ—Å–∞ –≤ Master DB"""
        if not self.master_db or not self.query_group:
            return
        
        try:
            self.master_db.update_serp_status(
                group_name=self.query_group,
                keyword=query,
                status=status,
                req_id=req_id,
                error_message=error_message
            )
            self.stats['status_updated'] += 1
        except Exception as e:
            # –ù–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –µ—Å–ª–∏ –Ω–µ –æ–±–Ω–æ–≤–∏—Ç—Å—è
            pass
    
    async def recover_pending_requests(self) -> int:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ –≤—Å–µ—Ö –≥—Ä—É–ø–ø
        
        –ò—â–µ—Ç –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º 'processing' —Å req_id –∏ –ø—ã—Ç–∞–µ—Ç—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
        –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–∞–∂–µ –µ—Å–ª–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –¥—Ä—É–≥–∞—è –≥—Ä—É–ø–ø–∞.
        
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        if not self.master_db:
            return 0
        
        print(f"\n{'='*80}")
        print(f"üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"{'='*80}")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –≥—Ä—É–ø–ø—ã —Å –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏
        import sqlite3
        conn = sqlite3.connect(self.master_db.db_path)
        cursor = conn.cursor()
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º 'processing', 'pending', 'error', 'failed' –∏–∑ –≤—Å–µ—Ö –≥—Ä—É–ø–ø
        # –¢–∞–∫–∂–µ –∏—â–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º 'completed', –Ω–æ –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö (found_docs = 0 –∏–ª–∏ NULL)
        # –í–∫–ª—é—á–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º 'error'/'failed' –ë–ï–ó req_id - –æ–Ω–∏ –±—É–¥—É—Ç –ø–µ—Ä–µ–∑–∞–ø—Ä–æ—à–µ–Ω—ã
        cursor.execute('''
            SELECT DISTINCT group_name 
            FROM master_queries
            WHERE (
                (serp_status = 'processing' AND serp_req_id IS NOT NULL)
                OR serp_status = 'pending'
                OR serp_status = 'error'
                OR serp_status = 'failed'
                OR (serp_status = 'completed' AND (serp_found_docs IS NULL OR serp_found_docs = 0))
            )
            ORDER BY group_name
        ''')
        groups_with_pending = [row[0] for row in cursor.fetchall()]
        
        if not groups_with_pending:
            print("‚úì –ù–µ—Ç –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è")
            conn.close()
            return 0
        
        print(f"üì¶ –ù–∞–π–¥–µ–Ω–æ –≥—Ä—É–ø–ø —Å –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏: {len(groups_with_pending)}")
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        all_pending = []
        for group_name in groups_with_pending:
            # –ó–∞–ø—Ä–æ—Å—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º processing —Å req_id (–º–æ–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ req_id)
            cursor.execute('''
                SELECT keyword, serp_req_id, group_name, serp_status
                FROM master_queries
                WHERE group_name = ?
                  AND serp_status = 'processing'
                  AND serp_req_id IS NOT NULL
                ORDER BY serp_updated_at
            ''', (group_name,))
            
            for row in cursor.fetchall():
                all_pending.append({
                    'query': row[0],
                    'req_id': row[1],
                    'group': row[2],
                    'status': row[3],
                    'needs_new_request': False  # –ú–æ–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ req_id
                })
            
            # –ó–∞–ø—Ä–æ—Å—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º pending (–Ω—É–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å)
            cursor.execute('''
                SELECT keyword, serp_req_id, group_name, serp_status
                FROM master_queries
                WHERE group_name = ?
                  AND serp_status = 'pending'
                ORDER BY created_at
            ''', (group_name,))
            
            for row in cursor.fetchall():
                all_pending.append({
                    'query': row[0],
                    'req_id': row[1],  # –ú–æ–∂–µ—Ç –±—ã—Ç—å NULL
                    'group': row[2],
                    'status': row[3],
                    'needs_new_request': True  # –ù—É–∂–µ–Ω –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
                })
            
            # –ó–∞–ø—Ä–æ—Å—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º error/failed —Å req_id (–º–æ–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ req_id)
            cursor.execute('''
                SELECT keyword, serp_req_id, group_name, serp_status
                FROM master_queries
                WHERE group_name = ?
                  AND (serp_status = 'error' OR serp_status = 'failed')
                  AND serp_req_id IS NOT NULL
                ORDER BY serp_updated_at
            ''', (group_name,))
            
            for row in cursor.fetchall():
                all_pending.append({
                    'query': row[0],
                    'req_id': row[1],
                    'group': row[2],
                    'status': row[3],
                    'needs_new_request': False  # –ú–æ–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ req_id
                })
            
            # –ó–∞–ø—Ä–æ—Å—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º error/failed –ë–ï–ó req_id (–Ω—É–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å)
            cursor.execute('''
                SELECT keyword, serp_req_id, group_name, serp_status
                FROM master_queries
                WHERE group_name = ?
                  AND (serp_status = 'error' OR serp_status = 'failed')
                  AND (serp_req_id IS NULL OR serp_req_id = '')
                ORDER BY serp_updated_at
            ''', (group_name,))
            
            for row in cursor.fetchall():
                all_pending.append({
                    'query': row[0],
                    'req_id': row[1],  # NULL –∏–ª–∏ –ø—É—Å—Ç–æ–π
                    'group': row[2],
                    'status': row[3],
                    'needs_new_request': True  # –ù—É–∂–µ–Ω –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
                })
            
            # –ó–∞–ø—Ä–æ—Å—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º completed, –Ω–æ –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö (–Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∑–∞–Ω–æ–≤–æ)
            cursor.execute('''
                SELECT keyword, serp_req_id, group_name, serp_status
                FROM master_queries
                WHERE group_name = ?
                  AND serp_status = 'completed'
                  AND (serp_found_docs IS NULL OR serp_found_docs = 0)
                ORDER BY serp_updated_at
            ''', (group_name,))
            
            for row in cursor.fetchall():
                all_pending.append({
                    'query': row[0],
                    'req_id': row[1],
                    'group': row[2]
                })
        
        conn.close()
        
        if not all_pending:
            print("‚úì –ù–µ—Ç –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è")
            conn.close()
            return 0
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –∑–∞–ø—Ä–æ—Å–æ–≤
        processing_count = sum(1 for item in all_pending if item.get('status') == 'processing')
        error_failed_count = sum(1 for item in all_pending if item.get('status') in ('error', 'failed'))
        pending_count = sum(1 for item in all_pending if item.get('status') == 'pending')
        completed_no_data = sum(1 for item in all_pending if item.get('status') == 'completed')
        
        print(f"üìã –ù–∞–π–¥–µ–Ω–æ –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(all_pending)}")
        print(f"   –ì—Ä—É–ø–ø—ã: {', '.join(groups_with_pending)}")
        if processing_count > 0:
            print(f"   ‚Ä¢ processing: {processing_count}")
        if error_failed_count > 0:
            print(f"   ‚Ä¢ error/failed: {error_failed_count}")
        if pending_count > 0:
            print(f"   ‚Ä¢ pending: {pending_count}")
        if completed_no_data > 0:
            print(f"   ‚Ä¢ completed –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö: {completed_no_data}")
        print()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º AsyncBatchSERPClient –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
        from .async_batch_client import PendingRequest
        
        batch_client = AsyncBatchSERPClient(
            api_key=self.api_key,
            lr=self.lr,
            max_concurrent_send=10,  # –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (–±—ã–ª–æ 50)
            max_concurrent_fetch=20,  # –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (–±—ã–ª–æ 50)
            initial_delay=5,  # –ú–µ–Ω—å—à–µ –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
            retry_delay=10,
            max_attempts=50,
            requests_per_second=90.0  # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è xmlstock: –Ω–µ –±–æ–ª–µ–µ 100 –∑–∞–ø—Ä–æ—Å–æ–≤/—Å–µ–∫
        )
        
        try:
            # –†–∞–∑–¥–µ–ª—è–µ–º –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ —Ç–µ —á—Ç–æ –º–æ–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ req_id –∏ —Ç–µ —á—Ç–æ –Ω—É–∂–¥–∞—é—Ç—Å—è –≤ –Ω–æ–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ
            recoverable_by_req_id = [item for item in all_pending if item.get('req_id') and not item.get('needs_new_request', False)]
            need_new_requests = [item for item in all_pending if item.get('needs_new_request', True) or not item.get('req_id')]
            total_recovered = 0
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ PendingRequest —Ç–æ–ª—å–∫–æ —Ç–µ —á—Ç–æ –∏–º–µ—é—Ç req_id
            pending_requests = [
                PendingRequest(
                    query=item['query'],
                    req_id=item['req_id'],
                    sent_at=datetime.now()
                )
                for item in recoverable_by_req_id
            ]
            
            if pending_requests:
                print(f"üîÑ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ {len(pending_requests)} –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ req_id...")
                
                # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                fetch_semaphore = asyncio.Semaphore(50)
                recovered_count = 0
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –ø–æ 500 –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
                batch_size = 500
                
                for i in range(0, len(pending_requests), batch_size):
                    batch = pending_requests[i:i + batch_size]
                    print(f"\n   –ë–∞—Ç—á –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è {i//batch_size + 1}: {len(batch)} –∑–∞–ø—Ä–æ—Å–æ–≤...")
                    
                    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    fetch_tasks = []
                    for pending in batch:
                        task = batch_client._fetch_result_by_req_id(
                            pending,
                            fetch_semaphore,
                            None
                        )
                        fetch_tasks.append(task)
                    
                    fetch_results = await asyncio.gather(*fetch_tasks, return_exceptions=True)
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    # –°–æ–∑–¥–∞—ë–º —ç–∫–∑–µ–º–ø–ª—è—Ä—ã enricher –∏ lsi_extractor –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    enricher = SERPDataEnricher()
                    lsi_extractor = LSIExtractor()
                    
                    for j, result in enumerate(fetch_results):
                        pending = batch[j]
                        original_group = recoverable_by_req_id[i + j]['group']
                        
                        if isinstance(result, dict):
                            if result.get('status') == 'completed':
                                # –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–∏–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                                xml_text = result.get('xml_response')
                                if xml_text:
                                    # –û–±–æ–≥–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                                    enriched = enricher.enrich_from_serp(xml_text, pending.query)
                                    lsi_phrases = lsi_extractor.extract_from_serp_documents(
                                        enriched['documents'],
                                        pending.query
                                    )
                                    
                                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Master DB
                                    if self.master_db:
                                        try:
                                            self.master_db.update_serp_status(
                                                group_name=original_group,
                                                keyword=pending.query,
                                                status='completed',
                                                req_id=pending.req_id
                                            )
                                            
                                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
                                            self.master_db.update_serp_metrics(
                                                group_name=original_group,
                                                keyword=pending.query,
                                                metrics=enriched['metrics'],
                                                documents=enriched['documents'],
                                                lsi_phrases=lsi_phrases
                                            )
                                            
                                            recovered_count += 1
                                        except Exception as e:
                                            print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–ª—è '{pending.query[:50]}...': {e}")
                    
                    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
                    if i + batch_size < len(pending_requests):
                        await asyncio.sleep(2)
                
                total_recovered += recovered_count
        finally:
            # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –∑–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏ aiohttp
            await batch_client.close()
        
        # –î–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ req_id –∏–ª–∏ –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö - —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Ç–µ–∫—É—â—É—é –≥—Ä—É–ø–ø—É –∏ –¥—Ä—É–≥–∏–µ
        if need_new_requests:
            current_group_requests = [item for item in need_new_requests if item['group'] == self.query_group]
            other_groups_requests = [item for item in need_new_requests if item['group'] != self.query_group]
            
            if current_group_requests:
                print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ req_id –≤ —Ç–µ–∫—É—â–µ–π –≥—Ä—É–ø–ø–µ '{self.query_group}': {len(current_group_requests)}")
                print(f"   ‚Üí –ë—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –°–ï–ô–ß–ê–° –≤ —Ä–∞–º–∫–∞—Ö —Ç–µ–∫—É—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
                print(f"   ‚Üí –û–Ω–∏ –ø–æ–ø–∞–¥—É—Ç –≤ —Å–ø–∏—Å–æ–∫ –Ω–µ–∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫—ç—à–∞")
            
            if other_groups_requests:
                print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ req_id –≤ –¥—Ä—É–≥–∏—Ö –≥—Ä—É–ø–ø–∞—Ö: {len(other_groups_requests)}")
                print(f"   ‚Üí –ë—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –∞–Ω–∞–ª–∏–∑–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –≥—Ä—É–ø–ø")
                # –û—Ç–º–µ—á–∞–µ–º –∏—Ö –∫–∞–∫ pending —á—Ç–æ–±—ã –æ–Ω–∏ –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –∑–∞–ø—É—Å–∫–µ
                for item in other_groups_requests:
                    if self.master_db:
                        try:
                            self.master_db.update_serp_status(
                                group_name=item['group'],
                                keyword=item['query'],
                                status='pending'  # –û—Ç–º–µ—á–∞–µ–º –∫–∞–∫ pending –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
                            )
                        except:
                            pass
        
        if not recoverable_by_req_id and not need_new_requests:
            print("‚úì –ù–µ—Ç –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è")
        
        print(f"\n‚úì –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ req_id: {total_recovered}")
        if need_new_requests:
            current_group_count = len([item for item in need_new_requests if item['group'] == self.query_group])
            other_groups_count = len([item for item in need_new_requests if item['group'] != self.query_group])
            if current_group_count > 0:
                print(f"‚úì –ë—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Å–µ–π—á–∞—Å (–≤ —Ç–µ–∫—É—â–µ–º –∞–Ω–∞–ª–∏–∑–µ): {current_group_count}")
            if other_groups_count > 0:
                print(f"‚úì –û—Ç–º–µ—á–µ–Ω–æ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –¥—Ä—É–≥–∏—Ö –≥—Ä—É–ø–ø–∞—Ö: {other_groups_count}")
        print(f"{'='*80}\n")
        
        return total_recovered
    
    async def analyze_queries_batch_async_mode(
        self,
        queries: List[str],
        progress_callback: Optional[callable] = None,
        auto_recover: bool = True
    ) -> List[Dict[str, Any]]:
        """
        –ú–∞—Å—Å–æ–≤–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ SERP (streaming mode)
        
        Workflow:
        0. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–µ—Å–ª–∏ auto_recover=True)
        1. –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        2. Streaming –æ–±—Ä–∞–±–æ—Ç–∫–∞: –º–∞–∫—Å–∏–º—É–º 50 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
           - –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å ‚Üí –ø–æ–ª—É—á–∞–µ–º req_id
           - –ñ–¥—ë–º 1 —Å–µ–∫
           - –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—Å –ø–æ–≤—Ç–æ—Ä–∞–º–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
           - –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –°–†–ê–ó–£
           - –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º —Å–ª–æ—Ç ‚Üí –±–µ—Ä—ë–º —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—Ä–æ—Å
        
        Args:
            queries: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
            progress_callback: Callback –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            auto_recover: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏–∑ –≤—Å–µ—Ö –≥—Ä—É–ø–ø
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ SERP
        """
        # ========================================
        # –≠–¢–ê–ü 0: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        # ========================================
        if auto_recover:
            await self.recover_pending_requests()
            print(f"\n‚è≠Ô∏è  –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–π –≥—Ä—É–ø–ø—ã '{self.query_group}'...")
        
        total = len(queries)
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
        self.stats['total_queries'] += total
        print(f"\nüöÄ BATCH ASYNC MODE: {total} –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        # ========================================
        # –≠–¢–ê–ü 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        # ========================================
        print(f"üì¶ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞...")
        cached_results = {}
        uncached_queries = []
        
        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: —Å–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å—Ç–∞—Ç—É—Å–∞–º –∑–∞–ø—Ä–æ—Å–æ–≤
        status_stats = {}
        
        for query in queries:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º Master DB
            if self.master_db and self.query_group:
                master_cached = self._get_from_master_db(query)
                if master_cached:
                    self.stats['cached_from_master'] += 1
                    cached_results[query] = master_cached
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º 'processing' —Å req_id
                # –ò—Ö –Ω–µ –Ω—É–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –∑–∞–Ω–æ–≤–æ - –æ–Ω–∏ –±—É–¥—É—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã —á–µ—Ä–µ–∑ recover_pending_requests
                import sqlite3
                conn = sqlite3.connect(self.master_db.db_path)
                cursor = conn.cursor()
                
                # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–æ–π —Å—Ç–∞—Ç—É—Å —É –∑–∞–ø—Ä–æ—Å–∞ –≤ Master DB
                cursor.execute('''
                    SELECT serp_status, serp_req_id, serp_found_docs, serp_top_urls
                    FROM master_queries
                    WHERE group_name = ? AND keyword = ?
                ''', (self.query_group, query))
                row = cursor.fetchone()
                
                if row:
                    status = row[0] or 'NULL'
                    req_id = row[1]
                    found_docs = row[2]
                    top_urls_json = row[3]
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å—Ç–∞—Ç—É—Å—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, "serp_status" –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞)
                    # –¢–∞–∫–∂–µ —É—á–∏—Ç—ã–≤–∞–µ–º "failed" –∫–∞–∫ —Å–∏–Ω–æ–Ω–∏–º "error"
                    valid_statuses = {'pending', 'processing', 'completed', 'error', 'failed'}
                    if status not in valid_statuses and status != 'NULL':
                        # –ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π —Å—Ç–∞—Ç—É—Å - —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ –æ—à–∏–±–∫—É –¥–∞–Ω–Ω—ã—Ö
                        status_key = f'INVALID_STATUS_{status}'
                        status_stats[status_key] = status_stats.get(status_key, 0) + 1
                        conn.close()
                        continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –±—É–¥–µ—Ç –ø–µ—Ä–µ–∑–∞–ø—Ä–æ—à–µ–Ω–æ
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏—á–∏–Ω—É –ø–æ—á–µ–º—É –∑–∞–ø—Ä–æ—Å –Ω–µ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω
                    if status == 'completed':
                        # –°—Ç–∞—Ç—É—Å completed, –Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º top_urls_json –±–µ–∑–æ–ø–∞—Å–Ω–æ
                        has_urls = top_urls_json and (
                            (isinstance(top_urls_json, str) and top_urls_json.strip() != '') or
                            (not isinstance(top_urls_json, str) and top_urls_json)
                        )
                        
                        if not has_urls:
                            status_key = 'completed_NO_URL'
                        elif found_docs is None or found_docs == 0:
                            status_key = 'completed_NO_DOCS'
                        else:
                            # –î–æ–ª–∂–µ–Ω –±—ã–ª –±—ã—Ç—å –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω, –Ω–æ –ø–æ—á–µ–º—É-—Ç–æ –Ω–µ –±—ã–ª
                            # –í–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–±–ª–µ–º–∞ –≤ –ø–∞—Ä—Å–∏–Ω–≥–µ JSON –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–∞–Ω–Ω—ã—Ö
                            status_key = 'completed_BUT_MISSED'
                    elif status in ('error', 'failed'):
                        # –°—Ç–∞—Ç—É—Å error/failed - –ø—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ req_id
                        if not req_id:
                            status_key = 'error_NO_REQ_ID'  # –ö—Ä–∏—Ç–∏—á–Ω–æ: –Ω–µ—Ç req_id –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
                        else:
                            status_key = 'error_WITH_REQ_ID'  # –ï—Å—Ç—å req_id, –º–æ–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
                    elif status == 'pending':
                        status_key = 'pending'
                    else:
                        status_key = status or 'NULL'
                    
                    status_stats[status_key] = status_stats.get(status_key, 0) + 1
                    
                    # –ï—Å–ª–∏ —Å—Ç–∞—Ç—É—Å 'processing' –∏ –µ—Å—Ç—å req_id - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Ç–ø—Ä–∞–≤–∫—É
                    if status == 'processing' and req_id:
                        cached_results[query] = {
                            'query': query,
                            'lr': self.lr,
                            'source': 'master_db',
                            'status': 'processing',
                            'req_id': req_id,
                            'error': '–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ',
                            'metrics': {},
                            'documents': [],
                            'lsi_phrases': []
                        }
                        self.stats['cached_from_master'] += 1
                        conn.close()
                        continue
                    
                    # –ï—Å–ª–∏ —Å—Ç–∞—Ç—É—Å 'completed' —Å –¥–∞–Ω–Ω—ã–º–∏, –Ω–æ –º–µ—Ç–æ–¥ _get_from_master_db –Ω–µ –Ω–∞—à—ë–ª –∏—Ö
                    # –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –ë–î (–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–±–ª–µ–º–∞ –≤ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–∞–Ω–Ω—ã—Ö)
                    if status == 'completed' and status_key == 'completed_BUT_MISSED':
                        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î
                        cursor.execute('''
                            SELECT 
                                serp_status,
                                serp_req_id,
                                serp_found_docs,
                                serp_main_pages_count,
                                serp_titles_with_keyword,
                                serp_commercial_domains,
                                serp_info_domains,
                                serp_top_urls,
                                serp_lsi_phrases
                            FROM master_queries
                            WHERE group_name = ? AND keyword = ?
                              AND serp_status = 'completed'
                        ''', (self.query_group, query))
                        full_row = cursor.fetchone()
                        
                        if full_row:
                            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ, –¥–∞–∂–µ –µ—Å–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –ø—Ä–æ—à–ª–∞
                            # –í–æ–∑–º–æ–∂–Ω–æ –¥–∞–Ω–Ω—ã–µ –µ—Å—Ç—å, –Ω–æ –≤ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
                            try:
                                import json
                                found_docs_check = full_row[2]
                                top_urls_json_check = full_row[7]
                                
                                # –ë–æ–ª–µ–µ –º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –µ—Å–ª–∏ found_docs > 0, –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å
                                if found_docs_check and found_docs_check > 0:
                                    # –ü—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON (–¥–∞–∂–µ –µ—Å–ª–∏ —ç—Ç–æ –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤)
                                    try:
                                        if isinstance(top_urls_json_check, str):
                                            top_urls_parsed = json.loads(top_urls_json_check) if top_urls_json_check.strip() else []
                                        else:
                                            top_urls_parsed = top_urls_json_check if top_urls_json_check else []
                                        
                                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ, –¥–∞–∂–µ –µ—Å–ª–∏ top_urls_parsed –ø—É—Å—Ç–æ–π
                                        # –ì–ª–∞–≤–Ω–æ–µ —á—Ç–æ found_docs > 0 –∏ —Å—Ç–∞—Ç—É—Å 'completed'
                                        cached_results[query] = self._format_serp_result(query, full_row)
                                        self.stats['cached_from_master'] += 1
                                        conn.close()
                                        continue
                                    except (json.JSONDecodeError, TypeError):
                                        # –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å - –Ω–æ –µ—Å–ª–∏ found_docs > 0, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
                                        try:
                                            # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ —Å –ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º URL
                                            cached_results[query] = self._format_serp_result(query, full_row)
                                            self.stats['cached_from_master'] += 1
                                            conn.close()
                                            continue
                                        except Exception:
                                            # –ï—Å–ª–∏ –∏ —ç—Ç–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º, –±—É–¥–µ—Ç –ø–µ—Ä–µ–∑–∞–ø—Ä–æ—à–µ–Ω–æ
                                            pass
                            except Exception:
                                # –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º, –±—É–¥–µ—Ç –ø–µ—Ä–µ–∑–∞–ø—Ä–æ—à–µ–Ω–æ
                                pass
                    
                    # –ï—Å–ª–∏ —Å—Ç–∞—Ç—É—Å 'error'/'failed' –±–µ–∑ req_id –∏–ª–∏ 'pending' - –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∑–∞–Ω–æ–≤–æ
                    # (–Ω–µ –¥–µ–ª–∞–µ–º continue, —á—Ç–æ–±—ã –∑–∞–ø—Ä–æ—Å –ø–æ–ø–∞–ª –≤ uncached_queries)
                    if status in ('error', 'failed', 'pending'):
                        conn.close()
                        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ - –∑–∞–ø—Ä–æ—Å –ø–æ–ø–∞–¥—ë—Ç –≤ uncached_queries
                    else:
                        # –î–ª—è –¥—Ä—É–≥–∏—Ö —Å—Ç–∞—Ç—É—Å–æ–≤ (completed –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö –∏ —Ç.–¥.) - —Ç–æ–∂–µ –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å
                        conn.close()
                        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ - –∑–∞–ø—Ä–æ—Å –ø–æ–ø–∞–¥—ë—Ç –≤ uncached_queries
                else:
                    # –ó–∞–ø—Ä–æ—Å –≤–æ–æ–±—â–µ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ Master DB
                    status_stats['NOT_FOUND'] = status_stats.get('NOT_FOUND', 0) + 1
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ –∫—ç—à–µ (completed —Å –¥–∞–Ω–Ω—ã–º–∏) - –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ API
            # –°—é–¥–∞ –ø–æ–ø–∞–¥–∞—é—Ç –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã: –Ω–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ, —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º error/failed/pending, completed –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö
            uncached_queries.append(query)
        
        print(f"‚úì –ó–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–æ: {len(cached_results)}/{total}")
        print(f"üì§ –ù—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å: {len(uncached_queries)}")
        
        if len(uncached_queries) > 0:
            print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É {len(uncached_queries)} –Ω–µ–∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É –ø–æ —Å—Ç–∞—Ç—É—Å–∞–º –Ω–µ–∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        if status_stats and uncached_queries:
            print(f"\nüîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –Ω–µ–∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:")
            for status, count in sorted(status_stats.items()):
                if status == 'NOT_FOUND':
                    print(f"   ‚Ä¢ –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤ Master DB: {count}")
                elif status.startswith('INVALID_STATUS_'):
                    invalid_value = status.replace('INVALID_STATUS_', '')
                    print(f"   ‚Ä¢ –ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π —Å—Ç–∞—Ç—É—Å '{invalid_value}': {count}")
                    print(f"     ‚Üí –°—Ç–∞—Ç—É—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å: pending/processing/completed/error")
                    print(f"     ‚Üí –ë—É–¥–µ—Ç –ø–µ—Ä–µ–∑–∞–ø—Ä–æ—à–µ–Ω–æ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö")
                elif status == 'completed_NO_URL':
                    print(f"   ‚Ä¢ –°—Ç–∞—Ç—É—Å 'completed', –Ω–æ –Ω–µ—Ç URL (serp_top_urls –ø—É—Å—Ç–æ–π): {count}")
                    print(f"     ‚Üí –ë—É–¥–µ—Ç –ø–µ—Ä–µ–∑–∞–ø—Ä–æ—à–µ–Ω–æ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö")
                elif status == 'completed_NO_DOCS':
                    print(f"   ‚Ä¢ –°—Ç–∞—Ç—É—Å 'completed', –Ω–æ –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (found_docs = 0): {count}")
                    print(f"     ‚Üí –ë—É–¥–µ—Ç –ø–µ—Ä–µ–∑–∞–ø—Ä–æ—à–µ–Ω–æ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö")
                elif status == 'completed_BUT_MISSED':
                    print(f"   ‚Ä¢ –°—Ç–∞—Ç—É—Å 'completed' —Å –¥–∞–Ω–Ω—ã–º–∏, –Ω–æ –Ω–µ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω: {count}")
                    print(f"     ‚Üí –í–æ–∑–º–æ–∂–Ω–∞ –æ—à–∏–±–∫–∞ –≤ –ª–æ–≥–∏–∫–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫—ç—à–∞")
                    print(f"     ‚Üí –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î (JSON –ø–∞—Ä—Å–∏–Ω–≥, —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö)")
                elif status == 'error_NO_REQ_ID':
                    print(f"   ‚Ä¢ –°—Ç–∞—Ç—É—Å 'error' –ë–ï–ó req_id (–∫—Ä–∏—Ç–∏—á–Ω–æ!): {count}")
                    print(f"     ‚Üí –ó–∞–ø—Ä–æ—Å—ã –±—ã–ª–∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã, –Ω–æ req_id –Ω–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω")
                    print(f"     ‚Üí –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö")
                    print(f"     ‚Üí –ë—É–¥–µ—Ç –ø–µ—Ä–µ–∑–∞–ø—Ä–æ—à–µ–Ω–æ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è")
                elif status == 'error_WITH_REQ_ID':
                    print(f"   ‚Ä¢ –°—Ç–∞—Ç—É—Å 'error' —Å req_id: {count}")
                    print(f"     ‚Üí –ú–æ–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —á–µ—Ä–µ–∑ req_id")
                    print(f"     ‚Üí –ë—É–¥–µ—Ç –ø–µ—Ä–µ–∑–∞–ø—Ä–æ—à–µ–Ω–æ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏")
                elif status == 'pending':
                    print(f"   ‚Ä¢ –°—Ç–∞—Ç—É—Å 'pending' (–µ—â—ë –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–ª—Å—è): {count}")
                else:
                    print(f"   ‚Ä¢ –°—Ç–∞—Ç—É—Å '{status}': {count}")
            print()
        
        print()
        
        if not uncached_queries:
            # –í—Å–µ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω—ã!
            return [cached_results[q] for q in queries]
        
        # ========================================
        # –≠–¢–ê–ü 2: Streaming –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ SyncBatchSERPClient (–≥–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º)
        # ========================================
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º SyncBatchSERPClient –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        # (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π requests –≤–Ω—É—Ç—Ä–∏ async executor)
        # Streaming —Ä–µ–∂–∏–º: –º–∞–∫—Å–∏–º—É–º 50 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π,
        # –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –ø–æ –º–µ—Ä–µ –∏—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        batch_client = SyncBatchSERPClient(
            api_key=self.api_key,
            lr=self.lr,
            max_concurrent_send=10,  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            max_concurrent_fetch=20,
            initial_delay=10,
            retry_delay=10,
            max_attempts=100,  # –ñ–¥—ë–º –ø–æ–∫–∞ –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–µ –±—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
            requests_per_second=90.0  # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è xmlstock: –Ω–µ –±–æ–ª–µ–µ 100 –∑–∞–ø—Ä–æ—Å–æ–≤/—Å–µ–∫ (–æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å)
        )
        
        # Callback –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ req_id
        def on_req_id_received(query: str, req_id: str):
            """
            –°–æ—Ö—Ä–∞–Ω—è–µ–º req_id –≤ Master DB –°–†–ê–ó–£ –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è
            
            –ü—Ä–∏ –æ—à–∏–±–∫–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–µ–ª–∞–µ–º –ø–∞—É–∑—É 10 —Å–µ–∫—É–Ω–¥ –∏ –ø–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–ø—ã—Ç–∫—É (–¥–æ 3 —Ä–∞–∑).
            –≠—Ç–æ –¥–∞—ë—Ç –ë–î –≤—Ä–µ–º—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è –ø—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö.
            """
            import time
            
            if not self.master_db or not self.query_group:
                print(f"   ‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: master_db –∏–ª–∏ query_group –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –¥–ª—è '{query[:50]}...'")
                print(f"      req_id '{req_id}' –Ω–µ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –ë–î")
                return
            
            max_retries = 3
            retry_delay = 10  # —Å–µ–∫—É–Ω–¥
            
            for attempt in range(max_retries):
                try:
                    self.master_db.update_serp_status(
                        group_name=self.query_group,
                        keyword=query,
                        status='processing',  # –°—Ç–∞—Ç—É—Å "–æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ, –∂–¥—ë–º –æ—Ç–≤–µ—Ç–∞"
                        req_id=req_id
                    )
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ req_id –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏–ª—Å—è
                    import sqlite3
                    conn = sqlite3.connect(self.master_db.db_path)
                    cursor = conn.cursor()
                    cursor.execute('''
                        SELECT serp_req_id FROM master_queries
                        WHERE group_name = ? AND keyword = ?
                    ''', (self.query_group, query))
                    saved_req_id = cursor.fetchone()
                    conn.close()
                    
                    if saved_req_id and saved_req_id[0] == req_id:
                        # –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ
                        if hasattr(self, '_req_id_saved_count'):
                            self._req_id_saved_count += 1
                        else:
                            self._req_id_saved_count = 1
                        
                        if self._req_id_saved_count % 100 == 0:
                            print(f"   ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ req_id: {self._req_id_saved_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
                        return  # –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ
                    else:
                        # req_id –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                        if attempt < max_retries - 1:
                            print(f"   ‚ö†Ô∏è  req_id '{req_id}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î –ø–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–ª—è '{query[:50]}...'")
                            print(f"      –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}, –ø–∞—É–∑–∞ {retry_delay} —Å–µ–∫...")
                            time.sleep(retry_delay)
                            continue
                        else:
                            print(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å req_id '{req_id}' –¥–ª—è '{query[:50]}...' –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                            print(f"      –ó–∞–ø—Ä–æ—Å –±—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω, –Ω–æ req_id –Ω–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –ë–î")
                            return
                    
                except Exception as e:
                    if attempt < max_retries - 1:
                        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è req_id –¥–ª—è '{query[:50]}...': {e}")
                        print(f"      –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}, –ø–∞—É–∑–∞ {retry_delay} —Å–µ–∫...")
                        time.sleep(retry_delay)
                        continue
                    else:
                        # –ü–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫ - –ª–æ–≥–∏—Ä—É–µ–º, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                        print(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å req_id '{req_id}' –¥–ª—è '{query[:50]}...' –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                        print(f"      –û—à–∏–±–∫–∞: {e}")
                        print(f"      –ó–∞–ø—Ä–æ—Å –±—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω, –Ω–æ req_id –Ω–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –ë–î")
                        return
        
        # Callback –¥–ª—è –ù–ï–ú–ï–î–õ–ï–ù–ù–û–ì–û —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        async def on_result_completed(raw_result: Dict):
            """–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –°–†–ê–ó–£ –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è"""
            enricher = SERPDataEnricher()
            lsi_extractor = LSIExtractor()
            
            query = raw_result['query']
            req_id = raw_result.get('req_id')
            
            if raw_result.get('status') == 'completed':
                # –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–∏–ª–∏ XML
                xml_text = raw_result['xml_response']
                
                # –û–±–æ–≥–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                enriched = enricher.enrich_from_serp(xml_text, query)
                lsi_phrases = lsi_extractor.extract_from_serp_documents(
                    enriched['documents'],
                    query
                )
                
                result = {
                    'query': query,
                    'lr': self.lr,
                    'source': 'api_batch_async',
                    'cached_at': None,
                    'error': None,
                    'status': 'completed',
                    'req_id': req_id,
                    'xml_response': xml_text,
                    'metrics': enriched['metrics'],
                    'documents': enriched['documents'],
                    'lsi_phrases': lsi_phrases
                }
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Master DB –ù–ï–ú–ï–î–õ–ï–ù–ù–û
                if self.master_db and self.query_group:
                    self._update_master_status(query, 'completed', req_id=req_id)
                    self.master_db.update_serp_metrics(
                        group_name=self.query_group,
                        keyword=query,
                        metrics=enriched['metrics'],
                        documents=enriched['documents'],
                        lsi_phrases=lsi_phrases
                    )
                    
                    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                    query_short = query[:50] + "..." if len(query) > 50 else query
                    urls_count = len(enriched['documents'])
                    lsi_count = len(lsi_phrases)
                    print(f"     ‚úì '{query_short}': {urls_count} URLs, {lsi_count} LSI —Ñ—Ä–∞–∑")
                
                cached_results[query] = result
                self.stats['api_requests'] += 1
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º streaming –æ–±—Ä–∞–±–æ—Ç–∫—É
        try:
            batch_result = await batch_client.process_queries_streaming(
                queries=uncached_queries,
                progress_callback=progress_callback,
                on_req_id_received=on_req_id_received,
                on_result_completed=on_result_completed,  # CALLBACK: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –°–†–ê–ó–£ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                max_concurrent=50,  # –ú–∞–∫—Å–∏–º—É–º 50 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
                fetch_delay=1,  # –ó–∞–¥–µ—Ä–∂–∫–∞ 1 —Å–µ–∫ –ø–µ—Ä–µ–¥ –ø–æ–ª—É—á–µ–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                max_fetch_attempts=20  # –ú–∞–∫—Å–∏–º—É–º 20 –ø–æ–ø—ã—Ç–æ–∫ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            )
            
            # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –£–ñ–ï —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã —á–µ—Ä–µ–∑ callback on_result_completed
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–ø–∞–ª–∏ –≤ callback
            for raw_result in batch_result['results']:
                query = raw_result['query']
                
                # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —É–∂–µ –≤ cached_results - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–æ–±—Ä–∞–±–æ—Ç–∞–Ω –≤ callback)
                if query in cached_results:
                    continue
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ failed –∑–∞–ø—Ä–æ—Å—ã (–Ω–µ –ø–æ–ø–∞–≤—à–∏–µ –≤ on_result_completed)
                if raw_result.get('status') != 'completed':
                    req_id = raw_result.get('req_id')
                    error_msg = raw_result.get('error', 'Unknown error')
                    
                    result = {
                        'query': query,
                        'lr': self.lr,
                        'source': 'error',
                        'cached_at': None,
                        'error': error_msg,
                        'status': raw_result.get('status', 'error'),
                        'req_id': req_id,
                        'metrics': SERPDataEnricher()._get_empty_metrics(),
                        'documents': [],
                        'lsi_phrases': []
                    }
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≤ Master DB
                    if self.master_db and self.query_group:
                        self._update_master_status(query, 'error', req_id=req_id, error_message=error_msg)
                    
                    cached_results[query] = result
                    self.stats['errors'] += 1
        finally:
            # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –∑–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏ aiohttp
            await batch_client.close()
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        return [cached_results.get(q, self._create_error_result(q, "Not processed")) for q in queries]
    
    def clear_caches(self, clear_database: bool = False):
        """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à–∏ (—É—Å—Ç–∞—Ä–µ–≤—à–∏–π –º–µ—Ç–æ–¥, –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        # –û—á–∏—Å—Ç–∫–∞ Master DB –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥
        if clear_database:
            print("‚ö†Ô∏è  –û—á–∏—Å—Ç–∫–∞ Master DB –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥")
        pass
    
    def recover_missing_lsi_from_urls(self, group_name: str = None) -> int:
        """
        –î–æ—Å–æ–±—Ä–∞—Ç—å LSI —Ñ—Ä–∞–∑—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, —É –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å URL, –Ω–æ –Ω–µ—Ç LSI
        
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∑–∞–ø—Ä–æ—Å—ã —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–º–∏ serp_top_urls, –Ω–æ –ø—É—Å—Ç—ã–º–∏ serp_lsi_phrases.
        - –ï—Å–ª–∏ URL –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Å–ª–æ–≤–∞—Ä–µ–π —Å title/snippet/passages - –∏–∑–≤–ª–µ–∫–∞–µ—Ç LSI –∏–∑ –Ω–∏—Ö
        - –ï—Å–ª–∏ URL —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ –∏ –µ—Å—Ç—å serp_req_id - –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ API
        
        Args:
            group_name: –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è self.query_group, –µ—Å–ª–∏ –∏ –µ–≥–æ –Ω–µ—Ç - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –≥—Ä—É–ø–ø—ã)
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        if not self.master_db:
            print("‚ö†Ô∏è  Master DB –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return 0
        
        import sqlite3
        import json
        import requests
        import re
        
        conn = sqlite3.connect(self.master_db.db_path)
        cursor = conn.cursor()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä—É–ø–ø—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        group = group_name or self.query_group
        
        if group:
            print(f"üîÑ –ü–æ–∏—Å–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å URL, –Ω–æ –±–µ–∑ LSI –≤ –≥—Ä—É–ø–ø–µ '{group}'...")
            
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: —Å—á–∏—Ç–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
            cursor.execute('''
                SELECT COUNT(*) FROM master_queries WHERE group_name = ?
            ''', (group,))
            total_in_group = cursor.fetchone()[0]
            
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: —Å—á–∏—Ç–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å completed —Å—Ç–∞—Ç—É—Å–æ–º
            cursor.execute('''
                SELECT COUNT(*) FROM master_queries 
                WHERE group_name = ? AND serp_status = 'completed'
            ''', (group,))
            completed_count = cursor.fetchone()[0]
            
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: —Å—á–∏—Ç–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å URL
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–∞ "position" –≤ JSON
            cursor.execute('''
                SELECT COUNT(*) FROM master_queries 
                WHERE group_name = ? 
                  AND serp_status = 'completed'
                  AND serp_top_urls IS NOT NULL
                  AND serp_top_urls != ''
                  AND serp_top_urls LIKE '%"position"%'
            ''', (group,))
            with_urls_count = cursor.fetchone()[0]
            
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: —Å—á–∏—Ç–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –±–µ–∑ LSI
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–ª—é—á–∞–º –≤ JSON: "position" –¥–ª—è URL, "phrase" –¥–ª—è LSI
            cursor.execute('''
                SELECT COUNT(*) FROM master_queries 
                WHERE group_name = ?
                  AND serp_status = 'completed'
                  AND serp_top_urls IS NOT NULL
                  AND serp_top_urls != ''
                  AND serp_top_urls LIKE '%"position"%'
                  AND (
                    serp_lsi_phrases IS NULL 
                    OR serp_lsi_phrases = '' 
                    OR serp_lsi_phrases = '[]'
                    OR serp_lsi_phrases NOT LIKE '%"phrase"%'
                  )
            ''', (group,))
            without_lsi_count = cursor.fetchone()[0]
            
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: —Å—á–∏—Ç–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –±–µ–∑ URL –≤–æ–æ–±—â–µ
            cursor.execute('''
                SELECT COUNT(*) FROM master_queries 
                WHERE group_name = ?
                  AND serp_status = 'completed'
                  AND (serp_top_urls IS NULL OR serp_top_urls = '' OR serp_top_urls = '[]')
            ''', (group,))
            without_urls_count = cursor.fetchone()[0]
            
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: —Å—á–∏—Ç–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –±–µ–∑ req_id
            cursor.execute('''
                SELECT COUNT(*) FROM master_queries 
                WHERE group_name = ?
                  AND serp_status = 'completed'
                  AND (serp_req_id IS NULL OR serp_req_id = '')
            ''', (group,))
            without_req_id_count = cursor.fetchone()[0]
            
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç LSI —Ñ—Ä–∞–∑ —É –∑–∞–ø—Ä–æ—Å–æ–≤ —Å URL
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–∞ "position" –≤ JSON
            cursor.execute('''
                SELECT serp_lsi_phrases 
                FROM master_queries 
                WHERE group_name = ?
                  AND serp_status = 'completed'
                  AND serp_top_urls IS NOT NULL
                  AND serp_top_urls != ''
                  AND serp_top_urls LIKE '%"position"%'
                LIMIT 5
            ''', (group,))
            sample_lsi = cursor.fetchall()
            if sample_lsi:
                print(f"   üîç –ü—Ä–∏–º–µ—Ä—ã —Ñ–æ—Ä–º–∞—Ç–∞ LSI —Ñ—Ä–∞–∑ (–ø–µ—Ä–≤—ã–µ 5):")
                for i, (lsi_val,) in enumerate(sample_lsi[:3], 1):
                    if lsi_val:
                        lsi_preview = str(lsi_val)[:100] + "..." if len(str(lsi_val)) > 100 else str(lsi_val)
                        print(f"      {i}. –î–ª–∏–Ω–∞: {len(str(lsi_val))}, –ù–∞—á–∞–ª–æ: {lsi_preview}")
                    else:
                        print(f"      {i}. NULL –∏–ª–∏ –ø—É—Å—Ç–æ")
                print()
            
            print(f"   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥—Ä—É–ø–ø–µ '{group}':")
            print(f"      –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {total_in_group}")
            print(f"      –°–æ —Å—Ç–∞—Ç—É—Å–æ–º 'completed': {completed_count}")
            print(f"      –° URL –¥–∞–Ω–Ω—ã–º–∏: {with_urls_count}")
            print(f"      –ë–µ–∑ URL –¥–∞–Ω–Ω—ã—Ö: {without_urls_count}")
            print(f"      –ë–µ–∑ LSI —Ñ—Ä–∞–∑: {without_lsi_count}")
            print(f"      –ë–µ–∑ req_id: {without_req_id_count}")
            if without_req_id_count > 0:
                print(f"      ‚ö†Ô∏è  {without_req_id_count} –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ req_id - –∏—Ö –Ω–µ–ª—å–∑—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏!")
            print()
            
            # –ù–∞—Ö–æ–¥–∏–º –∑–∞–ø—Ä–æ—Å—ã —Å URL, –Ω–æ –±–µ–∑ LSI (–≤–∫–ª—é—á–∞—è serp_req_id) –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≥—Ä—É–ø–ø—ã
            # –¢–∞–∫–∂–µ –∏—â–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º completed –∏ req_id, –Ω–æ –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö –≤–æ–æ–±—â–µ
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–ª—é—á–∞–º –≤ JSON: "position" –¥–ª—è URL, "phrase" –¥–ª—è LSI
            cursor.execute('''
                SELECT keyword, serp_top_urls, serp_req_id, group_name
                FROM master_queries
                WHERE group_name = ?
                  AND serp_status = 'completed'
                  AND serp_req_id IS NOT NULL
                  AND serp_req_id != ''
                  AND (
                    -- –í–∞—Ä–∏–∞–Ω—Ç 1: –ï—Å—Ç—å URL (–µ—Å—Ç—å –∫–ª—é—á "position" –≤ JSON), –Ω–æ –Ω–µ—Ç LSI (–Ω–µ—Ç –∫–ª—é—á–∞ "phrase")
                    (serp_top_urls IS NOT NULL 
                     AND serp_top_urls != '' 
                     AND serp_top_urls LIKE '%"position"%'
                     AND (
                       serp_lsi_phrases IS NULL 
                       OR serp_lsi_phrases = '' 
                       OR serp_lsi_phrases = '[]'
                       OR serp_lsi_phrases NOT LIKE '%"phrase"%'
                     ))
                    OR
                    -- –í–∞—Ä–∏–∞–Ω—Ç 2: –ù–µ—Ç URL –≤–æ–æ–±—â–µ (–Ω–µ—Ç –∫–ª—é—á–∞ "position"), –Ω–æ –µ—Å—Ç—å req_id (–¥–∞–Ω–Ω—ã–µ –Ω–µ –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã)
                    (serp_top_urls IS NULL 
                     OR serp_top_urls = '' 
                     OR serp_top_urls = '[]'
                     OR serp_top_urls NOT LIKE '%"position"%')
                  )
            ''', (group,))
        else:
            print("üîÑ –ü–æ–∏—Å–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å URL, –Ω–æ –±–µ–∑ LSI –≤–æ –í–°–ï–• –≥—Ä—É–ø–ø–∞—Ö...")
            
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: —Å—á–∏—Ç–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
            cursor.execute('SELECT COUNT(*) FROM master_queries')
            total_queries = cursor.fetchone()[0]
            
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: —Å—á–∏—Ç–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å completed —Å—Ç–∞—Ç—É—Å–æ–º
            cursor.execute('''
                SELECT COUNT(*) FROM master_queries WHERE serp_status = 'completed'
            ''')
            completed_count = cursor.fetchone()[0]
            
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: —Å—á–∏—Ç–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å URL
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–∞ "position" –≤ JSON
            cursor.execute('''
                SELECT COUNT(*) FROM master_queries 
                WHERE serp_status = 'completed'
                  AND serp_top_urls IS NOT NULL
                  AND serp_top_urls != ''
                  AND serp_top_urls LIKE '%"position"%'
            ''')
            with_urls_count = cursor.fetchone()[0]
            
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: —Å—á–∏—Ç–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –±–µ–∑ LSI
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–ª—é—á–∞–º –≤ JSON: "position" –¥–ª—è URL, "phrase" –¥–ª—è LSI
            cursor.execute('''
                SELECT COUNT(*) FROM master_queries 
                WHERE serp_status = 'completed'
                  AND serp_top_urls IS NOT NULL
                  AND serp_top_urls != ''
                  AND serp_top_urls LIKE '%"position"%'
                  AND (
                    serp_lsi_phrases IS NULL 
                    OR serp_lsi_phrases = '' 
                    OR serp_lsi_phrases = '[]'
                    OR serp_lsi_phrases NOT LIKE '%"phrase"%'
                  )
            ''')
            without_lsi_count = cursor.fetchone()[0]
            
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç LSI —Ñ—Ä–∞–∑ —É –∑–∞–ø—Ä–æ—Å–æ–≤ —Å URL
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–∞ "position" –≤ JSON
            cursor.execute('''
                SELECT serp_lsi_phrases 
                FROM master_queries 
                WHERE serp_status = 'completed'
                  AND serp_top_urls IS NOT NULL
                  AND serp_top_urls != ''
                  AND serp_top_urls LIKE '%"position"%'
                LIMIT 5
            ''')
            sample_lsi = cursor.fetchall()
            if sample_lsi:
                print(f"   üîç –ü—Ä–∏–º–µ—Ä—ã —Ñ–æ—Ä–º–∞—Ç–∞ LSI —Ñ—Ä–∞–∑ (–ø–µ—Ä–≤—ã–µ 5):")
                for i, (lsi_val,) in enumerate(sample_lsi[:3], 1):
                    if lsi_val:
                        lsi_preview = str(lsi_val)[:100] + "..." if len(str(lsi_val)) > 100 else str(lsi_val)
                        print(f"      {i}. –î–ª–∏–Ω–∞: {len(str(lsi_val))}, –ù–∞—á–∞–ª–æ: {lsi_preview}")
                    else:
                        print(f"      {i}. NULL –∏–ª–∏ –ø—É—Å—Ç–æ")
                print()
            
            print(f"   üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            print(f"      –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –ë–î: {total_queries}")
            print(f"      –°–æ —Å—Ç–∞—Ç—É—Å–æ–º 'completed': {completed_count}")
            print(f"      –° URL –¥–∞–Ω–Ω—ã–º–∏: {with_urls_count}")
            print(f"      –ë–µ–∑ LSI —Ñ—Ä–∞–∑: {without_lsi_count}")
            print()
            
            # –ù–∞—Ö–æ–¥–∏–º –∑–∞–ø—Ä–æ—Å—ã —Å URL, –Ω–æ –±–µ–∑ LSI (–≤–∫–ª—é—á–∞—è serp_req_id) –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–ª—é—á–∞–º –≤ JSON: "position" –¥–ª—è URL, "phrase" –¥–ª—è LSI
            cursor.execute('''
                SELECT keyword, serp_top_urls, serp_req_id, group_name
                FROM master_queries
                WHERE serp_status = 'completed'
                  AND serp_req_id IS NOT NULL
                  AND serp_req_id != ''
                  AND (
                    -- –í–∞—Ä–∏–∞–Ω—Ç 1: –ï—Å—Ç—å URL (–µ—Å—Ç—å –∫–ª—é—á "position" –≤ JSON), –Ω–æ –Ω–µ—Ç LSI (–Ω–µ—Ç –∫–ª—é—á–∞ "phrase")
                    (serp_top_urls IS NOT NULL 
                     AND serp_top_urls != '' 
                     AND serp_top_urls LIKE '%"position"%'
                     AND (
                       serp_lsi_phrases IS NULL 
                       OR serp_lsi_phrases = '' 
                       OR serp_lsi_phrases = '[]'
                       OR serp_lsi_phrases NOT LIKE '%"phrase"%'
                     ))
                    OR
                    -- –í–∞—Ä–∏–∞–Ω—Ç 2: –ù–µ—Ç URL –≤–æ–æ–±—â–µ (–Ω–µ—Ç –∫–ª—é—á–∞ "position"), –Ω–æ –µ—Å—Ç—å req_id (–¥–∞–Ω–Ω—ã–µ –Ω–µ –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã)
                    (serp_top_urls IS NULL 
                     OR serp_top_urls = '' 
                     OR serp_top_urls = '[]'
                     OR serp_top_urls NOT LIKE '%"position"%')
                  )
            ''')
        
        queries_to_process = cursor.fetchall()
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ —Å URL, –Ω–æ –±–µ–∑ LSI
        if group:
            cursor.execute('''
                SELECT keyword, 
                       LENGTH(serp_top_urls) as urls_len,
                       LENGTH(serp_lsi_phrases) as lsi_len,
                       SUBSTR(serp_top_urls, 1, 100) as urls_preview,
                       SUBSTR(serp_lsi_phrases, 1, 100) as lsi_preview
                FROM master_queries
                WHERE group_name = ?
                  AND serp_status = 'completed'
                  AND serp_top_urls IS NOT NULL
                  AND serp_top_urls != ''
                  AND serp_top_urls LIKE '%"position"%'
                LIMIT 5
            ''', (group,))
            sample_queries = cursor.fetchall()
            if sample_queries:
                print(f"   üîç –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ —Å URL –¥–∞–Ω–Ω—ã–º–∏ (–ø–µ—Ä–≤—ã–µ 5):")
                for i, (kw, urls_len, lsi_len, urls_prev, lsi_prev) in enumerate(sample_queries[:3], 1):
                    print(f"      {i}. '{kw[:50]}...'")
                    print(f"         URL –¥–ª–∏–Ω–∞: {urls_len}, LSI –¥–ª–∏–Ω–∞: {lsi_len if lsi_len else 'NULL'}")
                    print(f"         URL –Ω–∞—á–∞–ª–æ: {urls_prev}...")
                    if lsi_prev:
                        print(f"         LSI –Ω–∞—á–∞–ª–æ: {lsi_prev}...")
                    else:
                        print(f"         LSI: NULL –∏–ª–∏ –ø—É—Å—Ç–æ")
                print()
        
        if not queries_to_process:
            print("‚úì –ù–µ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            if without_lsi_count == 0 and with_urls_count > 0:
                print("   ‚ÑπÔ∏è  –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã —Å URL —É–∂–µ –∏–º–µ—é—Ç LSI —Ñ—Ä–∞–∑—ã")
            elif with_urls_count == 0:
                print("   ‚ÑπÔ∏è  –ù–µ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ —Å URL –¥–∞–Ω–Ω—ã–º–∏")
            else:
                print(f"   ‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {without_lsi_count} –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ LSI –ø–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ, –Ω–æ SQL –∑–∞–ø—Ä–æ—Å –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                print(f"      –í–æ–∑–º–æ–∂–Ω–æ, –¥–∞–Ω–Ω—ã–µ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏–ª–∏ —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è")
            return 0
        
        print(f"   –ù–∞–π–¥–µ–Ω–æ {len(queries_to_process)} –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ LSI")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –≥—Ä—É–ø–ø–∞–º –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if not group:
            groups_count = {}
            for _, _, _, query_group in queries_to_process:
                groups_count[query_group] = groups_count.get(query_group, 0) + 1
            print(f"   –ì—Ä—É–ø–ø: {len(groups_count)}")
            for grp, cnt in sorted(groups_count.items()):
                print(f"     - {grp}: {cnt} –∑–∞–ø—Ä–æ—Å–æ–≤")
            print()
        
        lsi_extractor = LSIExtractor()
        enricher = SERPDataEnricher()
        updated_count = 0
        api_fetched_count = 0
        
        # –ü–∞—Ä—Å–∏–º API –∫–ª—é—á
        if ':' in self.api_key:
            api_user, api_key = self.api_key.split(':', 1)
        else:
            api_user = api_key = self.api_key
        
        api_url = "https://xmlstock.com/yandex/xml/"
        
        for keyword, top_urls_json, req_id, query_group in queries_to_process:
            try:
                # –ü–∞—Ä—Å–∏–º URL
                if isinstance(top_urls_json, str):
                    top_urls = json.loads(top_urls_json) if top_urls_json.strip() else []
                else:
                    top_urls = top_urls_json if top_urls_json else []
                
                # –ï—Å–ª–∏ URL –ø—É—Å—Ç—ã–µ, –Ω–æ –µ—Å—Ç—å req_id - –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ API
                if not top_urls and req_id:
                    try:
                        # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ req_id
                        params = {
                            'user': api_user,
                            'key': api_key,
                            'req_id': req_id
                        }
                        
                        response = requests.get(api_url, params=params, timeout=30)
                        
                        if response.status_code == 200:
                            xml_text = response.text
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—à–∏–±–∫–∏
                            if '<error' in xml_text:
                                error_match = re.search(r'<error[^>]*code="([^"]*)"[^>]*>([^<]+)</error>', xml_text)
                                if error_match:
                                    code = error_match.group(1)
                                    if code == '202':
                                        # –ï—â—ë –Ω–µ –≥–æ—Ç–æ–≤–æ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                                        continue
                                    elif code == '203':
                                        # req_id –∏—Å—Ç—ë–∫ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∑–∞–ø—Ä–æ—Å)
                                        continue
                                    else:
                                        # –î—Ä—É–≥–∞—è –æ—à–∏–±–∫–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                                        continue
                            else:
                                # –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–∏–ª–∏ XML - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
                                enriched = enricher.enrich_from_serp(xml_text, keyword)
                                
                                if not enriched.get('error') and enriched.get('documents'):
                                    # –ò–∑–≤–ª–µ–∫–∞–µ–º LSI –∏–∑ –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                                    lsi_phrases = lsi_extractor.extract_from_serp_documents(
                                        enriched['documents'],
                                        keyword
                                    )
                                    
                                    # –û–±–Ω–æ–≤–ª—è–µ–º –∏ URL (–≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ) –∏ LSI
                                    top_urls_new = []
                                    for i, doc in enumerate(enriched['documents'][:20], 1):
                                        top_urls_new.append({
                                            'position': i,
                                            'url': doc.get('url', ''),
                                            'domain': doc.get('domain', ''),
                                            'title': doc.get('title', ''),
                                            'snippet': doc.get('snippet', ''),
                                            'passages': doc.get('passages', ''),
                                            'is_commercial': doc.get('is_commercial', False)
                                        })
                                    
                                    top_urls_json_new = json.dumps(top_urls_new, ensure_ascii=False)
                                    lsi_json = json.dumps(lsi_phrases, ensure_ascii=False) if lsi_phrases else '[]'
                                    
                                    cursor.execute('''
                                        UPDATE master_queries
                                        SET serp_top_urls = ?, serp_lsi_phrases = ?
                                        WHERE group_name = ? AND keyword = ?
                                    ''', (top_urls_json_new, lsi_json, query_group, keyword))
                                    
                                    updated_count += 1
                                    api_fetched_count += 1
                                    
                                    # Commit –∫–∞–∂–¥—ã–µ 10 –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                                    if updated_count % 10 == 0:
                                        conn.commit()
                                    
                                    # –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫–∞–∂–¥—ã–µ 5 –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ª—É—á—à–µ–π –≤–∏–¥–∏–º–æ—Å—Ç–∏
                                    if updated_count % 5 == 0:
                                        print(f"   ‚úì –û–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count}/{len(queries_to_process)} (—á–µ—Ä–µ–∑ API: {api_fetched_count})...")
                    
                    except Exception as e:
                        # –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ API - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        continue
                    
                    # –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—É—Å—Ç—ã—Ö URL - –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –∑–∞–ø—Ä–æ—Å—É
                    continue
                
                if not top_urls:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç URL
                documents = []
                has_title_data = False
                urls_are_strings_only = True
                
                for item in top_urls:
                    if isinstance(item, dict):
                        urls_are_strings_only = False
                        # –°–ª–æ–≤–∞—Ä—å - –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ title
                        if item.get('title'):
                            has_title_data = True
                        documents.append({
                            'title': item.get('title', ''),
                            'snippet': item.get('snippet', ''),
                            'passages': item.get('passages', ''),
                            'url': item.get('url', ''),
                            'domain': item.get('domain', ''),
                            'is_commercial': item.get('is_commercial', False)
                        })
                    elif isinstance(item, str):
                        # –¢–æ–ª—å–∫–æ URL - –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è LSI
                        documents.append({
                            'title': '',
                            'snippet': '',
                            'passages': '',
                            'url': item,
                            'domain': '',
                            'is_commercial': False
                        })
                
                # –ï—Å–ª–∏ URL —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ –∏ –µ—Å—Ç—å req_id - –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ API
                if urls_are_strings_only and req_id:
                    try:
                        # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ req_id
                        params = {
                            'user': api_user,
                            'key': api_key,
                            'req_id': req_id
                        }
                        
                        response = requests.get(api_url, params=params, timeout=30)
                        
                        if response.status_code == 200:
                            xml_text = response.text
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—à–∏–±–∫–∏
                            if '<error' in xml_text:
                                error_match = re.search(r'<error[^>]*code="([^"]*)"[^>]*>([^<]+)</error>', xml_text)
                                if error_match:
                                    code = error_match.group(1)
                                    if code == '202':
                                        # –ï—â—ë –Ω–µ –≥–æ—Ç–æ–≤–æ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                                        continue
                                    elif code == '203':
                                        # req_id –∏—Å—Ç—ë–∫ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                                        continue
                                    else:
                                        # –î—Ä—É–≥–∞—è –æ—à–∏–±–∫–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                                        continue
                            else:
                                # –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–∏–ª–∏ XML - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
                                enriched = enricher.enrich_from_serp(xml_text, keyword)
                                
                                if not enriched.get('error') and enriched.get('documents'):
                                    # –ò–∑–≤–ª–µ–∫–∞–µ–º LSI –∏–∑ –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                                    lsi_phrases = lsi_extractor.extract_from_serp_documents(
                                        enriched['documents'],
                                        keyword
                                    )
                                    
                                    if lsi_phrases:
                                        # –û–±–Ω–æ–≤–ª—è–µ–º –∏ URL (–≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏) –∏ LSI
                                        top_urls_new = []
                                        for i, doc in enumerate(enriched['documents'][:20], 1):
                                            top_urls_new.append({
                                                'position': i,
                                                'url': doc.get('url', ''),
                                                'domain': doc.get('domain', ''),
                                                'title': doc.get('title', ''),
                                                'snippet': doc.get('snippet', ''),
                                                'passages': doc.get('passages', ''),
                                                'is_commercial': doc.get('is_commercial', False)
                                            })
                                        
                                        top_urls_json_new = json.dumps(top_urls_new, ensure_ascii=False)
                                        lsi_json = json.dumps(lsi_phrases, ensure_ascii=False)
                                        
                                        cursor.execute('''
                                            UPDATE master_queries
                                            SET serp_top_urls = ?, serp_lsi_phrases = ?
                                            WHERE group_name = ? AND keyword = ?
                                        ''', (top_urls_json_new, lsi_json, query_group, keyword))
                                        
                                        updated_count += 1
                                        api_fetched_count += 1
                                        
                                        # Commit –∫–∞–∂–¥—ã–µ 10 –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                                        if updated_count % 10 == 0:
                                            conn.commit()
                                        
                                        # –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫–∞–∂–¥—ã–µ 5 –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ª—É—á—à–µ–π –≤–∏–¥–∏–º–æ—Å—Ç–∏
                                        if updated_count % 5 == 0:
                                            print(f"   ‚úì –û–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count}/{len(queries_to_process)} (—á–µ—Ä–µ–∑ API: {api_fetched_count})...")
                    
                    except Exception as e:
                        # –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ API - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        continue
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å title/snippet/passages - –∏–∑–≤–ª–µ–∫–∞–µ–º LSI –∏–∑ –∏–º–µ—é—â–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö
                elif has_title_data and documents:
                    lsi_phrases = lsi_extractor.extract_from_serp_documents(
                        documents,
                        keyword
                    )
                    
                    if lsi_phrases:
                        # –û–±–Ω–æ–≤–ª—è–µ–º –∏ LSI, –∏ serp_top_urls (–≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏)
                        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ URL —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                        top_urls_updated = []
                        for i, doc in enumerate(documents[:20], 1):
                            top_urls_updated.append({
                                'position': i,
                                'url': doc.get('url', ''),
                                'domain': doc.get('domain', ''),
                                'title': doc.get('title', ''),
                                'snippet': doc.get('snippet', ''),
                                'passages': doc.get('passages', ''),
                                'is_commercial': doc.get('is_commercial', False)
                            })
                        
                        top_urls_json_updated = json.dumps(top_urls_updated, ensure_ascii=False)
                        lsi_json = json.dumps(lsi_phrases, ensure_ascii=False)
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±–∞ –ø–æ–ª—è: –∏ LSI, –∏ serp_top_urls
                        cursor.execute('''
                            UPDATE master_queries
                            SET serp_top_urls = ?, serp_lsi_phrases = ?
                            WHERE group_name = ? AND keyword = ?
                        ''', (top_urls_json_updated, lsi_json, query_group, keyword))
                        
                        updated_count += 1
                        
                        # Commit –∫–∞–∂–¥—ã–µ 10 –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                        if updated_count % 10 == 0:
                            conn.commit()
                        
                        # –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫–∞–∂–¥—ã–µ 5 –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ª—É—á—à–µ–π –≤–∏–¥–∏–º–æ—Å—Ç–∏
                        if updated_count % 5 == 0:
                            print(f"   ‚úì –û–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count}/{len(queries_to_process)}...")
            
            except Exception as e:
                print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ '{keyword[:50]}...': {e}")
                continue
        
        conn.commit()
        conn.close()
        
        print(f"‚úì –î–æ—Å–æ–±–æ—Ä LSI –∑–∞–≤–µ—Ä—à–µ–Ω: –æ–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
        if api_fetched_count > 0:
            print(f"   üì° –ß–µ—Ä–µ–∑ API –ø–æ–ª—É—á–µ–Ω–æ: {api_fetched_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
        return updated_count

