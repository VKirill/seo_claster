"""
Master Query Database
–ï–¥–∏–Ω–∞—è –ë–î —Å–æ –í–°–ï–ú–ò –¥–∞–Ω–Ω—ã–º–∏ –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º –¥–ª—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–π –ø–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
"""

import sqlite3
import pandas as pd
import json
from pathlib import Path
from typing import Optional, Dict, List, Any
from datetime import datetime

from .master_query_schema import (
    MASTER_QUERY_TABLE_SCHEMA,
    MASTER_QUERY_INDEXES,
    MASTER_EXPORT_VIEW
)


class MasterQueryDatabase:
    """
    Master Database - –µ–¥–∏–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –í–°–ï–• –¥–∞–Ω–Ω—ã—Ö –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º
    
    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
    - –í—Å—ë –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ: –∏–Ω—Ç–µ–Ω—Ç + SERP + –º–µ—Ç—Ä–∏–∫–∏ + –±—Ä–µ–Ω–¥—ã + –≤–æ—Ä–æ–Ω–∫–∞
    - –ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –ø–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    - –ë—ã—Å—Ç—Ä—ã–π —ç–∫—Å–ø–æ—Ä—Ç –±–µ–∑ JOIN'–æ–≤
    - –ï–¥–∏–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–∞–≤–¥—ã (Single Source of Truth)
    
    –ß—Ç–æ —Ö—Ä–∞–Ω–∏—Ç—Å—è:
    - –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ (normalized, lemmatized, NER)
    - Intent classification (main_intent, scores, —Ñ–ª–∞–≥–∏)
    - SERP –¥–∞–Ω–Ω—ã–µ (found_docs, offer_info, —Ü–µ–Ω—ã, TOP-20 URLs)
    - Yandex Direct (CPC, CTR, competition)
    - SEO –º–µ—Ç—Ä–∏–∫–∏ (KEI, difficulty, traffic)
    - –ë—Ä–µ–Ω–¥—ã –∏ –≤–æ—Ä–æ–Ω–∫–∞
    
    –ß—Ç–æ –ù–ï —Ö—Ä–∞–Ω–∏—Ç—Å—è (—Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏):
    - –ö–ª–∞—Å—Ç–µ—Ä—ã (–∑–∞–≤–∏—Å—è—Ç –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ min_common_urls, mode –∏ —Ç.–¥.)
    - –ò–µ—Ä–∞—Ä—Ö–∏—è (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º–∞)
    - –ì—Ä–∞—Ñ —Å–≤—è–∑–µ–π
    """
    
    def __init__(self, db_path: Path = None):
        """
        Args:
            db_path: –ü—É—Ç—å –∫ –ë–î (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é output/master_queries.db)
        """
        if db_path is None:
            db_path = Path("output/master_queries.db")
        
        self.db_path = db_path
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        self._init_database()
    
    def _init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü –ë–î —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ (–∫–∞–∫ PostgreSQL)"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ========================================
        # PRAGMA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (–∞–Ω–∞–ª–æ–≥ PostgreSQL –Ω–∞—Å—Ç—Ä–æ–µ–∫)
        # ========================================
        
        # –ñ—É—Ä–Ω–∞–ª –≤ WAL —Ä–µ–∂–∏–º–µ - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —á—Ç–µ–Ω–∏—è (–∫–∞–∫ –≤ PostgreSQL)
        cursor.execute("PRAGMA journal_mode = WAL")
        
        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è NORMAL - –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏/–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        cursor.execute("PRAGMA synchronous = NORMAL")
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º cache size –¥–æ 64MB (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2MB)
        cursor.execute("PRAGMA cache_size = -64000")  # –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ = KB
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è temp store –≤ –ø–∞–º—è—Ç–∏
        cursor.execute("PRAGMA temp_store = MEMORY")
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º page size –¥–æ 32KB (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 4KB)
        # –¢–æ–ª—å–∫–æ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –Ω–æ–≤–æ–π –ë–î!
        cursor.execute("PRAGMA page_size = 32768")
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π VACUUM –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏
        cursor.execute("PRAGMA auto_vacuum = INCREMENTAL")
        
        # –í–∫–ª—é—á–∞–µ–º foreign keys
        cursor.execute("PRAGMA foreign_keys = ON")
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ (–∫–∞–∫ ANALYZE –≤ PostgreSQL)
        cursor.execute("PRAGMA optimize")
        
        print("‚úì SQLite –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã (WAL, cache 64MB, page 32KB)")
        
        # –°–æ–∑–¥–∞—ë–º —Ç–∞–±–ª–∏—Ü—É –≥—Ä—É–ø–ø (–∏–∑ query_cache_db)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS query_groups (
                group_name TEXT PRIMARY KEY,
                csv_file_path TEXT NOT NULL,
                csv_hash TEXT NOT NULL,
                total_queries INTEGER NOT NULL,
                unique_queries INTEGER NOT NULL,
                duplicates_removed INTEGER NOT NULL,
                imported_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                processing_version TEXT DEFAULT '2.0'
            )
        ''')
        
        # –°–æ–∑–¥–∞—ë–º master —Ç–∞–±–ª–∏—Ü—É
        cursor.execute(MASTER_QUERY_TABLE_SCHEMA)
        
        # –°–æ–∑–¥–∞—ë–º –∏–Ω–¥–µ–∫—Å—ã
        for index_sql in MASTER_QUERY_INDEXES:
            cursor.execute(index_sql)
        
        # –°–æ–∑–¥–∞—ë–º view –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
        cursor.execute(MASTER_EXPORT_VIEW)
        
        conn.commit()
        conn.close()
        
        print("‚úì Master Query Database –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    def save_queries(
        self,
        group_name: str,
        df: pd.DataFrame,
        csv_path: Path = None,
        csv_hash: str = None
    ):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç/–æ–±–Ω–æ–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –≤ master —Ç–∞–±–ª–∏—Ü–µ
        
        Args:
            group_name: –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã
            df: DataFrame —Å–æ –í–°–ï–ú–ò –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            csv_path: –ü—É—Ç—å –∫ CSV (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            csv_hash: Hash CSV (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        conn = sqlite3.connect(self.db_path)
        
        try:
            cursor = conn.cursor()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
            # –≠—Ç–æ –Ω—É–∂–Ω–æ —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å SERP –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã —á–µ—Ä–µ–∑ update_serp_metrics
            existing_df = None
            try:
                existing_df = self.load_queries(group_name)
                if existing_df is not None and len(existing_df) > 0:
                    # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –ø–æ keyword –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
                    existing_df = existing_df.set_index('keyword')
            except:
                # –ï—Å–ª–∏ –≥—Ä—É–ø–ø—ã –Ω–µ—Ç - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                existing_df = None
            
            # –ú–∞–ø–ø–∏–Ω–≥ –∏–º–µ–Ω –∫–æ–ª–æ–Ω–æ–∫ –∏–∑ DataFrame –≤ –ë–î
            column_mapping = {
                'serp_docs_count': 'serp_found_docs',
                'serp_titles_count': 'serp_titles_with_keyword',
            }
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –∏–º–µ–Ω –∫–æ–ª–æ–Ω–æ–∫
            df_copy = df.copy()
            for df_col, db_col in column_mapping.items():
                if df_col in df_copy.columns and db_col not in df_copy.columns:
                    df_copy[db_col] = df_copy[df_col]
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Å –Ω–æ–≤—ã–º–∏
            if existing_df is not None and len(existing_df) > 0:
                # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ DataFrame –æ–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
                for idx, row in df_copy.iterrows():
                    keyword = row.get('keyword')
                    if keyword and keyword in existing_df.index:
                        existing_row = existing_df.loc[keyword]
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º SERP –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ DataFrame
                        serp_fields = [
                            'serp_req_id', 'serp_status', 'serp_error_message',
                            'serp_found_docs', 'serp_main_pages_count', 'serp_titles_with_keyword',
                            'serp_commercial_domains', 'serp_info_domains',
                            'serp_intent', 'serp_confidence', 'serp_docs_with_offers',
                            'serp_total_docs', 'serp_offer_ratio',
                            'serp_avg_price', 'serp_min_price', 'serp_max_price',
                            'serp_median_price', 'serp_currency',
                            'serp_offers_count', 'serp_offers_with_discount',
                            'serp_avg_discount_percent', 'serp_top_urls', 'serp_lsi_phrases',
                            'serp_created_at', 'serp_updated_at'
                        ]
                        
                        for field in serp_fields:
                            # –ï—Å–ª–∏ –ø–æ–ª–µ –µ—Å—Ç—å –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—É—Å—Ç–æ–µ –≤ DataFrame
                            if field in existing_df.columns:
                                existing_val = existing_row.get(field)
                                df_val = row.get(field)
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –µ—Å–ª–∏ –≤ DataFrame –µ–≥–æ –Ω–µ—Ç –∏–ª–∏ –æ–Ω–æ –ø—É—Å—Ç–æ–µ
                                if pd.notna(existing_val) and (pd.isna(df_val) or df_val == '' or df_val == 0):
                                    df_copy.at[idx, field] = existing_val
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã
            if csv_path and csv_hash:
                cursor.execute('''
                    INSERT OR REPLACE INTO query_groups 
                    (group_name, csv_file_path, csv_hash, total_queries, unique_queries, 
                     duplicates_removed, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
                ''', (
                    group_name,
                    str(csv_path),
                    csv_hash,
                    len(df),
                    len(df),
                    0
                ))
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
            queries_data = []
            
            # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π
            def safe_get(row, key, default=None, cast=None):
                """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ–º —Ç–∏–ø–∞"""
                if key not in df_copy.columns:
                    return default
                val = row.get(key)
                if pd.isna(val):
                    return default
                if cast:
                    try:
                        return cast(val)
                    except (ValueError, TypeError):
                        return default
                return val
            
            for _, row in df_copy.iterrows():
                # SERP TOP URLs –∫–∞–∫ JSON
                # –í–ê–ñ–ù–û: –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –µ–¥–∏–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ - —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ {position, url, domain, title, is_commercial}
                serp_top_urls = None
                if 'serp_top_urls' in df_copy.columns:
                    val = row.get('serp_top_urls')
                    if val is not None and not (isinstance(val, float) and pd.isna(val)):
                        # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ —Å—Ç—Ä–æ–∫–∞ (JSON) - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
                        if isinstance(val, str):
                            serp_top_urls = val
                        else:
                            # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
                            if isinstance(val, list) and len(val) > 0:
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
                                if isinstance(val[0], dict):
                                    # –£–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç - —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤
                                    serp_top_urls = json.dumps(val, ensure_ascii=False)
                                elif isinstance(val[0], str):
                                    # –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤
                                    normalized_urls = []
                                    for i, url in enumerate(val[:20], 1):
                                        normalized_urls.append({
                                            'position': i,
                                            'url': url,
                                            'domain': '',
                                            'title': '',
                                            'is_commercial': False
                                        })
                                    serp_top_urls = json.dumps(normalized_urls, ensure_ascii=False)
                                else:
                                    serp_top_urls = json.dumps(val, ensure_ascii=False)
                            else:
                                serp_top_urls = json.dumps(val, ensure_ascii=False)
                elif 'serp_urls' in df_copy.columns:
                    # –¢–µ–∫—É—â–∏–π pipeline —Å–æ–∑–¥–∞–µ—Ç –∫–æ–ª–æ–Ω–∫—É serp_urls —Å–æ —Å–ø–∏—Å–∫–æ–º URL (—Å—Ç—Ä–æ–∫–∏)
                    val = row.get('serp_urls')
                    if val is not None and not (isinstance(val, float) and pd.isna(val)):
                        if isinstance(val, str):
                            # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ - –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å
                            try:
                                parsed = json.loads(val)
                                if isinstance(parsed, list) and len(parsed) > 0:
                                    if isinstance(parsed[0], str):
                                        # –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤
                                        normalized_urls = []
                                        for i, url in enumerate(parsed[:20], 1):
                                            normalized_urls.append({
                                                'position': i,
                                                'url': url,
                                                'domain': '',
                                                'title': '',
                                                'is_commercial': False
                                            })
                                        serp_top_urls = json.dumps(normalized_urls, ensure_ascii=False)
                                    else:
                                        serp_top_urls = val  # –£–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                                else:
                                    serp_top_urls = val
                            except:
                                serp_top_urls = val
                        elif isinstance(val, list):
                            # –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤
                            normalized_urls = []
                            for i, url in enumerate(val[:20], 1):
                                if isinstance(url, str):
                                    normalized_urls.append({
                                        'position': i,
                                        'url': url,
                                        'domain': '',
                                        'title': '',
                                        'is_commercial': False
                                    })
                                elif isinstance(url, dict):
                                    # –£–∂–µ –æ–±—ä–µ–∫—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
                                    normalized_urls.append(url)
                            serp_top_urls = json.dumps(normalized_urls, ensure_ascii=False) if normalized_urls else None
                        else:
                            serp_top_urls = json.dumps(val, ensure_ascii=False) if val else None
                elif 'top_urls' in df_copy.columns:
                    val = row.get('top_urls')
                    if val is not None and not (isinstance(val, float) and pd.isna(val)):
                        if isinstance(val, str):
                            serp_top_urls = val
                        else:
                            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç
                            if isinstance(val, list) and len(val) > 0:
                                if isinstance(val[0], dict):
                                    serp_top_urls = json.dumps(val, ensure_ascii=False)
                                elif isinstance(val[0], str):
                                    normalized_urls = []
                                    for i, url in enumerate(val[:20], 1):
                                        normalized_urls.append({
                                            'position': i,
                                            'url': url,
                                            'domain': '',
                                            'title': '',
                                            'is_commercial': False
                                        })
                                    serp_top_urls = json.dumps(normalized_urls, ensure_ascii=False)
                                else:
                                    serp_top_urls = json.dumps(val, ensure_ascii=False)
                            else:
                                serp_top_urls = json.dumps(val, ensure_ascii=False)
                
                # LSI phrases –∫–∞–∫ JSON
                serp_lsi_phrases = None
                if 'serp_lsi_phrases' in df_copy.columns:
                    val = row.get('serp_lsi_phrases')
                    if val is not None and not (isinstance(val, float) and pd.isna(val)):
                        serp_lsi_phrases = val if isinstance(val, str) else json.dumps(val)
                elif 'lsi_phrases' in df_copy.columns:
                    # –¢–µ–∫—É—â–∏–π pipeline —Å–æ–∑–¥–∞–µ—Ç –∫–æ–ª–æ–Ω–∫—É lsi_phrases
                    val = row.get('lsi_phrases')
                    if val is not None and not (isinstance(val, float) and pd.isna(val)):
                        serp_lsi_phrases = val if isinstance(val, str) else json.dumps(val)
                
                queries_data.append((
                    group_name,
                    # –ë–∞–∑–æ–≤—ã–µ
                    safe_get(row, 'keyword', ''),
                    safe_get(row, 'frequency_world', 0, int),
                    safe_get(row, 'frequency_exact', 0, int),
                    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
                    safe_get(row, 'normalized'),
                    safe_get(row, 'lemmatized'),
                    safe_get(row, 'words_count', 0, int),
                    safe_get(row, 'main_words'),
                    safe_get(row, 'key_phrase'),
                    # NER
                    safe_get(row, 'ner_entities'),
                    safe_get(row, 'ner_locations'),
                    safe_get(row, 'has_geo', False, bool),
                    safe_get(row, 'geo_type'),
                    safe_get(row, 'geo_country'),
                    safe_get(row, 'geo_city'),
                    # Intent
                    safe_get(row, 'main_intent'),
                    safe_get(row, 'commercial_score', 0.0, float),
                    safe_get(row, 'informational_score', 0.0, float),
                    safe_get(row, 'navigational_score', 0.0, float),
                    # –§–ª–∞–≥–∏
                    safe_get(row, 'is_commercial', False, bool),
                    safe_get(row, 'is_wholesale', False, bool),
                    safe_get(row, 'is_urgent', False, bool),
                    safe_get(row, 'is_diy', False, bool),
                    safe_get(row, 'is_review', False, bool),
                    safe_get(row, 'is_brand_query', False, bool),
                    # SERP –±–∞–∑–æ–≤—ã–µ
                    safe_get(row, 'serp_query_hash'),
                    # serp_req_id —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç –≤ DataFrame
                    # –≠—Ç–æ –Ω—É–∂–Ω–æ —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å req_id –ø–æ—Å–ª–µ completed —Å—Ç–∞—Ç—É—Å–∞
                    safe_get(row, 'serp_req_id'),
                    # –°—Ç–∞—Ç—É—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'completed' —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç –Ω–∏ –≤ DataFrame, –Ω–∏ –≤ –ë–î
                    safe_get(row, 'serp_status') or 'completed',
                    safe_get(row, 'serp_error_message'),  # –¢–µ–∫—Å—Ç –æ—à–∏–±–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
                    safe_get(row, 'serp_found_docs', None, int),
                    safe_get(row, 'serp_main_pages_count', None, int),
                    safe_get(row, 'serp_titles_with_keyword', None, int),
                    safe_get(row, 'serp_commercial_domains', None, int),
                    safe_get(row, 'serp_info_domains', None, int),
                    safe_get(row, 'serp_created_at'),
                    safe_get(row, 'serp_updated_at'),  # –ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
                    # SERP offer info
                    safe_get(row, 'serp_intent'),
                    safe_get(row, 'serp_confidence', 0.0, float),
                    safe_get(row, 'serp_docs_with_offers', 0, int),
                    safe_get(row, 'serp_total_docs', 0, int),
                    safe_get(row, 'serp_offer_ratio', 0.0, float),
                    # –¶–µ–Ω—ã
                    safe_get(row, 'serp_avg_price', None, float),
                    safe_get(row, 'serp_min_price', None, float),
                    safe_get(row, 'serp_max_price', None, float),
                    safe_get(row, 'serp_median_price', None, float),
                    safe_get(row, 'serp_currency', 'RUR'),
                    safe_get(row, 'serp_offers_count', 0, int),
                    safe_get(row, 'serp_offers_with_discount', 0, int),
                    safe_get(row, 'serp_avg_discount_percent', None, float),
                    # SERP URLs & LSI  
                    serp_top_urls,
                    serp_lsi_phrases,
                    # Yandex Direct
                    safe_get(row, 'direct_shows', None, int),
                    safe_get(row, 'direct_clicks', None, int),
                    safe_get(row, 'direct_ctr', None, float),
                    safe_get(row, 'direct_min_cpc', None, float),
                    safe_get(row, 'direct_avg_cpc', None, float),
                    safe_get(row, 'direct_max_cpc', None, float),
                    safe_get(row, 'direct_recommended_cpc', None, float),
                    safe_get(row, 'direct_competition_level'),
                    safe_get(row, 'direct_first_place_bid', None, float),
                    safe_get(row, 'direct_first_place_price', None, float),
                    # SEO –º–µ—Ç—Ä–∏–∫–∏
                    safe_get(row, 'kei', 0.0, float),
                    safe_get(row, 'difficulty', 0.0, float),
                    safe_get(row, 'competition_score', 0.0, float),
                    safe_get(row, 'potential_traffic', 0.0, float),
                    safe_get(row, 'expected_ctr', 0.0, float),
                    # –ë—Ä–µ–Ω–¥—ã
                    safe_get(row, 'detected_brand'),
                    safe_get(row, 'brand_confidence', 0.0, float),
                    # –í–æ—Ä–æ–Ω–∫–∞
                    safe_get(row, 'funnel_stage'),
                    safe_get(row, 'funnel_priority', 5, int),
                ))
        
            # –û—Ç–ª–∞–¥–∫–∞: –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—É—é –∑–∞–ø–∏—Å—å
            if queries_data:
                first_tuple = queries_data[0]
                print(f"  –û—Ç–ª–∞–¥–∫–∞: –ø–µ—Ä–≤—ã–π tuple —Å–æ–¥–µ—Ä–∂–∏—Ç {len(first_tuple)} –∑–Ω–∞—á–µ–Ω–∏–π")
                if len(first_tuple) != 70:
                    print(f"  ‚ö†Ô∏è  –û–∂–∏–¥–∞–ª–æ—Å—å 70 –∑–Ω–∞—á–µ–Ω–∏–π, –ø–æ–ª—É—á–µ–Ω–æ {len(first_tuple)}")
                    raise ValueError(f"–ù–µ–≤–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞—á–µ–Ω–∏–π –≤ tuple: {len(first_tuple)} –≤–º–µ—Å—Ç–æ 70")
            
            # Bulk insert –∏–ª–∏ replace (—Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ)
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–∫–∏ "too many SQL variables"
            # SQLite –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (~999), –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞—Ç—á–∏ –ø–æ 100 –∑–∞–ø–∏—Å–µ–π
            batch_size = 100
            total_batches = (len(queries_data) + batch_size - 1) // batch_size
            
            insert_query = '''
                INSERT OR REPLACE INTO master_queries 
                (group_name, keyword, frequency_world, frequency_exact,
                 normalized, lemmatized, words_count, main_words, key_phrase,
                 ner_entities, ner_locations,
                 has_geo, geo_type, geo_country, geo_city,
                 main_intent, commercial_score, informational_score, navigational_score,
                 is_commercial, is_wholesale, is_urgent, is_diy, is_review, is_brand_query,
                 serp_query_hash, serp_req_id, serp_status, serp_error_message,
                 serp_found_docs, serp_main_pages_count, serp_titles_with_keyword,
                 serp_commercial_domains, serp_info_domains, serp_created_at, serp_updated_at,
                 serp_intent, serp_confidence, serp_docs_with_offers, serp_total_docs, serp_offer_ratio,
                 serp_avg_price, serp_min_price, serp_max_price, serp_median_price, serp_currency,
                 serp_offers_count, serp_offers_with_discount, serp_avg_discount_percent,
                 serp_top_urls, serp_lsi_phrases,
                 direct_shows, direct_clicks, direct_ctr, direct_min_cpc, direct_avg_cpc,
                 direct_max_cpc, direct_recommended_cpc, direct_competition_level,
                 direct_first_place_bid, direct_first_place_price,
                 kei, difficulty, competition_score, potential_traffic, expected_ctr,
                 detected_brand, brand_confidence,
                 funnel_stage, funnel_priority)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,
                        ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,
                        ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            '''
            
            saved_count = 0
            for i in range(0, len(queries_data), batch_size):
                batch = queries_data[i:i + batch_size]
                batch_num = i // batch_size + 1
                
                try:
                    cursor.executemany(insert_query, batch)
                    saved_count += len(batch)
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 –±–∞—Ç—á–µ–π
                    if batch_num % 10 == 0 or batch_num == total_batches:
                        print(f"  üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {saved_count}/{len(queries_data)} –∑–∞–ø—Ä–æ—Å–æ–≤ ({batch_num}/{total_batches} –±–∞—Ç—á–µ–π)")
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –±–∞—Ç—á–∞ {batch_num}: {e}")
                    # –ü—Ä–æ–±—É–µ–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏ –µ—Å–ª–∏ –±–∞—Ç—á –Ω–µ –ø—Ä–æ—à—ë–ª
                    for single_record in batch:
                        try:
                            cursor.execute(insert_query, single_record)
                            saved_count += 1
                        except Exception as e2:
                            query_name = single_record[1][:50] if len(single_record) > 1 else 'unknown'
                            print(f"    ‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ '{query_name}...': {e2}")
            
            conn.commit()
            
            if saved_count < len(queries_data):
                print(f"  ‚ö†Ô∏è  –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Ç–æ–ª—å–∫–æ {saved_count} –∏–∑ {len(queries_data)} –∑–∞–ø—Ä–æ—Å–æ–≤")
            else:
                print(f"  ‚úì –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
            
            # –ö–æ–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –¥—Ä—É–≥–∏—Ö –≥—Ä—É–ø–ø –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –µ—Å—Ç—å —Ç–∞–º
            keywords = df['keyword'].tolist() if 'keyword' in df.columns else []
            if keywords:
                # –†–∞–∑–±–∏–≤–∞–µ–º keywords –Ω–∞ –±–∞—Ç—á–∏ —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–∫–∏ "too many SQL variables"
                # SQLite –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤ IN clause (~999)
                keyword_batch_size = 500  # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è IN clause
                total_keyword_batches = (len(keywords) + keyword_batch_size - 1) // keyword_batch_size
                updated_total = 0
                
                for kb_idx in range(0, len(keywords), keyword_batch_size):
                    keyword_batch = keywords[kb_idx:kb_idx + keyword_batch_size]
                    placeholders = ','.join(['?'] * len(keyword_batch))
                    
                    # –°–Ω–∞—á–∞–ª–∞ –æ–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏ –±–µ–∑ completed –¥–∞–Ω–Ω—ã—Ö
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º f-string –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏ placeholders
                    update_query = f'''
                        UPDATE master_queries AS target
                        SET 
                            normalized = COALESCE(target.normalized, src.normalized),
                            lemmatized = COALESCE(target.lemmatized, src.lemmatized),
                            words_count = COALESCE(target.words_count, src.words_count),
                            main_words = COALESCE(target.main_words, src.main_words),
                            key_phrase = COALESCE(target.key_phrase, src.key_phrase),
                            ner_entities = COALESCE(target.ner_entities, src.ner_entities),
                            ner_locations = COALESCE(target.ner_locations, src.ner_locations),
                            has_geo = COALESCE(target.has_geo, src.has_geo),
                            geo_type = COALESCE(target.geo_type, src.geo_type),
                            geo_country = COALESCE(target.geo_country, src.geo_country),
                            geo_city = COALESCE(target.geo_city, src.geo_city),
                            main_intent = COALESCE(target.main_intent, src.main_intent),
                            commercial_score = COALESCE(target.commercial_score, src.commercial_score),
                            informational_score = COALESCE(target.informational_score, src.informational_score),
                            navigational_score = COALESCE(target.navigational_score, src.navigational_score),
                            is_commercial = COALESCE(target.is_commercial, src.is_commercial),
                            is_wholesale = COALESCE(target.is_wholesale, src.is_wholesale),
                            is_urgent = COALESCE(target.is_urgent, src.is_urgent),
                            is_diy = COALESCE(target.is_diy, src.is_diy),
                            is_review = COALESCE(target.is_review, src.is_review),
                            is_brand_query = COALESCE(target.is_brand_query, src.is_brand_query),
                            serp_query_hash = COALESCE(target.serp_query_hash, src.serp_query_hash),
                            serp_req_id = COALESCE(target.serp_req_id, src.serp_req_id),
                            serp_status = CASE WHEN target.serp_status = 'completed' THEN target.serp_status ELSE src.serp_status END,
                            serp_error_message = COALESCE(target.serp_error_message, src.serp_error_message),
                            serp_found_docs = COALESCE(target.serp_found_docs, src.serp_found_docs),
                            serp_main_pages_count = COALESCE(target.serp_main_pages_count, src.serp_main_pages_count),
                            serp_titles_with_keyword = COALESCE(target.serp_titles_with_keyword, src.serp_titles_with_keyword),
                            serp_commercial_domains = COALESCE(target.serp_commercial_domains, src.serp_commercial_domains),
                            serp_info_domains = COALESCE(target.serp_info_domains, src.serp_info_domains),
                            serp_top_urls = COALESCE(target.serp_top_urls, src.serp_top_urls),
                            serp_lsi_phrases = COALESCE(target.serp_lsi_phrases, src.serp_lsi_phrases),
                            serp_intent = COALESCE(target.serp_intent, src.serp_intent),
                            serp_confidence = COALESCE(target.serp_confidence, src.serp_confidence),
                            serp_docs_with_offers = COALESCE(target.serp_docs_with_offers, src.serp_docs_with_offers),
                            serp_total_docs = COALESCE(target.serp_total_docs, src.serp_total_docs),
                            serp_offer_ratio = COALESCE(target.serp_offer_ratio, src.serp_offer_ratio),
                            serp_avg_price = COALESCE(target.serp_avg_price, src.serp_avg_price),
                            serp_min_price = COALESCE(target.serp_min_price, src.serp_min_price),
                            serp_max_price = COALESCE(target.serp_max_price, src.serp_max_price),
                            serp_median_price = COALESCE(target.serp_median_price, src.serp_median_price),
                            serp_currency = COALESCE(target.serp_currency, src.serp_currency),
                            serp_offers_count = COALESCE(target.serp_offers_count, src.serp_offers_count),
                            serp_offers_with_discount = COALESCE(target.serp_offers_with_discount, src.serp_offers_with_discount),
                            serp_avg_discount_percent = COALESCE(target.serp_avg_discount_percent, src.serp_avg_discount_percent),
                            direct_shows = COALESCE(target.direct_shows, src.direct_shows),
                            direct_clicks = COALESCE(target.direct_clicks, src.direct_clicks),
                            direct_ctr = COALESCE(target.direct_ctr, src.direct_ctr),
                            direct_min_cpc = COALESCE(target.direct_min_cpc, src.direct_min_cpc),
                            direct_avg_cpc = COALESCE(target.direct_avg_cpc, src.direct_avg_cpc),
                            direct_max_cpc = COALESCE(target.direct_max_cpc, src.direct_max_cpc),
                            direct_recommended_cpc = COALESCE(target.direct_recommended_cpc, src.direct_recommended_cpc),
                            direct_competition_level = COALESCE(target.direct_competition_level, src.direct_competition_level),
                            direct_first_place_bid = COALESCE(target.direct_first_place_bid, src.direct_first_place_bid),
                            direct_first_place_price = COALESCE(target.direct_first_place_price, src.direct_first_place_price),
                            kei = COALESCE(target.kei, src.kei),
                            difficulty = COALESCE(target.difficulty, src.difficulty),
                            competition_score = COALESCE(target.competition_score, src.competition_score),
                            potential_traffic = COALESCE(target.potential_traffic, src.potential_traffic),
                            expected_ctr = COALESCE(target.expected_ctr, src.expected_ctr),
                            detected_brand = COALESCE(target.detected_brand, src.detected_brand),
                            brand_confidence = COALESCE(target.brand_confidence, src.brand_confidence),
                            funnel_stage = COALESCE(target.funnel_stage, src.funnel_stage),
                            funnel_priority = COALESCE(target.funnel_priority, src.funnel_priority)
                        FROM (
                            SELECT *
                            FROM master_queries AS src_inner
                            WHERE src_inner.keyword IN ({placeholders})
                              AND src_inner.group_name != ?
                              AND src_inner.serp_status = 'completed'
                              AND src_inner.serp_found_docs IS NOT NULL
                              AND src_inner.id = (
                                  SELECT id FROM master_queries
                                  WHERE keyword = src_inner.keyword
                                    AND group_name != ?
                                    AND serp_status = 'completed'
                                    AND serp_found_docs IS NOT NULL
                                  ORDER BY updated_at DESC
                                  LIMIT 1
                              )
                        ) AS src
                        WHERE target.group_name = ?
                          AND target.keyword = src.keyword
                          AND (target.serp_status IS NULL OR target.serp_status != 'completed')
                    '''
                    try:
                        cursor.execute(update_query, keyword_batch + [group_name, group_name, group_name])
                        updated_total += cursor.rowcount
                    except Exception as e:
                        print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –±–∞—Ç—á–∞ keywords {kb_idx // keyword_batch_size + 1}: {e}")
                
                updated_count = updated_total
                
                # –ó–∞—Ç–µ–º –≤—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –Ω–æ–≤–æ–π –≥—Ä—É–ø–ø–µ
                # –¢–∞–∫–∂–µ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏
                inserted_total = 0
                for kb_idx in range(0, len(keywords), keyword_batch_size):
                    keyword_batch = keywords[kb_idx:kb_idx + keyword_batch_size]
                    placeholders = ','.join(['?'] * len(keyword_batch))
                    
                    insert_query = f'''
                        INSERT OR IGNORE INTO master_queries 
                        (group_name, keyword, frequency_world, frequency_exact,
                         normalized, lemmatized, words_count, main_words, key_phrase,
                         ner_entities, ner_locations, has_geo, geo_type, geo_country, geo_city,
                         main_intent, commercial_score, informational_score, navigational_score,
                         is_commercial, is_wholesale, is_urgent, is_diy, is_review, is_brand_query,
                         serp_query_hash, serp_req_id, serp_status, serp_error_message,
                         serp_found_docs, serp_main_pages_count, serp_titles_with_keyword,
                         serp_commercial_domains, serp_info_domains, serp_created_at, serp_updated_at,
                         serp_intent, serp_confidence, serp_docs_with_offers, serp_total_docs, serp_offer_ratio,
                         serp_avg_price, serp_min_price, serp_max_price, serp_median_price, serp_currency,
                         serp_offers_count, serp_offers_with_discount, serp_avg_discount_percent,
                         serp_top_urls, serp_lsi_phrases,
                         direct_shows, direct_clicks, direct_ctr, direct_min_cpc, direct_avg_cpc,
                         direct_max_cpc, direct_recommended_cpc, direct_competition_level,
                         direct_first_place_bid, direct_first_place_price,
                         kei, difficulty, competition_score, potential_traffic, expected_ctr,
                         detected_brand, brand_confidence, funnel_stage, funnel_priority)
                        SELECT 
                            ? as group_name,
                            keyword, frequency_world, frequency_exact,
                            normalized, lemmatized, words_count, main_words, key_phrase,
                            ner_entities, ner_locations, has_geo, geo_type, geo_country, geo_city,
                            main_intent, commercial_score, informational_score, navigational_score,
                            is_commercial, is_wholesale, is_urgent, is_diy, is_review, is_brand_query,
                            serp_query_hash, serp_req_id, serp_status, serp_error_message,
                            serp_found_docs, serp_main_pages_count, serp_titles_with_keyword,
                            serp_commercial_domains, serp_info_domains, serp_created_at, serp_updated_at,
                            serp_intent, serp_confidence, serp_docs_with_offers, serp_total_docs, serp_offer_ratio,
                            serp_avg_price, serp_min_price, serp_max_price, serp_median_price, serp_currency,
                            serp_offers_count, serp_offers_with_discount, serp_avg_discount_percent,
                            serp_top_urls, serp_lsi_phrases,
                            direct_shows, direct_clicks, direct_ctr, direct_min_cpc, direct_avg_cpc,
                            direct_max_cpc, direct_recommended_cpc, direct_competition_level,
                            direct_first_place_bid, direct_first_place_price,
                            kei, difficulty, competition_score, potential_traffic, expected_ctr,
                            detected_brand, brand_confidence, funnel_stage, funnel_priority
                        FROM master_queries AS src
                        WHERE src.keyword IN ({placeholders})
                          AND src.group_name != ?
                          AND src.serp_status = 'completed'
                          AND src.serp_found_docs IS NOT NULL
                          AND src.id = (
                              SELECT id FROM master_queries
                              WHERE keyword = src.keyword
                                AND group_name != ?
                                AND serp_status = 'completed'
                                AND serp_found_docs IS NOT NULL
                              ORDER BY updated_at DESC
                              LIMIT 1
                          )
                          AND NOT EXISTS (
                              SELECT 1 FROM master_queries AS existing
                              WHERE existing.group_name = ?
                                AND existing.keyword = src.keyword
                          )
                    '''
                    try:
                        cursor.execute(insert_query, [group_name] + keyword_batch + [group_name, group_name, group_name])
                        inserted_total += cursor.rowcount
                    except Exception as e:
                        print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ –±–∞—Ç—á–∞ keywords {kb_idx // keyword_batch_size + 1}: {e}")
                
                inserted_count = inserted_total
                
                copied_count = updated_count + inserted_count
                if copied_count > 0:
                    print(f"üìã –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥—Ä—É–≥–∏—Ö –≥—Ä—É–ø–ø: –æ–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count}, –¥–æ–±–∞–≤–ª–µ–Ω–æ {inserted_count}")
                conn.commit()
            
            print(f"üíæ Master DB: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(df)} –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –≥—Ä—É–ø–ø—ã '{group_name}'")
            
        finally:
            conn.close()
    
    def load_queries(
        self,
        group_name: str,
        include_serp_urls: bool = True
    ) -> Optional[pd.DataFrame]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –í–°–ï –¥–∞–Ω–Ω—ã–µ –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º –∏–∑ –º–∞—Å—Ç–µ—Ä-—Ç–∞–±–ª–∏—Ü—ã
        
        Args:
            group_name: –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã
            include_serp_urls: –í–∫–ª—é—á–∞—Ç—å –ª–∏ serp_top_urls (–±–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ)
            
        Returns:
            DataFrame —Å–æ –≤—Å–µ–º–∏ –ø–æ–ª—è–º–∏ –∏–ª–∏ None
        """
        conn = sqlite3.connect(self.db_path)
        
        # –í—ã–±–∏—Ä–∞–µ–º –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –∫—Ä–æ–º–µ id –∏ timestamps
        columns = """
            keyword, frequency_world, frequency_exact,
            normalized, lemmatized, words_count, main_words, key_phrase,
            ner_entities, ner_locations,
            has_geo, geo_type, geo_country, geo_city,
            main_intent, commercial_score, informational_score, navigational_score,
            is_commercial, is_wholesale, is_urgent, is_diy, is_review, is_brand_query,
            serp_query_hash, serp_found_docs, serp_main_pages_count, serp_titles_with_keyword,
            serp_commercial_domains, serp_info_domains, serp_created_at,
            serp_intent, serp_confidence, serp_docs_with_offers, serp_total_docs, serp_offer_ratio,
            serp_avg_price, serp_min_price, serp_max_price, serp_median_price, serp_currency,
            serp_offers_count, serp_offers_with_discount, serp_avg_discount_percent,
            {serp_urls}
            serp_lsi_phrases,
            direct_shows, direct_clicks, direct_ctr, direct_min_cpc, direct_avg_cpc,
            direct_max_cpc, direct_recommended_cpc, direct_competition_level,
            direct_first_place_bid, direct_first_place_price,
            kei, difficulty, competition_score, potential_traffic, expected_ctr,
            detected_brand, brand_confidence,
            funnel_stage, funnel_priority
        """.format(serp_urls='serp_top_urls,' if include_serp_urls else '')
        
        query = f'''
            SELECT {columns}
            FROM master_queries
            WHERE group_name = ?
            ORDER BY frequency_world DESC
        '''
        
        df = pd.read_sql_query(query, conn, params=(group_name,))
        conn.close()
        
        if df.empty:
            return None
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–ª–∏–∞—Å—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∫–æ–¥–æ–º
        if 'serp_found_docs' in df.columns:
            df['serp_docs_count'] = df['serp_found_docs']
        if 'serp_main_pages_count' in df.columns:
            df['serp_main_pages'] = df['serp_main_pages_count']
            # –ö–æ–ª-–≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–æ–±—â–µ–µ –∫–æ–ª-–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –º–∏–Ω—É—Å –≥–ª–∞–≤–Ω—ã–µ)
            df['serp_internal_pages_count'] = (
                df['serp_docs_count'] - df['serp_main_pages_count']
            ).fillna(0).clip(lower=0).astype(int)
        if 'serp_titles_with_keyword' in df.columns:
            df['serp_titles_count'] = df['serp_titles_with_keyword']
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º serp_lsi_phrases –≤ lsi_phrases –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if 'serp_lsi_phrases' in df.columns and 'lsi_phrases' not in df.columns:
            import json
            def parse_lsi_phrases(val):
                if pd.isna(val) or val is None or val == '':
                    return []
                if isinstance(val, str):
                    try:
                        parsed = json.loads(val)
                        # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
                        if isinstance(parsed, list):
                            result = []
                            for item in parsed:
                                if isinstance(item, str):
                                    result.append({'phrase': item, 'frequency': 1, 'source': 'unknown'})
                                elif isinstance(item, dict):
                                    result.append(item)
                            return result
                        return parsed if isinstance(parsed, list) else []
                    except (json.JSONDecodeError, TypeError):
                        return []
                elif isinstance(val, list):
                    return val
                return []
            
            df['lsi_phrases'] = df['serp_lsi_phrases'].apply(parse_lsi_phrases)
            
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ LSI —Ñ—Ä–∞–∑
            lsi_non_empty = df['lsi_phrases'].apply(lambda x: isinstance(x, list) and len(x) > 0).sum()
            lsi_empty = len(df) - lsi_non_empty
            print(f"   ‚úì LSI —Ñ—Ä–∞–∑—ã: {lsi_non_empty} –∑–∞–ø—Ä–æ—Å–æ–≤ —Å LSI, {lsi_empty} –±–µ–∑ LSI")
        
        print(f"üì¶ Master DB: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –≥—Ä—É–ø–ø—ã '{group_name}'")
        print(f"   ‚úì –ò–Ω—Ç–µ–Ω—Ç: {df['main_intent'].notna().sum()} –∑–∞–ø–∏—Å–µ–π")
        print(f"   ‚úì SERP: {df['serp_found_docs'].notna().sum()} –∑–∞–ø–∏—Å–µ–π")
        print(f"   ‚úì Direct: {df['direct_shows'].notna().sum()} –∑–∞–ø–∏—Å–µ–π")
        
        return df
    
    def group_exists(self, group_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –≥—Ä—É–ø–ø–∞"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute(
            'SELECT COUNT(*) FROM master_queries WHERE group_name = ?',
            (group_name,)
        )
        exists = cursor.fetchone()[0] > 0
        
        conn.close()
        return exists
    
    def get_group_info(self, group_name: str) -> Optional[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≥—Ä—É–ø–ø–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã query_groups
        
        Args:
            group_name: –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≥—Ä—É–ø–ø–µ (csv_hash, csv_file_path, total_queries –∏ —Ç.–¥.)
            –∏–ª–∏ None –µ—Å–ª–∏ –≥—Ä—É–ø–ø–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT csv_file_path, csv_hash, total_queries, unique_queries, 
                   duplicates_removed, imported_at, updated_at
            FROM query_groups
            WHERE group_name = ?
        ''', (group_name,))
        
        result = cursor.fetchone()
        conn.close()
        
        if result is None:
            return None
        
        return {
            'csv_file_path': result[0],
            'csv_hash': result[1],
            'total_queries': result[2],
            'unique_queries': result[3],
            'duplicates_removed': result[4],
            'imported_at': result[5],
            'updated_at': result[6]
        }
    
    def get_statistics(self, group_name: str = None) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥—Ä—É–ø–ø–µ –∏–ª–∏ –≤—Å–µ–π –ë–î"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        if group_name:
            cursor.execute('''
                SELECT 
                    COUNT(*) as total,
                    SUM(CASE WHEN main_intent IS NOT NULL THEN 1 ELSE 0 END) as with_intent,
                    SUM(CASE WHEN serp_found_docs IS NOT NULL THEN 1 ELSE 0 END) as with_serp,
                    SUM(CASE WHEN direct_shows IS NOT NULL THEN 1 ELSE 0 END) as with_direct,
                    AVG(kei) as avg_kei,
                    AVG(serp_offer_ratio) as avg_offer_ratio
                FROM master_queries
                WHERE group_name = ?
            ''', (group_name,))
        else:
            cursor.execute('''
                SELECT 
                    COUNT(*) as total,
                    SUM(CASE WHEN main_intent IS NOT NULL THEN 1 ELSE 0 END) as with_intent,
                    SUM(CASE WHEN serp_found_docs IS NOT NULL THEN 1 ELSE 0 END) as with_serp,
                    SUM(CASE WHEN direct_shows IS NOT NULL THEN 1 ELSE 0 END) as with_direct,
                    AVG(kei) as avg_kei,
                    AVG(serp_offer_ratio) as avg_offer_ratio
                FROM master_queries
            ''')
        
        result = cursor.fetchone()
        conn.close()
        
        return {
            'total_queries': result[0],
            'with_intent': result[1],
            'with_serp': result[2],
            'with_direct': result[3],
            'avg_kei': round(result[4], 2) if result[4] else 0,
            'avg_offer_ratio': round(result[5], 4) if result[5] else 0,
        }
    
    def analyze_query_performance(self, query: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å SQL –∑–∞–ø—Ä–æ—Å–∞ (EXPLAIN QUERY PLAN)
        –ê–Ω–∞–ª–æ–≥ EXPLAIN ANALYZE –≤ PostgreSQL
        
        Args:
            query: SQL –∑–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –ü–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # EXPLAIN QUERY PLAN –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞–∫–∏–µ –∏–Ω–¥–µ–∫—Å—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è
        cursor.execute(f"EXPLAIN QUERY PLAN {query}")
        plan = cursor.fetchall()
        
        conn.close()
        
        return {
            'query': query,
            'execution_plan': [
                {
                    'id': row[0],
                    'parent': row[1],
                    'detail': row[3]
                }
                for row in plan
            ]
        }
    
    def rebuild_indexes(self):
        """
        –ü–µ—Ä–µ—Å–æ–∑–¥–∞—ë—Ç –∏–Ω–¥–µ–∫—Å—ã (–∞–Ω–∞–ª–æ–≥ REINDEX –≤ PostgreSQL)
        –ü–æ–ª–µ–∑–Ω–æ –ø–æ—Å–ª–µ –º–∞—Å—Å–æ–≤—ã—Ö INSERT/UPDATE
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        print("üîÑ –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤...")
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∏–Ω–¥–µ–∫—Å—ã
        cursor.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='index' AND name LIKE 'idx_master_%'
        """)
        
        indexes = cursor.fetchall()
        for (index_name,) in indexes:
            cursor.execute(f"DROP INDEX IF EXISTS {index_name}")
            print(f"   ‚úì –£–¥–∞–ª—ë–Ω: {index_name}")
        
        # –°–æ–∑–¥–∞—ë–º –∑–∞–Ω–æ–≤–æ
        for index_sql in MASTER_QUERY_INDEXES:
            cursor.execute(index_sql)
        
        # ANALYZE –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        cursor.execute("ANALYZE master_queries")
        
        conn.commit()
        conn.close()
        
        print(f"‚úì –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–æ {len(MASTER_QUERY_INDEXES)} –∏–Ω–¥–µ–∫—Å–æ–≤")
        print("‚úì ANALYZE –≤—ã–ø–æ–ª–Ω–µ–Ω (—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∞)")
    
    def get_index_usage_stats(self) -> List[Dict[str, Any]]:
        """
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞–∫–∏–µ –∏–Ω–¥–µ–∫—Å—ã —Ä–µ–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤
        cursor.execute("""
            SELECT 
                name,
                tbl_name,
                sql
            FROM sqlite_master 
            WHERE type='index' AND name LIKE 'idx_master_%'
            ORDER BY name
        """)
        
        indexes = []
        for row in cursor.fetchall():
            indexes.append({
                'index_name': row[0],
                'table_name': row[1],
                'definition': row[2]
            })
        
        conn.close()
        
        return indexes
    
    def optimize_database(self):
        """
        –ü–æ–ª–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ë–î (–∫–∞–∫ VACUUM ANALYZE –≤ PostgreSQL)
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        print("üîÑ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ë–î...")
        
        # 1. VACUUM - –æ—á–∏—Å—Ç–∫–∞ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–≥–æ –º–µ—Å—Ç–∞
        print("   ‚Üí VACUUM (–¥–µ—Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è)...")
        cursor.execute("VACUUM")
        
        # 2. ANALYZE - –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        print("   ‚Üí ANALYZE (—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)...")
        cursor.execute("ANALYZE")
        
        # 3. PRAGMA optimize - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        print("   ‚Üí PRAGMA optimize...")
        cursor.execute("PRAGMA optimize")
        
        conn.commit()
        conn.close()
        
        # –†–∞–∑–º–µ—Ä –ë–î –ø–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        size_mb = self.db_path.stat().st_size / (1024 * 1024)
        
        print(f"‚úì –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        print(f"‚úì –†–∞–∑–º–µ—Ä –ë–î: {size_mb:.1f} MB")
    
    def get_pending_serp_queries(self, group_name: str) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∑–∞–ø—Ä–æ—Å—ã —Å –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π SERP
        (–¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ –ø–∞–¥–µ–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–∞)
        
        Args:
            group_name: –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º pending/processing
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT keyword, serp_req_id, serp_status, serp_error_message
            FROM master_queries
            WHERE group_name = ? 
              AND serp_status IN ('pending', 'processing')
            ORDER BY created_at
        ''', (group_name,))
        
        pending = []
        for row in cursor.fetchall():
            pending.append({
                'keyword': row[0],
                'serp_req_id': row[1],
                'serp_status': row[2],
                'serp_error_message': row[3]
            })
        
        conn.close()
        return pending
    
    def update_serp_status(
        self,
        group_name: str,
        keyword: str,
        status: str,
        req_id: str = None,
        error_message: str = None
    ):
        """
        –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å SERP –∑–∞–ø—Ä–æ—Å–∞
        
        –°—Ç–∞—Ç—É—Å—ã:
        - pending: –ó–∞–ø—Ä–æ—Å –µ—â—ë –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ xmlstock
        - processing: –ó–∞–ø—Ä–æ—Å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω (–ø–æ–ª—É—á–µ–Ω req_id), –∂–¥—ë–º –æ—Ç–≤–µ—Ç–∞ –æ—Ç xmlstock
        - completed: –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã
        - error: –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
        
        Args:
            group_name: –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã
            keyword: –ó–∞–ø—Ä–æ—Å
            status: –°—Ç–∞—Ç—É—Å (pending/processing/completed/error)
            req_id: ID –∑–∞–ø—Ä–æ—Å–∞ –≤ xmlstock (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –æ—Ç–ø—Ä–∞–≤–∫–∏)
            error_message: –¢–µ–∫—Å—Ç –æ—à–∏–±–∫–∏
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            UPDATE master_queries
            SET serp_status = ?,
                serp_req_id = COALESCE(?, serp_req_id),
                serp_error_message = ?,
                serp_updated_at = CURRENT_TIMESTAMP
            WHERE group_name = ? AND keyword = ?
        ''', (status, req_id, error_message, group_name, keyword))
        
        conn.commit()
        conn.close()
    
    def mark_serp_as_pending(self, group_name: str, keywords: List[str]):
        """
        –û—Ç–º–µ—Ç–∏—Ç—å –∑–∞–ø—Ä–æ—Å—ã –∫–∞–∫ –æ–∂–∏–¥–∞—é—â–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ SERP
        (–ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –º–∞—Å—Å–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏)
        
        Args:
            group_name: –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã
            keywords: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Batch update
        cursor.executemany('''
            UPDATE master_queries
            SET serp_status = 'pending',
                serp_updated_at = CURRENT_TIMESTAMP
            WHERE group_name = ? AND keyword = ?
        ''', [(group_name, kw) for kw in keywords])
        
        conn.commit()
        conn.close()
        
        print(f"‚úì –û—Ç–º–µ—á–µ–Ω–æ {len(keywords)} –∑–∞–ø—Ä–æ—Å–æ–≤ –∫–∞–∫ pending –¥–ª—è SERP –∑–∞–≥—Ä—É–∑–∫–∏")
    
    def get_serp_statistics(self, group_name: str) -> Dict[str, Any]:
        """
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ SERP –∑–∞–≥—Ä—É–∑–∫–µ
        
        Args:
            group_name: –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç–∞—Ç—É—Å–∞–º
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT 
                COUNT(*) as total,
                SUM(CASE WHEN serp_status = 'completed' THEN 1 ELSE 0 END) as completed,
                SUM(CASE WHEN serp_status = 'pending' THEN 1 ELSE 0 END) as pending,
                SUM(CASE WHEN serp_status = 'processing' THEN 1 ELSE 0 END) as processing,
                SUM(CASE WHEN serp_status = 'error' THEN 1 ELSE 0 END) as error,
                SUM(CASE WHEN serp_found_docs IS NOT NULL THEN 1 ELSE 0 END) as with_data
            FROM master_queries
            WHERE group_name = ?
        ''', (group_name,))
        
        row = cursor.fetchone()
        conn.close()
        
        if not row or row[0] == 0:
            return {
                'total': 0,
                'completed': 0,
                'pending': 0,
                'processing': 0,
                'error': 0,
                'with_data': 0,
                'completion_rate': 0.0
            }
        
        return {
            'total': row[0],
            'completed': row[1] or 0,
            'pending': row[2] or 0,
            'processing': row[3] or 0,
            'error': row[4] or 0,
            'with_data': row[5] or 0,
            'completion_rate': (row[1] or 0) / row[0] if row[0] > 0 else 0.0
        }
    
    def update_serp_metrics(
        self,
        group_name: str,
        keyword: str,
        metrics: Dict[str, Any],
        documents: List[Dict],
        lsi_phrases: List[Dict]
    ):
        """
        –û–±–Ω–æ–≤–∏—Ç—å SERP –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        
        Args:
            group_name: –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã
            keyword: –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
            metrics: –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏–∑ SERPDataEnricher
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è TOP-20)
            lsi_phrases: –°–ø–∏—Å–æ–∫ LSI —Ñ—Ä–∞–∑
        """
        conn = sqlite3.connect(self.db_path)
        try:
            cursor = conn.cursor()
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º TOP-20 URLs –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            top_urls = []
            for i, doc in enumerate(documents[:20], 1):
                top_urls.append({
                    'position': i,
                    'url': doc.get('url', ''),
                    'domain': doc.get('domain', ''),
                    'title': doc.get('title', ''),
                    'is_commercial': doc.get('is_commercial', False)
                })
            
            # LSI —Ñ—Ä–∞–∑—ã –∫–∞–∫ JSON
            lsi_json = json.dumps(lsi_phrases, ensure_ascii=False) if lsi_phrases else None
            top_urls_json = json.dumps(top_urls, ensure_ascii=False) if top_urls else None
            
            cursor.execute('''
                UPDATE master_queries
                SET
                    serp_found_docs = ?,
                    serp_main_pages_count = ?,
                    serp_titles_with_keyword = ?,
                    serp_commercial_domains = ?,
                    serp_info_domains = ?,
                    serp_intent = ?,
                    serp_confidence = ?,
                    serp_docs_with_offers = ?,
                    serp_total_docs = ?,
                    serp_offer_ratio = ?,
                    serp_avg_price = ?,
                    serp_min_price = ?,
                    serp_max_price = ?,
                    serp_median_price = ?,
                    serp_currency = ?,
                    serp_offers_count = ?,
                    serp_offers_with_discount = ?,
                    serp_avg_discount_percent = ?,
                    serp_top_urls = ?,
                    serp_lsi_phrases = ?,
                    serp_updated_at = CURRENT_TIMESTAMP
                WHERE group_name = ? AND keyword = ?
            ''', (
                metrics.get('found_docs'),
                metrics.get('main_pages_count'),
                metrics.get('titles_with_keyword'),
                metrics.get('commercial_domains_count'),
                metrics.get('informational_domains_count'),
                metrics.get('serp_intent'),
                metrics.get('serp_confidence'),
                metrics.get('docs_with_offers'),
                metrics.get('total_docs_analyzed'),
                metrics.get('offer_ratio'),
                metrics.get('avg_price'),
                metrics.get('min_price'),
                metrics.get('max_price'),
                metrics.get('median_price'),
                metrics.get('currency', 'RUR'),
                metrics.get('offers_count'),
                metrics.get('offers_with_discount'),
                metrics.get('avg_discount_percent'),
                top_urls_json,
                lsi_json,
                group_name,
                keyword
            ))
            
            conn.commit()
        
        finally:
            conn.close()


__all__ = ['MasterQueryDatabase']

