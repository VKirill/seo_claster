"""
–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ Master DB
"""

import sqlite3
import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Optional


class QuerySaver:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ Master DB"""
    
    def __init__(self, db_path: Path, query_loader):
        """
        Args:
            db_path: –ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
            query_loader: –≠–∫–∑–µ–º–ø–ª—è—Ä QueryLoader –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        self.db_path = db_path
        self.query_loader = query_loader
    
    def save_queries(
        self,
        group_name: str,
        df: pd.DataFrame,
        csv_path: Path = None,
        csv_hash: str = None
    ):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç/–æ–±–Ω–æ–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –≤ master —Ç–∞–±–ª–∏—Ü–µ
        
        Args:
            group_name: –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã
            df: DataFrame —Å–æ –í–°–ï–ú–ò –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            csv_path: –ü—É—Ç—å –∫ CSV (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            csv_hash: Hash CSV (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        conn = sqlite3.connect(self.db_path)
        
        try:
            cursor = conn.cursor()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã –∏–∑ CSV —Ñ–∞–π–ª–∞ –µ—Å–ª–∏ –æ–Ω —É–∫–∞–∑–∞–Ω (–¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è)
            csv_frequencies = {}
            if csv_path and csv_path.exists():
                try:
                    from seo_analyzer.core.helpers import load_csv_data, normalize_dataframe_columns
                    csv_raw_df = load_csv_data(csv_path)
                    if not csv_raw_df.empty:
                        csv_normalized = normalize_dataframe_columns(csv_raw_df)
                        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —á–∞—Å—Ç–æ—Ç –∏–∑ CSV –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
                        if 'keyword' in csv_normalized.columns:
                            for idx, row in csv_normalized.iterrows():
                                keyword = row.get('keyword')
                                if keyword:
                                    csv_frequencies[keyword] = {
                                        'frequency_world': row.get('frequency_world', 0) if pd.notna(row.get('frequency_world')) else 0,
                                        'frequency_exact': row.get('frequency_exact', 0) if pd.notna(row.get('frequency_exact')) else 0
                                    }
                            
                            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–æ—Ç—ã –≤ CSV
                            csv_non_zero_world = sum(1 for v in csv_frequencies.values() if v.get('frequency_world', 0) > 0)
                            csv_non_zero_exact = sum(1 for v in csv_frequencies.values() if v.get('frequency_exact', 0) > 0)
                            print(f"  üìÇ CSV —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {len(csv_frequencies)} –∑–∞–ø—Ä–æ—Å–æ–≤")
                            print(f"  üìä –ß–∞—Å—Ç–æ—Ç—ã –≤ CSV: {csv_non_zero_world} —Å –Ω–µ–Ω—É–ª–µ–≤–æ–π —á–∞—Å—Ç–æ—Ç–æ–π (–º–∏—Ä), {csv_non_zero_exact} (—Ç–æ—á–Ω–∞—è)")
                except Exception as e:
                    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å CSV - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –Ω–µ–≥–æ
                    print(f"  ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å CSV –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —á–∞—Å—Ç–æ—Ç: {e}")
            elif csv_path:
                print(f"  ‚ö†Ô∏è  CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {csv_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
            existing_df = None
            try:
                existing_df = self.query_loader.load_queries(group_name)
                if existing_df is not None and len(existing_df) > 0:
                    existing_df = existing_df.set_index('keyword')
            except:
                existing_df = None
            
            # –ú–∞–ø–ø–∏–Ω–≥ –∏–º–µ–Ω –∫–æ–ª–æ–Ω–æ–∫
            column_mapping = {
                'serp_docs_count': 'serp_found_docs',
                'serp_titles_count': 'serp_titles_with_keyword',
            }
            
            df_copy = df.copy()
            for df_col, db_col in column_mapping.items():
                if df_col in df_copy.columns and db_col not in df_copy.columns:
                    df_copy[db_col] = df_copy[df_col]
            
            # –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç –∏–∑ CSV –µ—Å–ª–∏ –æ–Ω–∏ –Ω—É–ª–µ–≤—ã–µ –≤ DataFrame
            frequencies_from_csv_count = 0
            frequencies_world_from_csv = 0
            frequencies_exact_from_csv = 0
            
            if csv_frequencies:
                for idx, row in df_copy.iterrows():
                    keyword = row.get('keyword')
                    if keyword and keyword in csv_frequencies:
                        csv_freq_data = csv_frequencies[keyword]
                        
                        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã –∏–∑ CSV –µ—Å–ª–∏ –≤ DataFrame –æ–Ω–∏ –Ω—É–ª–µ–≤—ã–µ –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç
                        if 'frequency_world' in df_copy.columns:
                            df_freq_world = row.get('frequency_world', 0)
                            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å–∫–∞–ª—è—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                            if isinstance(df_freq_world, pd.Series):
                                df_freq_world = df_freq_world.iloc[0] if len(df_freq_world) > 0 else 0
                            elif isinstance(df_freq_world, (list, tuple, np.ndarray)):
                                df_freq_world = df_freq_world[0] if len(df_freq_world) > 0 else 0
                            
                            csv_freq_world = csv_freq_data.get('frequency_world', 0)
                            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ —Å–∫–∞–ª—è—Ä
                            if (pd.isna(df_freq_world) or df_freq_world == 0) and csv_freq_world > 0:
                                df_copy.at[idx, 'frequency_world'] = csv_freq_world
                                frequencies_from_csv_count += 1
                                frequencies_world_from_csv += 1
                        
                        if 'frequency_exact' in df_copy.columns:
                            df_freq_exact = row.get('frequency_exact', 0)
                            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å–∫–∞–ª—è—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                            if isinstance(df_freq_exact, pd.Series):
                                df_freq_exact = df_freq_exact.iloc[0] if len(df_freq_exact) > 0 else 0
                            elif isinstance(df_freq_exact, (list, tuple, np.ndarray)):
                                df_freq_exact = df_freq_exact[0] if len(df_freq_exact) > 0 else 0
                            
                            csv_freq_exact = csv_freq_data.get('frequency_exact', 0)
                            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ —Å–∫–∞–ª—è—Ä
                            if (pd.isna(df_freq_exact) or df_freq_exact == 0) and csv_freq_exact > 0:
                                df_copy.at[idx, 'frequency_exact'] = csv_freq_exact
                                frequencies_from_csv_count += 1
                                frequencies_exact_from_csv += 1
            
            if frequencies_from_csv_count > 0:
                print(f"  ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {frequencies_from_csv_count} —á–∞—Å—Ç–æ—Ç –∏–∑ CSV —Ñ–∞–π–ª–∞")
                print(f"     - –ß–∞—Å—Ç–æ—Ç–∞ (–º–∏—Ä): {frequencies_world_from_csv} –∑–∞–ø—Ä–æ—Å–æ–≤")
                print(f"     - –ß–∞—Å—Ç–æ—Ç–∞ (—Ç–æ—á–Ω–∞—è): {frequencies_exact_from_csv} –∑–∞–ø—Ä–æ—Å–æ–≤")
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Å –Ω–æ–≤—ã–º–∏
            frequencies_restored_from_db_count = 0
            # –í–ê–ñ–ù–û: –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –ø–æ keyword –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            existing_data_by_keyword = {}
            if existing_df is not None and len(existing_df) > 0:
                existing_df_indexed = existing_df.set_index('keyword')
                for idx, row in df_copy.iterrows():
                    keyword = row.get('keyword')
                    if keyword and keyword in existing_df_indexed.index:
                        existing_row = existing_df_indexed.loc[keyword]
                        existing_data_by_keyword[keyword] = existing_row
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (—á–∞—Å—Ç–æ—Ç—ã) –∏–∑ –ë–î –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ DataFrame
                        # –í–ê–ñ–ù–û: –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã:
                        # 1. –ß–∞—Å—Ç–æ—Ç—ã –∏–∑ CSV —Ñ–∞–π–ª–∞ (—É–∂–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –≤—ã—à–µ)
                        # 2. –ß–∞—Å—Ç–æ—Ç—ã –∏–∑ –Ω–æ–≤–æ–≥–æ DataFrame (–µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å)
                        # 3. –ß–∞—Å—Ç–æ—Ç—ã –∏–∑ –ë–î (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤ –Ω–æ–≤–æ–º DataFrame –∏—Ö –Ω–µ—Ç)
                        basic_fields = ['frequency_world', 'frequency_exact']
                        for field in basic_fields:
                            if field in existing_df.columns and field in df_copy.columns:
                                existing_val = existing_row.get(field)
                                df_val = row.get(field)
                                
                                # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                                # –ï—Å–ª–∏ existing_val —ç—Ç–æ Series –∏–ª–∏ –º–∞—Å—Å–∏–≤ - –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                                if isinstance(existing_val, pd.Series):
                                    existing_val = existing_val.iloc[0] if len(existing_val) > 0 else None
                                elif isinstance(existing_val, (list, tuple, np.ndarray)):
                                    existing_val = existing_val[0] if len(existing_val) > 0 else None
                                
                                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ df_val –≤ —Å–∫–∞–ª—è—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                                if isinstance(df_val, pd.Series):
                                    df_val = df_val.iloc[0] if len(df_val) > 0 else None
                                elif isinstance(df_val, (list, tuple, np.ndarray)):
                                    df_val = df_val[0] if len(df_val) > 0 else None
                                
                                # –õ–æ–≥–∏–∫–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤:
                                # 1. –ï—Å–ª–∏ –≤ df_copy —É–∂–µ –µ—Å—Ç—å –Ω–µ–Ω—É–ª–µ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ (–∏–∑ CSV –∏–ª–∏ –∏–∑ DataFrame) - –Ω–µ —Ç—Ä–æ–≥–∞–µ–º
                                # 2. –ï—Å–ª–∏ –≤ df_copy —á–∞—Å—Ç–æ—Ç–∞ –Ω—É–ª–µ–≤–∞—è/NaN, –Ω–æ –≤ –ë–î –µ—Å—Ç—å –Ω–µ–Ω—É–ª–µ–≤–∞—è - –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–∑ –ë–î
                                # 3. –ï—Å–ª–∏ –≤ df_copy —á–∞—Å—Ç–æ—Ç–∞ –Ω—É–ª–µ–≤–∞—è –∏ –≤ –ë–î —Ç–æ–∂–µ –Ω—É–ª–µ–≤–∞—è - –æ—Å—Ç–∞–≤–ª—è–µ–º –Ω—É–ª–µ–≤—É—é
                                if pd.notna(df_val) and df_val != 0:
                                    # –í df_copy —É–∂–µ –µ—Å—Ç—å –Ω–µ–Ω—É–ª–µ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ - –Ω–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º
                                    pass
                                elif pd.notna(existing_val) and existing_val != 0:
                                    # –í df_copy –Ω–µ—Ç —á–∞—Å—Ç–æ—Ç—ã –∏–ª–∏ –æ–Ω–∞ –Ω—É–ª–µ–≤–∞—è, –Ω–æ –≤ –ë–î –µ—Å—Ç—å –Ω–µ–Ω—É–ª–µ–≤–∞—è - –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
                                    df_copy.at[idx, field] = existing_val
                                    frequencies_restored_from_db_count += 1
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º SERP –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ DataFrame
                        serp_fields = [
                            'serp_req_id', 'serp_status', 'serp_error_message',
                            'serp_found_docs', 'serp_main_pages_count', 'serp_titles_with_keyword',
                            'serp_commercial_domains', 'serp_info_domains',
                            'serp_intent', 'serp_confidence', 'serp_docs_with_offers',
                            'serp_total_docs', 'serp_offer_ratio',
                            'serp_avg_price', 'serp_min_price', 'serp_max_price',
                            'serp_median_price', 'serp_currency',
                            'serp_offers_count', 'serp_offers_with_discount',
                            'serp_avg_discount_percent', 'serp_top_urls', 'serp_lsi_phrases',
                            'serp_created_at', 'serp_updated_at'
                        ]
                        for field in serp_fields:
                            if field in existing_df.columns:
                                existing_val = existing_row.get(field)
                                df_val = row.get(field)
                                
                                # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                                # –ï—Å–ª–∏ existing_val —ç—Ç–æ Series –∏–ª–∏ –º–∞—Å—Å–∏–≤ - –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                                if isinstance(existing_val, pd.Series):
                                    existing_val = existing_val.iloc[0] if len(existing_val) > 0 else None
                                elif isinstance(existing_val, (list, tuple, np.ndarray)):
                                    existing_val = existing_val[0] if len(existing_val) > 0 else None
                                
                                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ df_val –≤ —Å–∫–∞–ª—è—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                                if isinstance(df_val, pd.Series):
                                    df_val = df_val.iloc[0] if len(df_val) > 0 else None
                                elif isinstance(df_val, (list, tuple, np.ndarray)):
                                    df_val = df_val[0] if len(df_val) > 0 else None
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–æ–ª–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ df_copy –ø–µ—Ä–µ–¥ —É—Å—Ç–∞–Ω–æ–≤–∫–æ–π
                                if field not in df_copy.columns:
                                    # –ï—Å–ª–∏ –ø–æ–ª—è –Ω–µ—Ç - —Å–æ–∑–¥–∞–µ–º –µ–≥–æ —Å None –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                                    df_copy[field] = None
                                
                                if pd.notna(existing_val):
                                    # –í–ê–ñ–ù–û: –î–ª—è serp_top_urls –∏ serp_lsi_phrases –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏
                                    # –ü—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏ –Ω–µ –¥–æ–ª–∂–Ω—ã –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î!
                                    is_empty_list = isinstance(df_val, list) and len(df_val) == 0
                                    is_empty_str = isinstance(df_val, str) and df_val.strip() in ('', '[]', 'null', 'NULL', 'None')
                                    
                                    # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ df_val –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ —Å–∫–∞–ª—è—Ä
                                    if pd.isna(df_val) or df_val == '' or df_val == 0 or is_empty_list or is_empty_str:
                                        # –î–ª—è SERP –ø–æ–ª–µ–π (—Å–ø–∏—Å–∫–∏/JSON) - –Ω–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –µ—Å–ª–∏ –≤ –ë–î –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
                                        if field in ('serp_top_urls', 'serp_lsi_phrases'):
                                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤ –ë–î –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ (–Ω–µ –ø—É—Å—Ç—ã–µ)
                                            if isinstance(existing_val, str):
                                                existing_val_str = existing_val.strip()
                                                if existing_val_str and existing_val_str not in ('', '[]', 'null', 'NULL', 'None'):
                                                    # –í –ë–î –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ - –Ω–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –ø—É—Å—Ç—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
                                                    continue
                                            elif isinstance(existing_val, (list, tuple)) and len(existing_val) > 0:
                                                # –í –ë–î –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ - –Ω–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –ø—É—Å—Ç—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
                                                continue
                                        
                                        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ existing_val –≤—Å–µ –µ—â–µ Series - –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
                                        if isinstance(existing_val, pd.Series):
                                            existing_val = existing_val.iloc[0] if len(existing_val) > 0 else None
                                        elif isinstance(existing_val, (list, tuple, np.ndarray)):
                                            # –î–ª—è —Å–ø–∏—Å–∫–æ–≤ –±–µ—Ä–µ–º –≤–µ—Å—å —Å–ø–∏—Å–æ–∫, –∞ –Ω–µ –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç
                                            if field in ('serp_top_urls', 'serp_lsi_phrases'):
                                                # –î–ª—è SERP –ø–æ–ª–µ–π —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å—å —Å–ø–∏—Å–æ–∫
                                                df_copy.at[idx, field] = existing_val
                                                continue
                                            else:
                                                existing_val = existing_val[0] if len(existing_val) > 0 else None
                                        
                                        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —ç—Ç–æ —Å–∫–∞–ª—è—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–µ—Ä–µ–¥ —É—Å—Ç–∞–Ω–æ–≤–∫–æ–π
                                        if not isinstance(existing_val, (pd.Series, list, tuple, np.ndarray)):
                                            df_copy.at[idx, field] = existing_val
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã
            if csv_path and csv_hash:
                cursor.execute('''
                    INSERT OR REPLACE INTO query_groups 
                    (group_name, csv_file_path, csv_hash, total_queries, unique_queries, 
                     duplicates_removed, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
                ''', (
                    group_name,
                    str(csv_path),
                    csv_hash,
                    len(df),
                    len(df),
                    0
                ))
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
            queries_data = []
            
            # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π
            def safe_get(row, key, default=None, cast=None):
                """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ–º —Ç–∏–ø–∞"""
                if key not in df_copy.columns:
                    return default
                val = row.get(key)
                if pd.isna(val):
                    return default
                if cast:
                    try:
                        return cast(val)
                    except (ValueError, TypeError):
                        return default
                return val
            
            for _, row in df_copy.iterrows():
                keyword = row.get('keyword')
                
                # SERP TOP URLs –∫–∞–∫ JSON
                serp_top_urls = None
                if 'serp_top_urls' in df_copy.columns:
                    val = row.get('serp_top_urls')
                    
                    # –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –≤ –ë–î –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                    existing_serp_data = None
                    if keyword and keyword in existing_data_by_keyword:
                        existing_row = existing_data_by_keyword[keyword]
                        if 'serp_top_urls' in existing_row.index:
                            existing_serp_data = existing_row.get('serp_top_urls')
                            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ Series –≤ —Å–∫–∞–ª—è—Ä
                            if isinstance(existing_serp_data, pd.Series):
                                existing_serp_data = existing_serp_data.iloc[0] if len(existing_serp_data) > 0 else None
                    
                    # –ï—Å–ª–∏ val —ç—Ç–æ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞, –Ω–æ –≤ –ë–î –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î
                    is_empty = False
                    if val is None or (isinstance(val, float) and pd.isna(val)):
                        is_empty = True
                    elif isinstance(val, list) and len(val) == 0:
                        is_empty = True
                    elif isinstance(val, str) and val.strip() in ('', '[]', 'null', 'NULL', 'None'):
                        is_empty = True
                    
                    if is_empty and existing_serp_data:
                        # –ü—É—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, –Ω–æ –≤ –ë–î –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î
                        if isinstance(existing_serp_data, str) and existing_serp_data.strip() not in ('', '[]', 'null', 'NULL', 'None'):
                            serp_top_urls = existing_serp_data
                        elif isinstance(existing_serp_data, (list, tuple)) and len(existing_serp_data) > 0:
                            serp_top_urls = json.dumps(existing_serp_data, ensure_ascii=False)
                    elif val is not None and not (isinstance(val, float) and pd.isna(val)):
                        # –ï—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ DataFrame - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
                        if isinstance(val, str):
                            serp_top_urls = val
                        elif isinstance(val, list) and len(val) > 0:
                            if isinstance(val[0], dict):
                                serp_top_urls = json.dumps(val, ensure_ascii=False)
                            elif isinstance(val[0], str):
                                normalized_urls = []
                                for i, url in enumerate(val[:20], 1):
                                    normalized_urls.append({
                                        'position': i,
                                        'url': url,
                                        'domain': '',
                                        'title': '',
                                        'is_commercial': False
                                    })
                                serp_top_urls = json.dumps(normalized_urls, ensure_ascii=False)
                            else:
                                serp_top_urls = json.dumps(val, ensure_ascii=False)
                        elif isinstance(val, list) and len(val) == 0:
                            # –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ - –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º (–æ—Å—Ç–∞–≤–ª—è–µ–º NULL –∏–ª–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î)
                            if existing_serp_data:
                                if isinstance(existing_serp_data, str) and existing_serp_data.strip() not in ('', '[]', 'null', 'NULL', 'None'):
                                    serp_top_urls = existing_serp_data
                                elif isinstance(existing_serp_data, (list, tuple)) and len(existing_serp_data) > 0:
                                    serp_top_urls = json.dumps(existing_serp_data, ensure_ascii=False)
                            # –ò–Ω–∞—á–µ –æ—Å—Ç–∞–≤–ª—è–µ–º None (–Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫)
                        else:
                            serp_top_urls = json.dumps(val, ensure_ascii=False) if val else None
                elif 'serp_urls' in df_copy.columns:
                    val = row.get('serp_urls')
                    if val is not None and not (isinstance(val, float) and pd.isna(val)):
                        if isinstance(val, str):
                            try:
                                parsed = json.loads(val)
                                if isinstance(parsed, list) and len(parsed) > 0:
                                    if isinstance(parsed[0], str):
                                        normalized_urls = []
                                        for i, url in enumerate(parsed[:20], 1):
                                            normalized_urls.append({
                                                'position': i,
                                                'url': url,
                                                'domain': '',
                                                'title': '',
                                                'is_commercial': False
                                            })
                                        serp_top_urls = json.dumps(normalized_urls, ensure_ascii=False)
                                    else:
                                        serp_top_urls = val
                                else:
                                    serp_top_urls = val
                            except:
                                serp_top_urls = val
                        elif isinstance(val, list):
                            normalized_urls = []
                            for i, url in enumerate(val[:20], 1):
                                if isinstance(url, str):
                                    normalized_urls.append({
                                        'position': i,
                                        'url': url,
                                        'domain': '',
                                        'title': '',
                                        'is_commercial': False
                                    })
                                elif isinstance(url, dict):
                                    normalized_urls.append(url)
                            serp_top_urls = json.dumps(normalized_urls, ensure_ascii=False) if normalized_urls else None
                        else:
                            serp_top_urls = json.dumps(val, ensure_ascii=False) if val else None
                
                # LSI phrases –∫–∞–∫ JSON
                serp_lsi_phrases = None
                if 'serp_lsi_phrases' in df_copy.columns:
                    val = row.get('serp_lsi_phrases')
                    if val is not None and not (isinstance(val, float) and pd.isna(val)):
                        serp_lsi_phrases = val if isinstance(val, str) else json.dumps(val)
                elif 'lsi_phrases' in df_copy.columns:
                    val = row.get('lsi_phrases')
                    if val is not None and not (isinstance(val, float) and pd.isna(val)):
                        serp_lsi_phrases = val if isinstance(val, str) else json.dumps(val)
                
                queries_data.append((
                    group_name,
                    safe_get(row, 'keyword', ''),
                    safe_get(row, 'frequency_world', 0, int),
                    safe_get(row, 'frequency_exact', 0, int),
                    safe_get(row, 'normalized'),
                    safe_get(row, 'lemmatized'),
                    safe_get(row, 'words_count', 0, int),
                    safe_get(row, 'main_words'),
                    safe_get(row, 'key_phrase'),
                    safe_get(row, 'ner_entities'),
                    safe_get(row, 'ner_locations'),
                    safe_get(row, 'has_geo', False, bool),
                    safe_get(row, 'geo_type'),
                    safe_get(row, 'geo_country'),
                    safe_get(row, 'geo_city'),
                    safe_get(row, 'main_intent'),
                    safe_get(row, 'commercial_score', 0.0, float),
                    safe_get(row, 'informational_score', 0.0, float),
                    safe_get(row, 'navigational_score', 0.0, float),
                    safe_get(row, 'is_commercial', False, bool),
                    safe_get(row, 'is_wholesale', False, bool),
                    safe_get(row, 'is_urgent', False, bool),
                    safe_get(row, 'is_diy', False, bool),
                    safe_get(row, 'is_review', False, bool),
                    safe_get(row, 'is_brand_query', False, bool),
                    safe_get(row, 'serp_query_hash'),
                    safe_get(row, 'serp_req_id'),
                    safe_get(row, 'serp_status') or 'completed',
                    safe_get(row, 'serp_error_message'),
                    safe_get(row, 'serp_found_docs', None, int),
                    safe_get(row, 'serp_main_pages_count', None, int),
                    safe_get(row, 'serp_titles_with_keyword', None, int),
                    safe_get(row, 'serp_commercial_domains', None, int),
                    safe_get(row, 'serp_info_domains', None, int),
                    safe_get(row, 'serp_created_at'),
                    safe_get(row, 'serp_updated_at'),
                    safe_get(row, 'serp_intent'),
                    safe_get(row, 'serp_confidence', 0.0, float),
                    safe_get(row, 'serp_docs_with_offers', 0, int),
                    safe_get(row, 'serp_total_docs', 0, int),
                    safe_get(row, 'serp_offer_ratio', 0.0, float),
                    safe_get(row, 'serp_avg_price', None, float),
                    safe_get(row, 'serp_min_price', None, float),
                    safe_get(row, 'serp_max_price', None, float),
                    safe_get(row, 'serp_median_price', None, float),
                    safe_get(row, 'serp_currency', 'RUR'),
                    safe_get(row, 'serp_offers_count', 0, int),
                    safe_get(row, 'serp_offers_with_discount', 0, int),
                    safe_get(row, 'serp_avg_discount_percent', None, float),
                    serp_top_urls,
                    serp_lsi_phrases,
                    safe_get(row, 'direct_shows', None, int),
                    safe_get(row, 'direct_clicks', None, int),
                    safe_get(row, 'direct_ctr', None, float),
                    safe_get(row, 'direct_min_cpc', None, float),
                    safe_get(row, 'direct_avg_cpc', None, float),
                    safe_get(row, 'direct_max_cpc', None, float),
                    safe_get(row, 'direct_recommended_cpc', None, float),
                    safe_get(row, 'direct_competition_level'),
                    safe_get(row, 'direct_first_place_bid', None, float),
                    safe_get(row, 'direct_first_place_price', None, float),
                    safe_get(row, 'kei', 0.0, float),
                    safe_get(row, 'difficulty', 0.0, float),
                    safe_get(row, 'competition_score', 0.0, float),
                    safe_get(row, 'potential_traffic', 0.0, float),
                    safe_get(row, 'expected_ctr', 0.0, float),
                    safe_get(row, 'detected_brand'),
                    safe_get(row, 'brand_confidence', 0.0, float),
                    safe_get(row, 'funnel_stage'),
                    safe_get(row, 'funnel_priority', 5, int),
                ))
            
            # Bulk insert –∏–ª–∏ replace
            batch_size = 100
            total_batches = (len(queries_data) + batch_size - 1) // batch_size
            
            insert_query = '''
                INSERT OR REPLACE INTO master_queries 
                (group_name, keyword, frequency_world, frequency_exact,
                 normalized, lemmatized, words_count, main_words, key_phrase,
                 ner_entities, ner_locations,
                 has_geo, geo_type, geo_country, geo_city,
                 main_intent, commercial_score, informational_score, navigational_score,
                 is_commercial, is_wholesale, is_urgent, is_diy, is_review, is_brand_query,
                 serp_query_hash, serp_req_id, serp_status, serp_error_message,
                 serp_found_docs, serp_main_pages_count, serp_titles_with_keyword,
                 serp_commercial_domains, serp_info_domains, serp_created_at, serp_updated_at,
                 serp_intent, serp_confidence, serp_docs_with_offers, serp_total_docs, serp_offer_ratio,
                 serp_avg_price, serp_min_price, serp_max_price, serp_median_price, serp_currency,
                 serp_offers_count, serp_offers_with_discount, serp_avg_discount_percent,
                 serp_top_urls, serp_lsi_phrases,
                 direct_shows, direct_clicks, direct_ctr, direct_min_cpc, direct_avg_cpc,
                 direct_max_cpc, direct_recommended_cpc, direct_competition_level,
                 direct_first_place_bid, direct_first_place_price,
                 kei, difficulty, competition_score, potential_traffic, expected_ctr,
                 detected_brand, brand_confidence,
                 funnel_stage, funnel_priority)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,
                        ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,
                        ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            '''
            
            saved_count = 0
            for i in range(0, len(queries_data), batch_size):
                batch = queries_data[i:i + batch_size]
                batch_num = i // batch_size + 1
                
                try:
                    cursor.executemany(insert_query, batch)
                    saved_count += len(batch)
                    
                    if batch_num % 10 == 0 or batch_num == total_batches:
                        print(f"  üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count}/{len(queries_data)} –∑–∞–ø—Ä–æ—Å–æ–≤...")
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –±–∞—Ç—á–∞ {batch_num}: {e}")
                    raise
            
            conn.commit()
            
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–∞—Å—Ç–æ—Ç
            if 'frequency_world' in df_copy.columns:
                non_zero_freq_world = (df_copy['frequency_world'] > 0).sum()
                total_rows = len(df_copy)
                print(f"  üìä –ß–∞—Å—Ç–æ—Ç—ã –≤ —Å–æ—Ö—Ä–∞–Ω—è–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {non_zero_freq_world} –∏–∑ {total_rows} —Å –Ω–µ–Ω—É–ª–µ–≤–æ–π —á–∞—Å—Ç–æ—Ç–æ–π (–º–∏—Ä)")
                if frequencies_from_csv_count > 0:
                    print(f"  ‚ÑπÔ∏è  –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {frequencies_from_csv_count} —á–∞—Å—Ç–æ—Ç –∏–∑ CSV —Ñ–∞–π–ª–∞")
                if frequencies_restored_from_db_count > 0:
                    print(f"  ‚ÑπÔ∏è  –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {frequencies_restored_from_db_count} —á–∞—Å—Ç–æ—Ç –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ë–î")
                if non_zero_freq_world == 0 and total_rows > 0:
                    print(f"  ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –í—Å–µ —á–∞—Å—Ç–æ—Ç—ã —Ä–∞–≤–Ω—ã –Ω—É–ª—é –≤ —Å–æ—Ö—Ä–∞–Ω—è–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
                    print(f"  ‚ÑπÔ∏è  –ü—Ä–æ–≤–µ—Ä—å—Ç–µ CSV —Ñ–∞–π–ª –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–æ–∫ frequency_world –∏ frequency_exact")
            
        finally:
            conn.close()

