"""–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ª–æ–≤–∞—Ä–µ–π –∏ –¥–∞–Ω–Ω—ã—Ö"""

import asyncio
from pathlib import Path
from typing import Dict, List, Set
import aiofiles
import pandas as pd
from .config import (
    KEYWORD_GROUP_DIR,
    KEYWORDS_STOP_DIR,
    KEYWORD_DICTIONARIES,
    GEO_DICTIONARIES,
)


async def load_text_file_async(file_path: Path) -> Set[str]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
    
    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        
    Returns:
        –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∏–∑ —Ñ–∞–π–ª–∞ (lowercase, stripped, without BOM)
    """
    try:
        async with aiofiles.open(file_path, 'r', encoding='utf-8-sig') as f:  # utf-8-sig –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–¥–∞–ª—è–µ—Ç BOM
            content = await f.read()
            return {line.strip().lower() for line in content.split('\n') if line.strip()}
    except FileNotFoundError:
        print(f"‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return set()
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
        return set()


def load_text_file_sync(file_path: Path) -> Set[str]:
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞
    
    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        
    Returns:
        –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∏–∑ —Ñ–∞–π–ª–∞ (–±–µ–∑ BOM)
    """
    try:
        with open(file_path, 'r', encoding='utf-8-sig') as f:  # utf-8-sig –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–¥–∞–ª—è–µ—Ç BOM
            return {line.strip().lower() for line in f if line.strip()}
    except FileNotFoundError:
        print(f"‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return set()
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
        return set()


async def load_intent_weights() -> Dict[str, float]:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –∏–Ω—Ç–µ–Ω—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞ intent_weights.txt
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å {intent_type: weight}
    """
    weights_file = KEYWORD_GROUP_DIR / "intent_weights.txt"
    weights = {}
    
    if not weights_file.exists():
        # –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return {
            "commercial": 3.0,
            "transactional": 3.0,
            "informational": 4.0,
            "navigational": 4.0,
        }
    
    try:
        async with aiofiles.open(weights_file, 'r', encoding='utf-8') as f:
            content = await f.read()
            
        for line in content.splitlines():
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            
            if ':' in line:
                intent_type, weight_str = line.split(':', 1)
                try:
                    weights[intent_type.strip()] = float(weight_str.strip())
                except ValueError:
                    continue
        
        return weights
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤ –∏–Ω—Ç–µ–Ω—Ç–æ–≤: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return {
            "commercial": 3.0,
            "transactional": 3.0,
            "informational": 4.0,
            "navigational": 4.0,
        }


async def load_all_keyword_dictionaries() -> Dict[str, Dict[str, any]]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Å–ª–æ–≤–∞—Ä–µ–π –∏–∑ keyword_group
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    """
    result = {}
    tasks = []
    
    for dict_name, dict_info in KEYWORD_DICTIONARIES.items():
        file_path = KEYWORD_GROUP_DIR / dict_info["file"]
        tasks.append((dict_name, dict_info, load_text_file_async(file_path)))
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    for dict_name, dict_info, task in tasks:
        words = await task
        result[dict_name] = {
            "words": words,
            "weight": dict_info["weight"],
            "flag": dict_info["flag"],
        }
    
    return result


async def load_all_geo_dictionaries() -> Dict[str, Set[str]]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤–∞—Ä–µ–π
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –≥–µ–æ-—Å–ª–æ–≤–∞—Ä—è–º–∏
    """
    result = {}
    tasks = []
    
    for geo_name, filename in GEO_DICTIONARIES.items():
        file_path = KEYWORD_GROUP_DIR / filename
        tasks.append((geo_name, load_text_file_async(file_path)))
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    for geo_name, task in tasks:
        result[geo_name] = await task
    
    return result


async def load_stopwords_async() -> Set[str]:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤ (Adult –∫–æ–Ω—Ç–µ–Ω—Ç)
    
    Returns:
        –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Å—Ç–æ–ø-—Å–ª–æ–≤
    """
    stopwords_file = KEYWORDS_STOP_DIR / "Adult_hard.txt"
    return await load_text_file_async(stopwords_file)


def load_csv_data(file_path: Path) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ CSV —Ñ–∞–π–ª–∞ —Å —Å–µ–º–∞–Ω—Ç–∏–∫–æ–π
    
    Args:
        file_path: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É
        
    Returns:
        DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
    """
    try:
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
        for encoding in ['utf-8', 'utf-8-sig', 'cp1251', 'windows-1251']:
            for sep in [';', ',', '\t']:
                try:
                    df = pd.read_csv(file_path, encoding=encoding, sep=sep)
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–∞–≥—Ä—É–∑–∏–ª–æ—Å—å –∞–¥–µ–∫–≤–∞—Ç–Ω–æ (–±–æ–ª—å—à–µ 1 –∫–æ–ª–æ–Ω–∫–∏)
                    if len(df.columns) > 1:
                        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ {file_path.name} ({encoding}, sep='{sep}')")
                        return df
                except (UnicodeDecodeError, pd.errors.ParserError):
                    continue
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–¥–æ—à–ª–æ
        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ {file_path}")
        
    except FileNotFoundError:
        print(f"‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return pd.DataFrame()
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV {file_path}: {e}")
        return pd.DataFrame()


def detect_csv_columns(df: pd.DataFrame) -> Dict[str, str]:
    """
    –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –≤ CSV
    
    Args:
        df: DataFrame
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∏–º–µ–Ω–∞–º–∏ –∫–æ–ª–æ–Ω–æ–∫
    """
    columns = {}
    
    # –í–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–∑–≤–∞–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –∫–æ–ª–æ–Ω–æ–∫
    keyword_variants = ['keyword', '–∑–∞–ø—Ä–æ—Å', 'query', '–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ', '—Ñ—Ä–∞–∑–∞', '–∫–ª—é—á–µ–≤–æ–µ—Å–ª–æ–≤–æ']
    freq_world_variants = ['frequency_world', '—á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å', 'freq', '–ø–æ–∫–∞–∑—ã', 'impressions', 
                           'max —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å –≤–µ—Å—å –º–∏—Ä', 'max—á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å–≤–µ—Å—å–º–∏—Ä', '–≤–µ—Å—å–º–∏—Ä']
    freq_exact_variants = ['frequency_exact', '—Ç–æ—á–Ω–∞—è —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å', 'exact', '"!"', 
                          '!max —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å !–≤–µ—Å—å !–º–∏—Ä', '"!max—á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å!–≤–µ—Å—å!–º–∏—Ä"',
                          '!max—á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å!–≤–µ—Å—å!–º–∏—Ä',  # –ë–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∫–∞–≤—ã—á–µ–∫
                          '!–≤–µ—Å—å!–º–∏—Ä', 'max—á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å!–≤–µ—Å—å!–º–∏—Ä']
    
    df_columns_lower = {col.lower().replace(' ', '').replace('"', ''): col for col in df.columns}
    
    # –ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–∫–∏ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
    for variant in keyword_variants:
        variant_clean = variant.replace(' ', '').replace('"', '')
        if variant_clean in df_columns_lower:
            columns['keyword'] = df_columns_lower[variant_clean]
            break
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é, –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Ç–µ–∫—Å—Ç–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É
    if 'keyword' not in columns:
        for col in df.columns:
            if df[col].dtype == 'object':
                columns['keyword'] = col
                break
    
    # –ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–∫–∏ —Å —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å—é
    for variant in freq_world_variants:
        variant_clean = variant.replace(' ', '').replace('"', '')
        if variant_clean in df_columns_lower:
            columns['frequency_world'] = df_columns_lower[variant_clean]
            break
    
    # –¢–æ—á–Ω–∞—è —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å
    # –ù–ï —É–¥–∞–ª—è–µ–º ! –ø—Ä–∏ –ø–æ–∏—Å–∫–µ, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ —á–∞—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–∫–∏
    for variant in freq_exact_variants:
        variant_clean = variant.replace(' ', '').replace('"', '')
        if variant_clean in df_columns_lower:
            columns['frequency_exact'] = df_columns_lower[variant_clean]
            break
    
    return columns


def normalize_dataframe_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–æ–Ω–æ–∫ DataFrame
    
    Args:
        df: –ò—Å—Ö–æ–¥–Ω—ã–π DataFrame
        
    Returns:
        DataFrame —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏
    """
    detected_cols = detect_csv_columns(df)
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π DataFrame —Å –Ω—É–∂–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏
    result_df = pd.DataFrame()
    
    if 'keyword' in detected_cols:
        result_df['keyword'] = df[detected_cols['keyword']].astype(str)
    
    if 'frequency_world' in detected_cols:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã: —É–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –¥—Ä—É–≥–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Ç—ã—Å—è—á –∏–∑ —á–∏—Å–µ–ª
        # –ù–∞–ø—Ä–∏–º–µ—Ä: "3 000" -> "3000", "3,000" -> "3000", "3.000" -> "3000"
        # –í–ê–ñ–ù–û: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫—É —á–∞—Å—Ç–æ—Ç, –Ω–µ keyword!
        freq_world_col = df[detected_cols['frequency_world']]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –∏—Å—Ö–æ–¥–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏
        if freq_world_col.dtype == 'object':
            # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∏, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏ —É–¥–∞–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Ç—ã—Å—è—á
            freq_world_series = freq_world_col.astype(str)
            # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã, –∑–∞–ø—è—Ç—ã–µ –∏ —Ç–æ—á–∫–∏ (—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Ç—ã—Å—è—á)
            freq_world_series = freq_world_series.str.replace(r'[\s,\.]', '', regex=True)
        else:
            # –ï—Å–ª–∏ —É–∂–µ —á–∏—Å–ª–æ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
            freq_world_series = freq_world_col
        
        result_df['frequency_world'] = pd.to_numeric(
            freq_world_series, errors='coerce'
        ).fillna(0).astype(int)
    else:
        result_df['frequency_world'] = 0
    
    if 'frequency_exact' in detected_cols:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã: —É–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –¥—Ä—É–≥–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Ç—ã—Å—è—á –∏–∑ —á–∏—Å–µ–ª
        # –ù–∞–ø—Ä–∏–º–µ—Ä: "3 000" -> "3000", "3,000" -> "3000", "3.000" -> "3000"
        # –í–ê–ñ–ù–û: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫—É —á–∞—Å—Ç–æ—Ç, –Ω–µ keyword!
        freq_exact_col = df[detected_cols['frequency_exact']]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –∏—Å—Ö–æ–¥–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏
        if freq_exact_col.dtype == 'object':
            # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∏, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏ —É–¥–∞–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Ç—ã—Å—è—á
            freq_exact_series = freq_exact_col.astype(str)
            # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã, –∑–∞–ø—è—Ç—ã–µ –∏ —Ç–æ—á–∫–∏ (—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Ç—ã—Å—è—á)
            freq_exact_series = freq_exact_series.str.replace(r'[\s,\.]', '', regex=True)
        else:
            # –ï—Å–ª–∏ —É–∂–µ —á–∏—Å–ª–æ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
            freq_exact_series = freq_exact_col
        
        result_df['frequency_exact'] = pd.to_numeric(
            freq_exact_series, errors='coerce'
        ).fillna(0).astype(int)
    else:
        result_df['frequency_exact'] = 0
    
    return result_df


async def load_all_data():
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    
    Returns:
        Tuple —Å –¥–∞–Ω–Ω—ã–º–∏ (keyword_dicts, geo_dicts, stopwords)
    """
    print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä–µ–π...")
    
    keyword_dicts, geo_dicts, stopwords = await asyncio.gather(
        load_all_keyword_dictionaries(),
        load_all_geo_dictionaries(),
        load_stopwords_async(),
    )
    
    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(keyword_dicts)} —Å–ª–æ–≤–∞—Ä–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(geo_dicts)} –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤–∞—Ä–µ–π")
    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(stopwords)} —Å—Ç–æ–ø-—Å–ª–æ–≤")
    
    return keyword_dicts, geo_dicts, stopwords

