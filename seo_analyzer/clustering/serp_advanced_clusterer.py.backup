"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è SOFT-–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å —Ä–µ–∂–∏–º–∞–º–∏ strict/balanced/soft
–û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Numba JIT –∫–æ–º–ø–∏–ª—è—Ü–∏—é –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤ 10-20 —Ä–∞–∑
"""
from typing import List, Dict, Set, Tuple, Optional
import pandas as pd
from collections import defaultdict
import re

from .semantic_checker import SemanticClusterChecker
from .fast_similarity import FastSimilarityCalculator


class AdvancedSERPClusterer:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è SERP –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω–æ—Å—Ç–∏
    
    –†–µ–∂–∏–º—ã:
    - STRICT: –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ö–æ–∂ —Å –ö–ê–ñ–î–´–ú –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
    - BALANCED: –∑–∞–ø—Ä–æ—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ö–æ–∂ –º–∏–Ω–∏–º—É–º —Å 50% –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ  
    - SOFT: –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ö–æ—Ç—è –±—ã —Å –æ–¥–Ω–∏–º (—Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω–æ–µ –∑–∞–º—ã–∫–∞–Ω–∏–µ)
    """
    
    MODE_STRICT = "strict"
    MODE_BALANCED = "balanced"
    MODE_SOFT = "soft"
    
    def __init__(
        self,
        min_common_urls: int = 7,
        top_positions: int = 30,
        max_cluster_size: int = 100,
        mode: str = "balanced",
        semantic_check: bool = True,
        min_cluster_cohesion: float = 0.5,
        geo_dicts: Dict[str, Set[str]] = None
    ):
        """
        Args:
            min_common_urls: –ú–∏–Ω–∏–º—É–º –æ–±—â–∏—Ö URL –¥–ª—è —Å–≤—è–∑–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 7)
            top_positions: –ì–ª—É–±–∏–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞ SERP (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 30)
            max_cluster_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 100)
            mode: –†–µ–∂–∏–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (strict/balanced/soft)
            semantic_check: –ü—Ä–æ–≤–µ—Ä—è—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å—Ö–æ–∂–µ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–æ–≤
            min_cluster_cohesion: –ú–∏–Ω. —Å–≤—è–∑–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∞ (0-1) –¥–ª—è balanced —Ä–µ–∂–∏–º–∞
            geo_dicts: –°–ª–æ–≤–∞—Ä–∏ —Å –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        """
        self.min_common_urls = min_common_urls
        self.top_positions = top_positions
        self.max_cluster_size = max_cluster_size
        self.mode = mode
        self.semantic_check = semantic_check
        self.min_cluster_cohesion = min_cluster_cohesion
        
        self.clusters = {}  # query -> cluster_id
        self.cluster_queries = defaultdict(list)  # cluster_id -> [queries]
        self.cluster_geo_cache = {}  # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: cluster_id -> –≥–µ–æ–≥—Ä–∞—Ñ–∏—è (–∫—ç—à)
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–µ–∫–µ—Ä –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.semantic_checker = SemanticClusterChecker(geo_dicts=geo_dicts) if semantic_check else None
        
        # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ë—ã—Å—Ç—Ä—ã–π –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä —Å—Ö–æ–∂–µ—Å—Ç–∏
        self.fast_similarity = FastSimilarityCalculator(
            top_positions=top_positions
        )
        
        # –ö—ç—à —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        self.similarity_cache = {}  # (query1, query2) -> common_count
    
    def _build_url_index(self, query_urls_dict: Dict[str, List[str]]) -> Dict[str, Set[str]]:
        """
        –°—Ç—Ä–æ–∏—Ç –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å URL ‚Üí –∑–∞–ø—Ä–æ—Å—ã.
        
        –≠—Ç–æ —É—Å–∫–æ—Ä—è–µ—Ç –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Å O(n¬≤) –¥–æ O(n √ó k),
        –≥–¥–µ k = —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL (~30).
        
        Args:
            query_urls_dict: –°–ª–æ–≤–∞—Ä—å –∑–∞–ø—Ä–æ—Å ‚Üí —Å–ø–∏—Å–æ–∫ URL
            
        Returns:
            –ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å: URL ‚Üí –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        from collections import defaultdict
        
        url_to_queries = defaultdict(set)
        
        for query, urls in query_urls_dict.items():
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-N –ø–æ–∑–∏—Ü–∏–π
            for url in urls[:self.top_positions]:
                url_to_queries[url].add(query)
        
        return url_to_queries
    
    def _find_similar_queries_fast(
        self,
        query: str,
        query_urls: List[str],
        url_index: Dict[str, Set[str]]
    ) -> Dict[str, int]:
        """
        –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ —Ö—ç—à-–∏–Ω–¥–µ–∫—Å.
        
        –í–º–µ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (O(n)),
        –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å URL ‚Üí –∑–∞–ø—Ä–æ—Å—ã (O(k), –≥–¥–µ k = –∫–æ–ª-–≤–æ URL).
        
        Args:
            query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            query_urls: URL –∑–∞–ø—Ä–æ—Å–∞
            url_index: –ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å URL ‚Üí –∑–∞–ø—Ä–æ—Å—ã
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å: –∑–∞–ø—Ä–æ—Å ‚Üí –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö URL
        """
        from collections import Counter
        
        candidates = Counter()
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ URL –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞—Ö–æ–¥–∏–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã —Å —Ç–∞–∫–∏–º –∂–µ URL
        for url in query_urls[:self.top_positions]:
            for candidate_query in url_index.get(url, set()):
                if candidate_query != query:
                    candidates[candidate_query] += 1
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ—Ö, —É –∫–æ–≥–æ >= min_common_urls
        return {
            q: count for q, count in candidates.items() 
            if count >= self.min_common_urls
        }
    
    
    def _are_semantically_different(self, query1: str, query2: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π SemanticClusterChecker –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–≤–µ—Ä—è–µ—Ç:
        - –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫—É—é –ø—Ä–∏–≤—è–∑–∫—É (–ú–æ—Å–∫–≤–∞ vs –°–ü–±)
        - –¢–∏–ø—ã –ø—Ä–æ–¥—É–∫—Ç–æ–≤ (–∫–∞—Ä—Ç–∞ vs –±—Ä–∞—Å–ª–µ—Ç)
        - –ò–Ω—Ç–µ–Ω—Ç—ã (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π vs –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π)
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å—ã –ù–ï –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
        """
        if not self.semantic_check or not self.semantic_checker:
            return False
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–µ–∫–µ—Ä (—Ç–æ–ª—å–∫–æ –≥–µ–æ–≥—Ä–∞—Ñ–∏—è)
        compatible, reason = self.semantic_checker.are_queries_compatible(
            query1, query2,
            check_geo=True
        )
        
        return not compatible
    
    def _normalize_url(self, url: str) -> str:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç URL –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (—É–±–∏—Ä–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —è–∫–æ—Ä—è, trailing slash)
        
        Args:
            url: –ü–æ–ª–Ω—ã–π URL
            
        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π URL –±–µ–∑ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ www –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        """
        if not url:
            return ""
        
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª
        url = re.sub(r'^https?://', '', url)
        url = re.sub(r'^www\.', '', url)
        
        # –£–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∏ —è–∫–æ—Ä—è
        url = url.split('?')[0].split('#')[0]
        
        # –£–±–∏—Ä–∞–µ–º trailing slash
        url = url.rstrip('/')
        
        return url.lower()
    
    def extract_serp_urls(self, serp_data) -> List[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç URL –∏–∑ SERP –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            serp_data: –°–ø–∏—Å–æ–∫ URL –∏–ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å URL —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–º–∏ |
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö URL (–ë–ï–ó –¥–æ–º–µ–Ω–æ–≤ - –ø–æ–ª–Ω—ã–µ –ø—É—Ç–∏!)
        """
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/None/–ø—É—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        if serp_data is None:
            return []
        
        # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ —Å–ø–∏—Å–æ–∫ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if isinstance(serp_data, list):
            urls = []
            for url in serp_data:
                if url:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ
                    normalized = self._normalize_url(str(url).strip())
                    if normalized:
                        urls.append(normalized)
            return urls
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ pandas NaN
        try:
            if pd.isna(serp_data):
                return []
        except (TypeError, ValueError):
            pass
        
        # –ï—Å–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
        if not serp_data:
            return []
        
        # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ - —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ |
        url_list = str(serp_data).split('|')
        urls = []
        
        for url in url_list:
            normalized = self._normalize_url(url.strip())
            if normalized:
                urls.append(normalized)
        
        return urls
    
    def _extract_domain(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–º–µ–Ω –∏–∑ URL"""
        if not url:
            return ""
        
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª
        url = re.sub(r'^https?://', '', url)
        url = re.sub(r'^www\.', '', url)
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –¥–æ–º–µ–Ω
        domain = url.split('/')[0]
        
        return domain.lower()
    
    def calculate_similarity(
        self,
        urls1: List[str],
        urls2: List[str]
    ) -> int:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è —Å–ø–∏—Å–∫–∞–º–∏ URL (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö URL)
        
        üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        
        Args:
            urls1: –ü–µ—Ä–≤—ã–π —Å–ø–∏—Å–æ–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö URL
            urls2: –í—Ç–æ—Ä–æ–π —Å–ø–∏—Å–æ–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö URL
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö URL
        """
        if not urls1 or not urls2:
            return 0
        
        # üöÄ –ë—ã—Å—Ç—Ä–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥
        return self.fast_similarity.calculate_similarity(urls1, urls2)
    
    def _can_add_to_cluster(
        self,
        query: str,
        cluster_queries: List[str],
        query_urls_dict: Dict[str, List[str]],
        query_geo_dict: Dict[str, str] = None,
        debug: bool = False,
        cluster_id: int = None  # –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫—ç—à–∞ –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏
    ) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –º–æ–∂–µ—Ç –ª–∏ –∑–∞–ø—Ä–æ—Å –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∫–ª–∞—Å—Ç–µ—Ä
        
        –í–ê–ñ–ù–û: –¢—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω–æ–µ –∑–∞–º—ã–∫–∞–Ω–∏–µ –û–¢–ö–õ–Æ–ß–ï–ù–û - –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å
        –Ω–∞–ø—Ä—è–º—É—é —Å–≤—è–∑–∞–Ω —Å–æ –í–°–ï–ú–ò –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ (>= min_common_urls –æ–±—â–∏—Ö URL).
        
        –ù–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ä–µ–∂–∏–º–∞ (STRICT/BALANCED/SOFT), —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä—è–º–∞—è —Å–≤—è–∑—å —Å–æ –í–°–ï–ú–ò.
        
        Args:
            query_geo_dict: –°–ª–æ–≤–∞—Ä—å –∑–∞–ø—Ä–æ—Å ‚Üí –≥–µ–æ–≥—Ä–∞—Ñ–∏—è (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
            cluster_id: ID –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫—ç—à–∞ –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏
        """
        if not cluster_queries:
            return True
        
        # üåç –ü–†–û–í–ï–†–ö–ê –ì–ï–û–ì–†–ê–§–ò–ò: –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞
        if query_geo_dict is not None:
            query_geo = query_geo_dict.get(query)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            if cluster_id is not None and cluster_id in self.cluster_geo_cache:
                cluster_geo = self.cluster_geo_cache[cluster_id]
            else:
                # –ï—Å–ª–∏ –∫—ç—à –ø—É—Å—Ç - –±–µ—Ä–µ–º –≥–µ–æ–≥—Ä–∞—Ñ–∏—é –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                cluster_geo = query_geo_dict.get(cluster_queries[0])
                if cluster_id is not None:
                    self.cluster_geo_cache[cluster_id] = cluster_geo
            
            if query_geo != cluster_geo:
                if debug:
                    print(f"      ‚ùå –ì–ï–û–ì–†–ê–§–ò–Ø –ù–ï –°–û–í–ü–ê–î–ê–ï–¢: '{query}' ({query_geo}) vs –∫–ª–∞—Å—Ç–µ—Ä ({cluster_geo})")
                return False
        
        query_urls = query_urls_dict.get(query, [])
        similar_count = 0
        not_similar_count = 0
        
        # –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º –í–°–ï –∑–∞–ø—Ä–æ—Å—ã –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω–æ–≥–æ –∑–∞–º—ã–∫–∞–Ω–∏—è
        # –ö–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞–ø—Ä—è–º—É—é —Å–≤—è–∑–∞–Ω —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∑–∞–ø—Ä–æ—Å–æ–≤
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ (–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)
        queries_to_check = cluster_queries
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ –≤—Å–µ–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞
        for cluster_query in queries_to_check:
            # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ö–æ–∂–µ—Å—Ç–∏
            cache_key = tuple(sorted([query, cluster_query]))
            
            if cache_key in self.similarity_cache:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                common_count = self.similarity_cache[cache_key]
            else:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Ä–∞–∑–Ω–∏—Ü—ã (–ë–´–°–¢–†–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏–¥–µ—Ç –ü–ï–†–í–û–ô)
                if self._are_semantically_different(query, cluster_query):
                    if debug:
                        print(f"      '{query}' ‚Üî '{cluster_query}': –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò –†–ê–ó–ù–´–ï")
                    # –í STRICT —Ä–µ–∂–∏–º–µ –¥–∞–∂–µ –æ–¥–Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ = –æ—Ç–∫–∞–∑
                    if self.mode == self.MODE_STRICT:
                        if debug:
                            print(f"      ‚Üí STRICT: –æ—Ç–∫–∞–∑ –∏–∑-–∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Ä–∞–∑–Ω–∏—Ü—ã")
                        return False
                    not_similar_count += 1
                    continue
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å (–ë–´–°–¢–†–û!)
                cluster_query_urls = query_urls_dict.get(cluster_query, [])
                common_count = self.calculate_similarity(
                    query_urls,
                    cluster_query_urls
                )
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                self.similarity_cache[cache_key] = common_count
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö URL
            threshold = self.min_common_urls
            
            if debug:
                print(f"      '{query}' ‚Üî '{cluster_query}': {common_count} –æ–±—â–∏—Ö, –ø–æ—Ä–æ–≥={threshold}")
            
            if common_count >= threshold:
                similar_count += 1
                if debug:
                    print(f"      ‚Üí –°–•–û–ñ–ò ‚úÖ (similar_count={similar_count})")
            else:
                not_similar_count += 1
                if debug:
                    print(f"      ‚Üí –ù–ï –°–•–û–ñ–ò ‚ùå")
                # –í STRICT —Ä–µ–∂–∏–º–µ –µ—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –Ω–µ —Å—Ö–æ–∂ - —Å—Ä–∞–∑—É –æ—Ç–∫–∞–∑
                if self.mode == self.MODE_STRICT:
                    if debug:
                        print(f"      ‚Üí STRICT: –æ—Ç–∫–∞–∑ –∏–∑-–∑–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏")
                    return False
        
        # –í–ê–ñ–ù–û: –£–±—Ä–∞–ª–∏ —Ä–∞–Ω–Ω–∏–π –≤—ã—Ö–æ–¥ - —Ç–µ–ø–µ—Ä—å –ø—Ä–æ–≤–µ—Ä—è–µ–º –í–°–ï –∑–∞–ø—Ä–æ—Å—ã –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
        # –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω–æ–µ –∑–∞–º—ã–∫–∞–Ω–∏–µ
        
        # –£–ë–†–ê–ù–û –¢–†–ê–ù–ó–ò–¢–ò–í–ù–û–ï –ó–ê–ú–´–ö–ê–ù–ò–ï: –í—Å–µ–≥–¥–∞ —Ç—Ä–µ–±—É–µ–º –ø—Ä—è–º—É—é —Å–≤—è–∑—å —Å–æ –í–°–ï–ú–ò –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
        # –ù–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ä–µ–∂–∏–º–∞, –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å >= min_common_urls –æ–±—â–∏—Ö URL
        # —Å–æ –í–°–ï–ú–ò –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
        
        total_checked = len(queries_to_check)
        if total_checked == 0:
            return True
        
        # –í–ê–ñ–ù–û: –¢—Ä–µ–±—É–µ–º –ø—Ä—è–º—É—é —Å–≤—è–∑—å —Å–æ –í–°–ï–ú–ò –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–≤—è–∑–∞–Ω —Ç–æ–ª—å–∫–æ —Å —á–∞—Å—Ç—å—é –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ, –æ–Ω –ù–ï –ø–æ–ø–∞–¥–µ—Ç –≤ –∫–ª–∞—Å—Ç–µ—Ä
        # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ —Å–≤—è–∑–∞–Ω—ã –º–µ–∂–¥—É —Å–æ–±–æ–π >= min_common_urls –æ–±—â–∏—Ö URL
        # –¢–†–ê–ù–ó–ò–¢–ò–í–ù–û–ï –ó–ê–ú–´–ö–ê–ù–ò–ï –û–¢–ö–õ–Æ–ß–ï–ù–û
        
        if debug:
            mode_name = {
                self.MODE_STRICT: "STRICT",
                self.MODE_BALANCED: "BALANCED",
                self.MODE_SOFT: "SOFT"
            }.get(self.mode, self.mode)
            print(f"      ‚Üí {mode_name}: {similar_count}/{total_checked} —Å—Ö–æ–∂–∏—Ö, —Ç—Ä–µ–±—É–µ—Ç—Å—è {total_checked} (—Å–≤—è–∑—å —Å–æ –í–°–ï–ú–ò)")
            if similar_count < total_checked:
                print(f"      ‚Üí –û–¢–ö–ê–ó: –∑–∞–ø—Ä–æ—Å —Å–≤—è–∑–∞–Ω —Ç–æ–ª—å–∫–æ —Å {similar_count} –∏–∑ {total_checked} –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ")
                print(f"      ‚Üí –¢—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω–æ–µ –∑–∞–º—ã–∫–∞–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ - —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä—è–º–∞—è —Å–≤—è–∑—å —Å–æ –í–°–ï–ú–ò")
        
        # –ó–∞–ø—Ä–æ—Å –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–≤—è–∑–∞–Ω —Å–æ –í–°–ï–ú–ò –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
        # (–ø—Ä–æ–≤–µ—Ä–∫–∞ >= min_common_urls —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –≤ —Ü–∏–∫–ª–µ –≤—ã—à–µ)
        return similar_count == total_checked
    
    async def cluster_by_serp(
        self,
        df: pd.DataFrame,
        serp_column: str = 'serp_main_pages',
        geo_processor=None
    ) -> pd.DataFrame:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é SOFT-–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ SERP
        
        Args:
            df: DataFrame —Å –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∏ SERP –¥–∞–Ω–Ω—ã–º–∏
            serp_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å SERP –¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏ semantic_cluster_id –∏ cluster_name
        """
        mode_name = {
            self.MODE_STRICT: "STRICT (—Å—Ç—Ä–æ–≥–∏–π)",
            self.MODE_BALANCED: f"BALANCED (—Å–≤—è–∑–Ω–æ—Å—Ç—å {int(self.min_cluster_cohesion*100)}%)",
            self.MODE_SOFT: "SOFT (–º—è–≥–∫–∏–π)"
        }.get(self.mode, self.mode)
        
        print(f"üîÑ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è SERP –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (—Ä–µ–∂–∏–º: {mode_name})...")
        print(f"   –ü–æ—Ä–æ–≥: {self.min_common_urls} –æ–±—â–∏—Ö URL, —Ç–æ–ø-{self.top_positions} –ø–æ–∑–∏—Ü–∏–π")
        print(f"   ‚ö†Ô∏è  –¢—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω–æ–µ –∑–∞–º—ã–∫–∞–Ω–∏–µ –û–¢–ö–õ–Æ–ß–ï–ù–û - —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä—è–º–∞—è —Å–≤—è–∑—å —Å–æ –í–°–ï–ú–ò –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ")
        if self.semantic_check:
            print(f"   –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: –≤–∫–ª—é—á–µ–Ω (—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º –ø—Ä–æ–¥—É–∫—Ç–æ–≤)")
        
        # üöÄ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è—Ö
        print(f"   üöÄ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ + URL –∏–Ω–¥–µ–∫—Å (2-3x –±—ã—Å—Ç—Ä–µ–µ)")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ SERP –¥–∞–Ω–Ω—ã—Ö
        if serp_column not in df.columns:
            print(f"‚ö†Ô∏è  –ö–æ–ª–æ–Ω–∫–∞ '{serp_column}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é.")
            df['semantic_cluster_id'] = -1
            df['cluster_name'] = df['keyword']
            return df
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∏–∑ SERP –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        print("  üìä –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL –∏–∑ SERP...")
        query_urls_dict = {}
        for idx, row in df.iterrows():
            query = row['keyword']
            serp_data = row[serp_column]
            urls = self.extract_serp_urls(serp_data)
            query_urls_dict[query] = urls
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã –±–µ–∑ SERP –¥–∞–Ω–Ω—ã—Ö
        queries_with_serp = [q for q, urls in query_urls_dict.items() if len(urls) > 0]
        print(f"  ‚úì –ó–∞–ø—Ä–æ—Å–æ–≤ —Å SERP –¥–∞–Ω–Ω—ã–º–∏: {len(queries_with_serp)} –∏–∑ {len(df)}")
        
        if len(queries_with_serp) == 0:
            print("‚ö†Ô∏è  –ù–µ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ —Å SERP –¥–∞–Ω–Ω—ã–º–∏. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞.")
            df['semantic_cluster_id'] = -1
            df['cluster_name'] = df['keyword']
            return df
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã –ø–æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        if 'frequency_world' in df.columns:
            queries_sorted = df.sort_values('frequency_world', ascending=False)['keyword'].tolist()
            queries_with_serp = [q for q in queries_sorted if q in queries_with_serp]
        
        # üåç –ì–ï–û–ì–†–ê–§–ò–ß–ï–°–ö–ê–Ø –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é –≥–µ–æ–≥—Ä–∞—Ñ–∏—é –∏–ª–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        query_geo_dict = {}
        geo_groups = {}
        
        if geo_processor is not None:
            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–æ–Ω–æ–≤–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
            print(f"  üåç –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ñ–æ–Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if geo_processor.is_ready():
                print(f"  ‚úì –ì–µ–æ–≥—Ä–∞—Ñ–∏—è —É–∂–µ –≥–æ—Ç–æ–≤–∞ (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)!")
            else:
                print(f"  ‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏...")
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–∂–¥–µ–º –µ—Å–ª–∏ –µ—â–µ –Ω–µ –≥–æ—Ç–æ–≤—ã)
            geo_result = await geo_processor.get_result()
            
            query_geo_dict = geo_result.query_geo_dict
            geo_groups = geo_result.geo_groups
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å—ã —Å SERP –¥–∞–Ω–Ω—ã–º–∏
            geo_groups = {
                geo: [q for q in queries if q in queries_with_serp]
                for geo, queries in geo_groups.items()
            }
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            geo_stats = geo_result.geo_stats
            print(f"  ‚úì –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ–ª—É—á–µ–Ω—ã –∏–∑ –∫—ç—à–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        else:
            # FALLBACK: –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–µ–æ–≥—Ä–∞—Ñ–∏—é —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ (—Å—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–±)
            print(f"  üåç –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ Natasha NER...")
            
            if self.semantic_checker:
                for query in queries_with_serp:
                    geo = self.semantic_checker.extract_geo(query)
                    query_geo_dict[query] = geo  # None –µ—Å–ª–∏ –Ω–µ—Ç –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã –ø–æ –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏
            geo_groups = {'__no_geo__': []}  # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –≥—Ä—É–ø–ø–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏
            for query in queries_with_serp:
                geo = query_geo_dict.get(query)
                if geo:
                    if geo not in geo_groups:
                        geo_groups[geo] = []
                    geo_groups[geo].append(query)
                else:
                    geo_groups['__no_geo__'].append(query)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏
        geo_count = len([g for g in geo_groups.keys() if g != '__no_geo__'])
        if geo_count > 0:
            print(f"  ‚úì –ù–∞–π–¥–µ–Ω–æ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {geo_count}")
            for geo, queries in sorted(geo_groups.items(), key=lambda x: len(x[1]), reverse=True)[:5]:
                if geo != '__no_geo__':
                    print(f"    - {geo}: {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤")
            if len(geo_groups.get('__no_geo__', [])) > 0:
                print(f"    - –±–µ–∑ –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏: {len(geo_groups['__no_geo__'])} –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –°—Ç—Ä–æ–∏–º –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å URL ‚Üí –∑–∞–ø—Ä–æ—Å—ã
        print(f"  üîó –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ URL –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞...")
        url_index = self._build_url_index(query_urls_dict)
        print(f"  ‚úì –ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω: {len(url_index)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL")
        
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞ (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø + –ì–ï–û-–°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø)
        print(f"  üîó –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ({self.mode} —Ä–µ–∂–∏–º, FAST, —Å –≥–µ–æ-—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π)...")
        
        # –û–¢–õ–ê–î–ö–ê: –≤–∫–ª—é—á–∏—Ç—å –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏  
        debug_mode = False
        
        cluster_id = 0
        processed = set()
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        total_queries = len(queries_with_serp)
        progress_step = max(1, total_queries // 20)  # 5% —à–∞–≥
        
        # üåç –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø –ü–û –ì–ï–û–ì–†–ê–§–ò–ß–ï–°–ö–ò–ú –°–ï–ì–ú–ï–ù–¢–ê–ú
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π —Å–µ–≥–º–µ–Ω—Ç –æ—Ç–¥–µ–ª—å–Ω–æ
        processed_geo_segments = 0
        
        for geo_segment, geo_queries in geo_groups.items():
            if len(geo_queries) == 0:
                continue
            
            if geo_segment != '__no_geo__' and processed_geo_segments < 5:
                print(f"    üìç –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞: {geo_segment} ({len(geo_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤)")
            processed_geo_segments += 1
        
            for i, query in enumerate(geo_queries, 1):
                if query in processed:
                    continue
                
                query_urls = query_urls_dict[query]
                
                # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
                # –ù–û: —Ç–æ–ª—å–∫–æ —Å—Ä–µ–¥–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –¢–û–ì–û –ñ–ï –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞!
                similar_queries = self._find_similar_queries_fast(query, query_urls, url_index)
                
                # üåç –§–ò–õ–¨–¢–†: –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏–∑ —Ç–æ–≥–æ –∂–µ –≥–µ–æ-—Å–µ–≥–º–µ–Ω—Ç–∞
                query_geo = query_geo_dict.get(query)
                similar_queries_same_geo = {
                    sq: score for sq, score in similar_queries.items()
                    if query_geo_dict.get(sq) == query_geo  # –¢–∞ –∂–µ –≥–µ–æ–≥—Ä–∞—Ñ–∏—è!
                }
                
                # –ù–∞—Ö–æ–¥–∏–º –∫–ª–∞—Å—Ç–µ—Ä—ã —Å –ø–æ—Ö–æ–∂–∏–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Ç–æ–ª—å–∫–æ –∏–∑ —Ç–æ–≥–æ –∂–µ –≥–µ–æ-—Å–µ–≥–º–µ–Ω—Ç–∞)
                candidate_clusters = set()
                for similar_query in similar_queries_same_geo.keys():
                    if similar_query in self.clusters:
                        cluster_id_candidate = self.clusters[similar_query]
                        
                        # üîí –ü–†–û–í–ï–†–ö–ê: –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –¢–£ –ñ–ï –≥–µ–æ–≥—Ä–∞—Ñ–∏—é
                        cluster_geo_match = True
                        for existing_query in self.cluster_queries[cluster_id_candidate]:
                            if query_geo_dict.get(existing_query) != query_geo:
                                cluster_geo_match = False
                                break
                        
                        if cluster_geo_match:
                            candidate_clusters.add(cluster_id_candidate)
                
                # –ü—ã—Ç–∞–µ–º—Å—è –¥–æ–±–∞–≤–∏—Ç—å –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–ª–∞—Å—Ç–µ—Ä (—Ç–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã!)
                added = False
                
                for cid in candidate_clusters:
                    cluster_queries = self.cluster_queries[cid]
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
                    if len(cluster_queries) >= self.max_cluster_size:
                        continue
                    
                    if debug_mode:
                        print(f"  DEBUG: –ü—Ä–æ–≤–µ—Ä–∫–∞ '{query}' ‚Üí –ö–ª–∞—Å—Ç–µ—Ä {cid} {cluster_queries}:")
                    
                    can_add = self._can_add_to_cluster(
                        query, cluster_queries, query_urls_dict, query_geo_dict, 
                        debug=debug_mode, cluster_id=cid
                    )
                    
                    if debug_mode:
                        print(f"  DEBUG: –†–µ–∑—É–ª—å—Ç–∞—Ç: {'‚úÖ –î–ê' if can_add else '‚ùå –ù–ï–¢'}")
                    
                    if can_add:
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–ª–∞—Å—Ç–µ—Ä
                        self.clusters[query] = cid
                        self.cluster_queries[cid].append(query)
                        processed.add(query)
                        added = True
                        if debug_mode:
                            print(f"  DEBUG: ‚úÖ '{query}' –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∫–ª–∞—Å—Ç–µ—Ä {cid}")
                        break
                
                if not added:
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä
                    if debug_mode:
                        print(f"  DEBUG: üÜï '{query}' —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä {cluster_id}")
                    self.clusters[query] = cluster_id
                    self.cluster_queries[cluster_id].append(query)
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–µ–æ–≥—Ä–∞—Ñ–∏—é –Ω–æ–≤–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –≤ –∫—ç—à
                    if query_geo_dict is not None:
                        self.cluster_geo_cache[cluster_id] = query_geo_dict.get(query)
                    processed.add(query)
                    cluster_id += 1
            
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å (–æ–±—â–∏–π –ø–æ –≤—Å–µ–º —Å–µ–≥–º–µ–Ω—Ç–∞–º)
                if len(processed) % progress_step == 0 or len(processed) == total_queries:
                    percent = (len(processed) / total_queries) * 100
                    print(f"      –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(processed)}/{total_queries} ({percent:.0f}%)")
        
        # –ó–∞–ø—Ä–æ—Å—ã –±–µ–∑ SERP ‚Üí –∫–∞–∂–¥—ã–π –ø–æ–ª—É—á–∞–µ—Ç —Å–≤–æ–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
        for idx, row in df.iterrows():
            query = row['keyword']
            if query not in self.clusters:
                self.clusters[query] = cluster_id
                self.cluster_queries[cluster_id].append(query)
                cluster_id += 1
        
        total_clusters = len(self.cluster_queries)
        avg_cluster_size = sum(len(q) for q in self.cluster_queries.values()) / max(total_clusters, 1)
        
        print(f"  ‚úì –°–æ–∑–¥–∞–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {total_clusters}")
        print(f"  ‚úì –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞: {avg_cluster_size:.1f} –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ DataFrame
        df['semantic_cluster_id'] = df['keyword'].map(self.clusters)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫—É—é –º–µ—Ç–∫—É
        df['cluster_geo'] = df['keyword'].map(query_geo_dict)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è –∫–ª–∞—Å—Ç–µ—Ä–∞ (–∑–∞–ø—Ä–æ—Å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å—é)
        cluster_names = {}
        cluster_geos = {}  # –ì–µ–æ–≥—Ä–∞—Ñ–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞
        for cid, queries in self.cluster_queries.items():
            cluster_df = df[df['keyword'].isin(queries)]
            if len(cluster_df) > 0:
                main_query = cluster_df.nlargest(1, 'frequency_world')['keyword'].iloc[0]
                cluster_names[cid] = main_query
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–µ–æ–≥—Ä–∞—Ñ–∏—é –∫–ª–∞—Å—Ç–µ—Ä–∞ (–∏–∑ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞)
                cluster_geo = query_geo_dict.get(queries[0])
                cluster_geos[cid] = cluster_geo
        
        df['cluster_name'] = df['semantic_cluster_id'].map(cluster_names)
        df['cluster_geo_label'] = df['semantic_cluster_id'].map(cluster_geos)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        if geo_count > 0:
            geo_clusters = df[df['cluster_geo_label'].notna()]['semantic_cluster_id'].nunique()
            no_geo_clusters = df[df['cluster_geo_label'].isna()]['semantic_cluster_id'].nunique()
            print(f"  ‚úì –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –≥–µ–æ–≥—Ä–∞—Ñ–∏–µ–π: {geo_clusters}")
            print(f"  ‚úì –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ –±–µ–∑ –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏: {no_geo_clusters}")
        
        return df
    
    def get_cluster_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º"""
        if not self.cluster_queries:
            return {}
        
        cluster_sizes = [len(queries) for queries in self.cluster_queries.values()]
        
        return {
            'total_clusters': len(self.cluster_queries),
            'avg_cluster_size': sum(cluster_sizes) / len(cluster_sizes),
            'max_cluster_size': max(cluster_sizes),
            'min_cluster_size': min(cluster_sizes),
            'singleton_clusters': sum(1 for size in cluster_sizes if size == 1),
        }

