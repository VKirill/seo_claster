"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è SOFT-–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å —Ä–µ–∂–∏–º–∞–º–∏ strict/balanced/soft
–§–∞—Å–∞–¥ –¥–ª—è –º–æ–¥—É–ª–µ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
"""

from typing import List, Dict, Set, Tuple, Optional
import pandas as pd
from collections import defaultdict

from .semantic_checker import SemanticClusterChecker
from .fast_similarity import FastSimilarityCalculator
from .serp_clustering.url_index_builder import URLIndexBuilder
from .serp_clustering.url_normalizer import URLNormalizer
from .serp_clustering.similarity_finder import SimilarityFinder
from .serp_clustering.cluster_validator import ClusterValidator
from .serp_clustering.cluster_processor import ClusterProcessor


class AdvancedSERPClusterer:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è SERP –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω–æ—Å—Ç–∏
    
    –†–µ–∂–∏–º—ã:
    - STRICT: –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ö–æ–∂ —Å –ö–ê–ñ–î–´–ú –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
    - BALANCED: –∑–∞–ø—Ä–æ—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ö–æ–∂ –º–∏–Ω–∏–º—É–º —Å 50% –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ  
    - SOFT: –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ö–æ—Ç—è –±—ã —Å –æ–¥–Ω–∏–º (—Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω–æ–µ –∑–∞–º—ã–∫–∞–Ω–∏–µ)
    """
    
    MODE_STRICT = "strict"
    MODE_BALANCED = "balanced"
    MODE_SOFT = "soft"
    
    def __init__(
        self,
        min_common_urls: int = 7,
        top_positions: int = 30,
        max_cluster_size: int = 100,
        mode: str = "balanced",
        semantic_check: bool = True,
        min_cluster_cohesion: float = 0.5,
        geo_dicts: Dict[str, Set[str]] = None
    ):
        """
        Args:
            min_common_urls: –ú–∏–Ω–∏–º—É–º –æ–±—â–∏—Ö URL –¥–ª—è —Å–≤—è–∑–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 7)
            top_positions: –ì–ª—É–±–∏–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞ SERP (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 30)
            max_cluster_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 100)
            mode: –†–µ–∂–∏–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (strict/balanced/soft)
            semantic_check: –ü—Ä–æ–≤–µ—Ä—è—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å—Ö–æ–∂–µ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–æ–≤
            min_cluster_cohesion: –ú–∏–Ω. —Å–≤—è–∑–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∞ (0-1) –¥–ª—è balanced —Ä–µ–∂–∏–º–∞
            geo_dicts: –°–ª–æ–≤–∞—Ä–∏ —Å –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        """
        self.min_common_urls = min_common_urls
        self.top_positions = top_positions
        self.max_cluster_size = max_cluster_size
        self.mode = mode
        self.semantic_check = semantic_check
        self.min_cluster_cohesion = min_cluster_cohesion
        
        self.clusters = {}  # query -> cluster_id
        self.cluster_queries = defaultdict(list)  # cluster_id -> [queries]
        self.cluster_geo_cache = {}  # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: cluster_id -> –≥–µ–æ–≥—Ä–∞—Ñ–∏—è (–∫—ç—à)
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–µ–∫–µ—Ä –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.semantic_checker = SemanticClusterChecker(geo_dicts=geo_dicts) if semantic_check else None
        
        # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ë—ã—Å—Ç—Ä—ã–π –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä —Å—Ö–æ–∂–µ—Å—Ç–∏
        self.fast_similarity = FastSimilarityCalculator(
            top_positions=top_positions
        )
        
        # –ö—ç—à —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        self.similarity_cache = {}  # (query1, query2) -> common_count
    
    def _build_url_index(self, query_urls_dict: Dict[str, List[str]]) -> Dict[str, Set[str]]:
        """–°—Ç—Ä–æ–∏—Ç –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å URL ‚Üí –∑–∞–ø—Ä–æ—Å—ã"""
        return URLIndexBuilder.build_url_index(query_urls_dict, self.top_positions)
    
    def _find_similar_queries_fast(
        self,
        query: str,
        query_urls: List[str],
        url_index: Dict[str, Set[str]]
    ) -> Dict[str, int]:
        """–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å"""
        return SimilarityFinder.find_similar_queries_fast(
            query, query_urls, url_index, self.fast_similarity, self.top_positions
        )
    
    def _are_semantically_different(self, query1: str, query2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
        if not self.semantic_checker:
            return False
        return self.semantic_checker.are_semantically_different(query1, query2)
    
    def _normalize_url(self, url: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç URL –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        return URLNormalizer.normalize_url(url)
    
    def extract_serp_urls(self, serp_data) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç URL –∏–∑ SERP –¥–∞–Ω–Ω—ã—Ö"""
        return URLNormalizer.extract_serp_urls(serp_data)
    
    def _extract_domain(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–º–µ–Ω –∏–∑ URL"""
        return URLNormalizer.extract_domain(url)
    
    def calculate_similarity(self, urls1: List[str], urls2: List[str]) -> int:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è —Å–ø–∏—Å–∫–∞–º–∏ URL"""
        if not urls1 or not urls2:
            return 0
        return self.fast_similarity.calculate_similarity(urls1, urls2)
    
    def _can_add_to_cluster(
        self,
        query: str,
        cluster_queries: List[str],
        query_urls_dict: Dict[str, List[str]],
        query_geo_dict: Dict[str, str] = None,
        debug: bool = False,
        cluster_id: int = None
    ) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –º–æ–∂–µ—Ç –ª–∏ –∑–∞–ø—Ä–æ—Å –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∫–ª–∞—Å—Ç–µ—Ä"""
        return ClusterValidator.can_add_to_cluster(
            query, cluster_queries, query_urls_dict,
            self.min_common_urls, self.mode, self.semantic_checker,
            query_geo_dict, cluster_id, self.cluster_geo_cache,
            self.similarity_cache, self.fast_similarity, debug
        )
    
    async def cluster_by_serp(
        self,
        df: pd.DataFrame,
        serp_column: str = 'serp_main_pages',
        geo_processor=None
    ) -> pd.DataFrame:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é SOFT-–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ SERP"""
        return await ClusterProcessor.cluster_by_serp(df, self, serp_column, geo_processor)
    
    def get_cluster_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º"""
        if not self.cluster_queries:
            return {
                'total_clusters': 0,
                'total_queries': len(self.clusters),
                'avg_cluster_size': 0.0,
                'max_cluster_size': 0,
                'min_cluster_size': 0,
                'singleton_clusters': 0
            }
        
        cluster_sizes = [len(queries) for queries in self.cluster_queries.values()]
        
        return {
            'total_clusters': len(self.cluster_queries),
            'total_queries': len(self.clusters),
            'avg_cluster_size': sum(cluster_sizes) / len(cluster_sizes) if cluster_sizes else 0.0,
            'max_cluster_size': max(cluster_sizes) if cluster_sizes else 0,
            'min_cluster_size': min(cluster_sizes) if cluster_sizes else 0,
            'singleton_clusters': sum(1 for size in cluster_sizes if size == 1)
        }


__all__ = ['AdvancedSERPClusterer']
