"""
–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è SERP –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É –ø–æ—Ä–æ–≥—É
–°–Ω–∞—á–∞–ª–∞ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –∫–ª–∞—Å—Ç–µ—Ä—ã —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏ (20 URL), –∑–∞—Ç–µ–º –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è –ø–æ—Ä–æ–≥ –¥–æ 4 URL
"""

from typing import List, Dict, Set, Tuple, Optional
import pandas as pd
from collections import defaultdict

from .semantic_checker import SemanticClusterChecker
from .fast_similarity import FastSimilarityCalculator


class IterativeSERPClusterer:
    """
    –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è SERP –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É –ø–æ—Ä–æ–≥—É
    
    –ê–ª–≥–æ—Ä–∏—Ç–º:
    1. –ù–∞—á–∏–Ω–∞–µ–º —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ (20 –æ–±—â–∏—Ö URL)
    2. –ù–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –ø—ã—Ç–∞–µ–º—Å—è –¥–æ–±–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
       –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã —Å —Ç–µ–∫—É—â–∏–º –ø–æ—Ä–æ–≥–æ–º
    3. –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Å–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–æ –º–∏–Ω–∏–º—É–º–∞ (4 –æ–±—â–∏—Ö URL)
    4. –ù–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    
    –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Å–Ω–∞—á–∞–ª–∞ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è —Å–∞–º—ã–µ —Å–∏–ª—å–Ω—ã–µ —Å–≤—è–∑–∏,
    –∞ –∑–∞—Ç–µ–º –∫ –Ω–∏–º –ø—Ä–∏—Å–æ–µ–¥–∏–Ω—è—é—Ç—Å—è –∑–∞–ø—Ä–æ—Å—ã —Å –º–µ–Ω—å—à–∏–º–∏ —Å–≤—è–∑—è–º–∏.
    """
    
    def __init__(
        self,
        min_threshold: int = 4,
        max_threshold: int = 20,
        top_positions: int = 20,
        max_cluster_size: int = 100,
        semantic_check: bool = True,
        geo_dicts: Dict[str, Set[str]] = None,
        verbose: bool = False
    ):
        """
        Args:
            min_threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –æ–±—â–∏—Ö URL (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 4)
            max_threshold: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –æ–±—â–∏—Ö URL (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 20)
            top_positions: –ì–ª—É–±–∏–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞ SERP (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 20)
            max_cluster_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 100)
            semantic_check: –ü—Ä–æ–≤–µ—Ä—è—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å—Ö–æ–∂–µ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–æ–≤
            geo_dicts: –°–ª–æ–≤–∞—Ä–∏ —Å –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            verbose: –í—ã–≤–æ–¥–∏—Ç—å –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        """
        self.min_threshold = min_threshold
        self.max_threshold = max_threshold
        self.top_positions = top_positions
        self.max_cluster_size = max_cluster_size
        self.semantic_check = semantic_check
        self.verbose = verbose
        
        self.clusters = {}  # query -> cluster_id
        self.cluster_queries = defaultdict(list)  # cluster_id -> [queries]
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–µ–∫–µ—Ä –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.semantic_checker = SemanticClusterChecker(geo_dicts=geo_dicts) if semantic_check else None
        
        # –ë—ã—Å—Ç—Ä—ã–π –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä —Å—Ö–æ–∂–µ—Å—Ç–∏
        self.fast_similarity = FastSimilarityCalculator(top_positions=top_positions)
    
    def _normalize_url(self, url: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç URL –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if not url:
            return ""
        url = url.replace("https://", "").replace("http://", "")
        url = url.replace("www.", "")
        url = url.split("?")[0].split("#")[0]
        return url.rstrip("/").lower()
    
    def _extract_urls_from_serp(self, serp_data) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ SERP –¥–∞–Ω–Ω—ã—Ö"""
        if not serp_data:
            return []
        
        urls = []
        if isinstance(serp_data, list):
            for item in serp_data:
                if isinstance(item, dict):
                    url = item.get('url', '') or item.get('link', '')
                elif isinstance(item, str):
                    url = item
                else:
                    continue
                if url:
                    normalized = self._normalize_url(url)
                    if normalized:
                        urls.append(normalized)
        elif isinstance(serp_data, str):
            # –ü–æ–ø—ã—Ç–∫–∞ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON —Å—Ç—Ä–æ–∫—É
            import json
            try:
                data = json.loads(serp_data)
                if isinstance(data, list):
                    return self._extract_urls_from_serp(data)
            except:
                pass
        
        return urls[:self.top_positions]
    
    def _calculate_url_overlap(self, urls1: List[str], urls2: List[str]) -> int:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö URL –º–µ–∂–¥—É –¥–≤—É–º—è —Å–ø–∏—Å–∫–∞–º–∏"""
        set1 = set(urls1[:self.top_positions])
        set2 = set(urls2[:self.top_positions])
        return len(set1 & set2)
    
    def _calculate_url_ids_overlap(self, url_ids1: Set[int], url_ids2: Set[int]) -> int:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö URL –º–µ–∂–¥—É –¥–≤—É–º—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞–º–∏ —á–∏—Å–ª–æ–≤—ã—Ö ID (–±—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è)"""
        return len(url_ids1 & url_ids2)
    
    def _can_add_to_cluster(
        self,
        query: str,
        cluster_queries: List[str],
        query_urls_dict: Dict[str, List[str]],
        threshold: int
    ) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –º–æ–∂–µ—Ç –ª–∏ –∑–∞–ø—Ä–æ—Å –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∫–ª–∞—Å—Ç–µ—Ä
        –ë–ï–ó —Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω–æ–≥–æ –∑–∞–º—ã–∫–∞–Ω–∏—è - —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä—è–º–∞—è —Å–≤—è–∑—å —Å–æ –í–°–ï–ú–ò –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
        
        –í–ê–ñ–ù–û: –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Å–æ —Å–≤—è–∑—å—é >= threshold * 2,
        —Ç–æ –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É –Ω–µ–≥–æ —Ç–æ–∂–µ –µ—Å—Ç—å —Å–≤—è–∑—å >= threshold * 2
        —Å –æ–±–æ–∏–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ (–∑–∞—â–∏—Ç–∞ –æ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–ª–∞–±—ã—Ö —Å–≤—è–∑–µ–π –≤ —Å–∏–ª—å–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã).
        """
        if not cluster_queries:
            return True
        
        query_urls = query_urls_dict.get(query, [])
        if not query_urls:
            return False
        
        strong_bond_threshold = threshold * 2
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏–∑ –¥–≤—É—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —Å–∏–ª—å–Ω–æ–π —Å–≤—è–∑—å—é
        if len(cluster_queries) == 2:
            cluster_query1_urls = query_urls_dict.get(cluster_queries[0], [])
            cluster_query2_urls = query_urls_dict.get(cluster_queries[1], [])
            cluster_bond = self._calculate_url_overlap(cluster_query1_urls, cluster_query2_urls)
            
            # –ï—Å–ª–∏ —Å–≤—è–∑—å –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ –æ—á–µ–Ω—å —Å–∏–ª—å–Ω–∞—è (>= strong_bond_threshold),
            # —Ç–æ –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É –Ω–µ–≥–æ —Ç–æ–∂–µ –æ—á–µ–Ω—å —Å–∏–ª—å–Ω–∞—è —Å–≤—è–∑—å
            if cluster_bond >= strong_bond_threshold:
                overlap1 = self._calculate_url_overlap(query_urls, cluster_query1_urls)
                overlap2 = self._calculate_url_overlap(query_urls, cluster_query2_urls)
                
                # –û–±–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å >= strong_bond_threshold
                if overlap1 < strong_bond_threshold or overlap2 < strong_bond_threshold:
                    return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤—è–∑—å —Å–æ –í–°–ï–ú–ò –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
        for cluster_query in cluster_queries:
            cluster_query_urls = query_urls_dict.get(cluster_query, [])
            overlap = self._calculate_url_overlap(query_urls, cluster_query_urls)
            
            # –ï—Å–ª–∏ —Ö–æ—Ç—è –±—ã —Å –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–±—â–∏—Ö URL - –æ—Ç–∫–∞–∑
            if overlap < threshold:
                return False
            
            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞)
            if self.semantic_checker:
                compatible, reason = self.semantic_checker.are_queries_compatible(
                    query, cluster_query, check_geo=True
                )
                if not compatible:
                    return False
        
        # –í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã - –∑–∞–ø—Ä–æ—Å —Å–≤—è–∑–∞–Ω —Å–æ –í–°–ï–ú–ò –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
        return True
    
    def _can_add_to_cluster_fast(
        self,
        query: str,
        cluster_queries: List[str],
        query_url_ids_dict: Dict[str, Set[int]],
        threshold: int
    ) -> bool:
        """
        –ë—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ –≤ –∫–ª–∞—Å—Ç–µ—Ä (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç —á–∏—Å–ª–æ–≤—ã–µ ID URL)
        
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –º–æ–∂–µ—Ç –ª–∏ –∑–∞–ø—Ä–æ—Å –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∫–ª–∞—Å—Ç–µ—Ä
        –ë–ï–ó —Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω–æ–≥–æ –∑–∞–º—ã–∫–∞–Ω–∏—è - —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä—è–º–∞—è —Å–≤—è–∑—å —Å–æ –í–°–ï–ú–ò –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
        
        –í–ê–ñ–ù–û: –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Å–æ —Å–≤—è–∑—å—é >= threshold * 2,
        —Ç–æ –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É –Ω–µ–≥–æ —Ç–æ–∂–µ –µ—Å—Ç—å —Å–≤—è–∑—å >= threshold * 2
        —Å –æ–±–æ–∏–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ (–∑–∞—â–∏—Ç–∞ –æ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–ª–∞–±—ã—Ö —Å–≤—è–∑–µ–π –≤ —Å–∏–ª—å–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã).
        """
        if not cluster_queries:
            return True
        
        query_url_ids = query_url_ids_dict.get(query)
        if not query_url_ids:
            return False
        
        strong_bond_threshold = threshold * 2
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏–∑ –¥–≤—É—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —Å–∏–ª—å–Ω–æ–π —Å–≤—è–∑—å—é
        if len(cluster_queries) == 2:
            cluster_query1_url_ids = query_url_ids_dict.get(cluster_queries[0])
            cluster_query2_url_ids = query_url_ids_dict.get(cluster_queries[1])
            
            if cluster_query1_url_ids and cluster_query2_url_ids:
                cluster_bond = self._calculate_url_ids_overlap(cluster_query1_url_ids, cluster_query2_url_ids)
                
                # –ï—Å–ª–∏ —Å–≤—è–∑—å –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ –æ—á–µ–Ω—å —Å–∏–ª—å–Ω–∞—è (>= strong_bond_threshold),
                # —Ç–æ –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É –Ω–µ–≥–æ —Ç–æ–∂–µ –æ—á–µ–Ω—å —Å–∏–ª—å–Ω–∞—è —Å–≤—è–∑—å
                if cluster_bond >= strong_bond_threshold:
                    overlap1 = self._calculate_url_ids_overlap(query_url_ids, cluster_query1_url_ids)
                    overlap2 = self._calculate_url_ids_overlap(query_url_ids, cluster_query2_url_ids)
                    
                    # –û–±–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å >= strong_bond_threshold
                    if overlap1 < strong_bond_threshold or overlap2 < strong_bond_threshold:
                        return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤—è–∑—å —Å–æ –í–°–ï–ú–ò –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
        for cluster_query in cluster_queries:
            cluster_query_url_ids = query_url_ids_dict.get(cluster_query)
            if not cluster_query_url_ids:
                return False
            
            overlap = self._calculate_url_ids_overlap(query_url_ids, cluster_query_url_ids)
            
            # –ï—Å–ª–∏ —Ö–æ—Ç—è –±—ã —Å –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–±—â–∏—Ö URL - –æ—Ç–∫–∞–∑
            if overlap < threshold:
                return False
            
            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞)
            if self.semantic_checker:
                compatible, reason = self.semantic_checker.are_queries_compatible(
                    query, cluster_query, check_geo=True
                )
                if not compatible:
                    return False
        
        # –í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã - –∑–∞–ø—Ä–æ—Å —Å–≤—è–∑–∞–Ω —Å–æ –í–°–ï–ú–ò –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
        return True
    
    async def cluster_by_serp(
        self,
        df: pd.DataFrame,
        serp_column: str = 'serp_urls',
        geo_processor=None
    ) -> pd.DataFrame:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—É—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ SERP
        
        Args:
            df: DataFrame —Å –∑–∞–ø—Ä–æ—Å–∞–º–∏
            serp_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å SERP –¥–∞–Ω–Ω—ã–º–∏
            geo_processor: –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏ cluster_id –∏ cluster_name
        """
        if len(df) == 0:
            return df
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è geo_processor –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        if geo_processor is not None:
            await geo_processor.get_result()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        query_urls_dict = {}
        queries = df['keyword'].tolist()
        
        for idx, row in df.iterrows():
            query = row['keyword']
            serp_data = row.get(serp_column)
            urls = self._extract_urls_from_serp(serp_data)
            if urls:
                query_urls_dict[query] = urls
        
        if not query_urls_dict:
            if self.verbose:
                print("‚ö†Ô∏è  –ù–µ—Ç SERP –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
            df['semantic_cluster_id'] = -1
            df['cluster_name'] = df['keyword']
            return df
        
        if self.verbose:
            print(f"üì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ URL –¥–ª—è {len(query_urls_dict)} –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –°–æ–∑–¥–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ ID –¥–ª—è –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL
        # –≠—Ç–æ —É—Å–∫–æ—Ä—è–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤ 5-10 —Ä–∞–∑ (—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ int –≤–º–µ—Å—Ç–æ —Å—Ç—Ä–æ–∫)
        url_to_id = {}  # normalized_url -> int
        url_id_counter = 0
        
        for query, urls in query_urls_dict.items():
            for url in urls[:self.top_positions]:
                if url not in url_to_id:
                    url_to_id[url] = url_id_counter
                    url_id_counter += 1
        
        if self.verbose:
            print(f"üî¢ –°–æ–∑–¥–∞–Ω–æ {len(url_to_id)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL ID")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–∫–∏ URL –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —á–∏—Å–ª–æ–≤—ã—Ö ID –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        query_url_ids_dict = {}  # query -> Set[int]
        for query, urls in query_urls_dict.items():
            url_ids = {url_to_id[url] for url in urls[:self.top_positions] if url in url_to_id}
            query_url_ids_dict[query] = url_ids
        
        # –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        clusters = []
        processed = set()  # –ó–∞–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –ø–æ–ø–∞–ª–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä—ã
        query_to_cluster = {}  # query -> cluster_idx
        
        # –ò—Ç–µ—Ä–∞—Ü–∏–∏ –æ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ –∫ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É
        for threshold in range(self.max_threshold, self.min_threshold - 1, -1):
            if self.verbose:
                unprocessed_count = len(queries) - len(processed)
                if unprocessed_count > 0:
                    print(f"\nüîç –ò—Ç–µ—Ä–∞—Ü–∏—è: –ø–æ—Ä–æ–≥ = {threshold} –æ–±—â–∏—Ö URL (–Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {unprocessed_count})")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            unprocessed_queries = [q for q in queries if q not in processed and q in query_urls_dict]
            
            if not unprocessed_queries:
                if self.verbose:
                    print(f"   ‚úì –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã, –ø–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –ø–æ—Ä–æ–≥—É")
                continue
            
            # –ù–∞—Ö–æ–¥–∏–º –ø–∞—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —Ç–µ–∫—É—â–∏–º –ø–æ—Ä–æ–≥–æ–º –æ–±—â–∏—Ö URL
            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å - –Ω–∞—Ö–æ–¥–∏–º —Ç–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –æ–±—â–∏–º–∏ URL
            # –í–º–µ—Å—Ç–æ O(n¬≤) —Å—Ä–∞–≤–Ω–µ–Ω–∏–π –¥–µ–ª–∞–µ–º O(n √ó k), –≥–¥–µ k - —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL (~20-30)
            
            # –°—Ç—Ä–æ–∏–º –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å: URL ID ‚Üí —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
            url_id_to_queries = defaultdict(set)
            for query in unprocessed_queries:
                url_ids = query_url_ids_dict.get(query)
                if url_ids:
                    for url_id in url_ids:
                        url_id_to_queries[url_id].add(query)
            
            # –ù–∞—Ö–æ–¥–∏–º –ø–∞—Ä—ã —á–µ—Ä–µ–∑ –∏–Ω–¥–µ–∫—Å (—Ç–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã —Å –æ–±—â–∏–º–∏ URL)
            pairs = []
            seen_pairs = set()  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            
            for query1 in unprocessed_queries:
                query1_url_ids = query_url_ids_dict.get(query1)
                if not query1_url_ids:
                    continue
                
                # –ù–∞—Ö–æ–¥–∏–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ - –∑–∞–ø—Ä–æ—Å—ã —Å –æ–±—â–∏–º–∏ URL
                candidate_counts = defaultdict(int)
                for url_id in query1_url_ids:
                    for candidate in url_id_to_queries[url_id]:
                        if candidate != query1 and candidate > query1:  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                            candidate_counts[candidate] += 1
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ–±—â–∏—Ö URL
                for query2, common_urls_count in candidate_counts.items():
                    if common_urls_count < threshold:
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–µ —á–µ–º common_urls_count)
                    query2_url_ids = query_url_ids_dict.get(query2)
                    if not query2_url_ids:
                        continue
                    
                    # –ë–´–°–¢–†–û–ï –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤ —á–∏—Å–µ–ª
                    overlap = self._calculate_url_ids_overlap(query1_url_ids, query2_url_ids)
                    if overlap >= threshold:
                        pair_key = (query1, query2) if query1 < query2 else (query2, query1)
                        if pair_key not in seen_pairs:
                            seen_pairs.add(pair_key)
                            pairs.append((query1, query2, overlap))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—ã –ø–æ —É–±—ã–≤–∞–Ω–∏—é –æ–±—â–∏—Ö URL
            pairs.sort(key=lambda x: x[2], reverse=True)
            
            if self.verbose and pairs:
                print(f"   üìä –ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä —Å >= {threshold} –æ–±—â–∏—Ö URL: {len(pairs)}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞—Ä—ã, –Ω–∞—á–∏–Ω–∞—è —Å —Å–∞–º—ã—Ö —Å–∏–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π
            for query1, query2, overlap in pairs:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –æ–±–∞ –∑–∞–ø—Ä–æ—Å–∞ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
                if query1 in processed and query2 in processed:
                    continue
                
                # –ï—Å–ª–∏ –æ–±–∞ –∑–∞–ø—Ä–æ—Å–∞ –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã - —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä
                if query1 not in processed and query2 not in processed:
                    # –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–µ–æ-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∞
                    # –ì–µ–æ-–∑–∞–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞—Ç—å—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ –æ—Ç –Ω–µ-–≥–µ–æ –∑–∞–ø—Ä–æ—Å–æ–≤
                    if self.semantic_checker:
                        compatible, reason = self.semantic_checker.are_queries_compatible(
                            query1, query2, check_geo=True
                        )
                        if not compatible:
                            # –ì–µ–æ-–Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç—É –ø–∞—Ä—É
                            if self.verbose:
                                print(f"   ‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–∞ –ø–∞—Ä–∞ (–≥–µ–æ-–Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å): '{query1}' + '{query2}' ({reason})")
                            continue
                    
                    new_cluster = [query1, query2]
                    clusters.append(new_cluster)
                    cluster_idx = len(clusters) - 1
                    query_to_cluster[query1] = cluster_idx
                    query_to_cluster[query2] = cluster_idx
                    processed.add(query1)
                    processed.add(query2)
                    
                    if self.verbose:
                        print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω –∫–ª–∞—Å—Ç–µ—Ä {cluster_idx + 1}: '{query1}' + '{query2}' ({overlap} URL)")
                    continue
                
                # –ï—Å–ª–∏ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –∞ –¥—Ä—É–≥–æ–π –Ω–µ—Ç - –ø—ã—Ç–∞–µ–º—Å—è –¥–æ–±–∞–≤–∏—Ç—å –≤ –∫–ª–∞—Å—Ç–µ—Ä
                if query1 in processed and query2 not in processed:
                    cluster_idx = query_to_cluster[query1]
                    cluster = clusters[cluster_idx]
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
                    if len(cluster) >= self.max_cluster_size:
                        continue
                    
                    # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ–º –±—ã—Å—Ç—Ä—É—é –≤–µ—Ä—Å–∏—é —Å —á–∏—Å–ª–æ–≤—ã–º–∏ ID
                    if self._can_add_to_cluster_fast(query2, cluster, query_url_ids_dict, threshold):
                        cluster.append(query2)
                        query_to_cluster[query2] = cluster_idx
                        processed.add(query2)
                        if self.verbose:
                            print(f"   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –≤ –∫–ª–∞—Å—Ç–µ—Ä {cluster_idx + 1}: '{query2}' (—Å–≤—è–∑—å —Å '{query1}': {overlap} URL)")
                
                elif query2 in processed and query1 not in processed:
                    cluster_idx = query_to_cluster[query2]
                    cluster = clusters[cluster_idx]
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
                    if len(cluster) >= self.max_cluster_size:
                        continue
                    
                    # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ–º –±—ã—Å—Ç—Ä—É—é –≤–µ—Ä—Å–∏—é —Å —á–∏—Å–ª–æ–≤—ã–º–∏ ID
                    if self._can_add_to_cluster_fast(query1, cluster, query_url_ids_dict, threshold):
                        cluster.append(query1)
                        query_to_cluster[query1] = cluster_idx
                        processed.add(query1)
                        if self.verbose:
                            print(f"   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –≤ –∫–ª–∞—Å—Ç–µ—Ä {cluster_idx + 1}: '{query1}' (—Å–≤—è–∑—å —Å '{query2}': {overlap} URL)")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∑–∞–ø—Ä–æ—Å—ã –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
        for query in queries:
            if query not in processed and query in query_urls_dict:
                clusters.append([query])
                query_to_cluster[query] = len(clusters) - 1
                processed.add(query)
        
        # –°–æ–∑–¥–∞–µ–º mapping –¥–ª—è DataFrame
        cluster_id_map = {}
        cluster_name_map = {}
        
        for cluster_idx, cluster in enumerate(clusters):
            # –ò–º—è –∫–ª–∞—Å—Ç–µ—Ä–∞ - –ø–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å (–∏–ª–∏ —Å–∞–º—ã–π —á–∞—Å—Ç–æ—Ç–Ω—ã–π)
            cluster_name = cluster[0]
            for query in cluster:
                cluster_id_map[query] = cluster_idx
                cluster_name_map[query] = cluster_name
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ –≤ DataFrame
        df['semantic_cluster_id'] = df['keyword'].map(cluster_id_map).fillna(-1).astype(int)
        df['cluster_name'] = df['keyword'].map(cluster_name_map).fillna(df['keyword'])
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.clusters = cluster_id_map
        self.cluster_queries = {i: cluster for i, cluster in enumerate(clusters)}
        
        if self.verbose:
            print(f"\n‚úì –°–æ–∑–¥–∞–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(clusters)}")
        
        return df
    
    def get_cluster_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        if not self.cluster_queries:
            return {}
        
        cluster_sizes = [len(cluster) for cluster in self.cluster_queries.values()]
        
        return {
            'total_clusters': len(self.cluster_queries),
            'avg_cluster_size': sum(cluster_sizes) / len(cluster_sizes) if cluster_sizes else 0,
            'min_cluster_size': min(cluster_sizes) if cluster_sizes else 0,
            'max_cluster_size': max(cluster_sizes) if cluster_sizes else 0,
            'singleton_clusters': sum(1 for size in cluster_sizes if size == 1)
        }
    
    def get_cluster_url_overlaps(self, df: pd.DataFrame, serp_column: str = 'serp_urls') -> Dict[int, List[Dict]]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö URL –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
        
        Args:
            df: DataFrame —Å –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∏ SERP –¥–∞–Ω–Ω—ã–º–∏
            serp_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å SERP –¥–∞–Ω–Ω—ã–º–∏
        
        Returns:
            Dict[cluster_id -> List[Dict]] –≥–¥–µ –∫–∞–∂–¥—ã–π Dict —Å–æ–¥–µ—Ä–∂–∏—Ç:
            {
                'query1': str,
                'query2': str,
                'overlap': int
            }
        """
        result = {}
        
        for cluster_id, cluster_queries in self.cluster_queries.items():
            if len(cluster_queries) <= 1:
                continue
            
            overlaps = []
            
            # –ü–æ–ª—É—á–∞–µ–º URL –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
            query_urls_dict = {}
            for query in cluster_queries:
                row = df[df['keyword'] == query]
                if not row.empty:
                    serp_data = row.iloc[0].get(serp_column)
                    urls = self._extract_urls_from_serp(serp_data)
                    if urls:
                        query_urls_dict[query] = urls
            
            # –í—ã—á–∏—Å–ª—è–µ–º overlaps –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä –∑–∞–ø—Ä–æ—Å–æ–≤
            queries_list = list(cluster_queries)
            for i, query1 in enumerate(queries_list):
                for query2 in queries_list[i+1:]:
                    if query1 in query_urls_dict and query2 in query_urls_dict:
                        overlap = self._calculate_url_overlap(
                            query_urls_dict[query1],
                            query_urls_dict[query2]
                        )
                        overlaps.append({
                            'query1': query1,
                            'query2': query2,
                            'overlap': overlap
                        })
            
            if overlaps:
                result[cluster_id] = overlaps
        
        return result

