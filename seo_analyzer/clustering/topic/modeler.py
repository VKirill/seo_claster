"""–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å Topic Modeling"""

from typing import Dict, List, Tuple, Optional
import numpy as np
import pandas as pd
from sklearn.decomposition import LatentDirichletAllocation, NMF

from .vectorizer import create_lda_vectorizer, create_nmf_vectorizer
from .analyzer import get_topic_top_words, get_all_topics, get_topic_distribution


class TopicModeler:
    """Topic Modeling —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LDA –∏ NMF"""
    
    def __init__(self, config: Dict = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        """
        self.config = config or {}
        self.vectorizer = None
        self.model = None
        self.feature_names = None
        self.doc_topic_matrix = None
        self.n_topics = None
        self.method = None
    
    def fit_lda(
        self,
        texts: List[str],
        n_topics: Optional[int] = None,
        max_iter: int = 50
    ) -> 'TopicModeler':
        """
        –û–±—É—á–∞–µ—Ç LDA –º–æ–¥–µ–ª—å
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            n_topics: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º (–µ—Å–ª–∏ None, –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
            max_iter: –ú–∞–∫—Å–∏–º—É–º –∏—Ç–µ—Ä–∞—Ü–∏–π
            
        Returns:
            Self
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º
        if n_topics is None:
            topic_config = self.config.get('topic_modeling', {})
            n_topics_range = topic_config.get('n_topics_range', (5, 20))
            n_topics = self._find_optimal_topics(texts, n_topics_range, 'lda')
        
        self.n_topics = n_topics
        self.method = 'lda'
        
        print(f"üîÑ –û–±—É—á–µ–Ω–∏–µ LDA (—Ç–µ–º—ã={n_topics})...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º CountVectorizer –¥–ª—è LDA
        self.vectorizer = create_lda_vectorizer()
        
        doc_term_matrix = self.vectorizer.fit_transform(texts)
        self.feature_names = self.vectorizer.get_feature_names_out()
        
        # –û–±—É—á–∞–µ–º LDA
        self.model = LatentDirichletAllocation(
            n_components=n_topics,
            max_iter=max_iter,
            random_state=42,
            n_jobs=-1
        )
        
        self.doc_topic_matrix = self.model.fit_transform(doc_term_matrix)
        
        print(f"‚úì LDA –æ–±—É—á–µ–Ω–∞: {n_topics} —Ç–µ–º")
        return self
    
    def fit_nmf(
        self,
        texts: List[str],
        n_topics: Optional[int] = None,
        max_iter: int = 200
    ) -> 'TopicModeler':
        """
        –û–±—É—á–∞–µ—Ç NMF –º–æ–¥–µ–ª—å
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            n_topics: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º
            max_iter: –ú–∞–∫—Å–∏–º—É–º –∏—Ç–µ—Ä–∞—Ü–∏–π
            
        Returns:
            Self
        """
        if n_topics is None:
            topic_config = self.config.get('topic_modeling', {})
            n_topics_range = topic_config.get('n_topics_range', (5, 20))
            n_topics = self._find_optimal_topics(texts, n_topics_range, 'nmf')
        
        self.n_topics = n_topics
        self.method = 'nmf'
        
        print(f"üîÑ –û–±—É—á–µ–Ω–∏–µ NMF (—Ç–µ–º—ã={n_topics})...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º TF-IDF –¥–ª—è NMF
        self.vectorizer = create_nmf_vectorizer()
        
        doc_term_matrix = self.vectorizer.fit_transform(texts)
        self.feature_names = self.vectorizer.get_feature_names_out()
        
        # –û–±—É—á–∞–µ–º NMF
        self.model = NMF(
            n_components=n_topics,
            max_iter=max_iter,
            random_state=42,
            init='nndsvda',
            alpha_W=0.01,
            alpha_H=0.01
        )
        
        self.doc_topic_matrix = self.model.fit_transform(doc_term_matrix)
        
        print(f"‚úì NMF –æ–±—É—á–µ–Ω–∞: {n_topics} —Ç–µ–º")
        return self
    
    def _find_optimal_topics(
        self,
        texts: List[str],
        n_range: Tuple[int, int],
        method: str
    ) -> int:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º
        
        Args:
            texts: –¢–µ–∫—Å—Ç—ã
            n_range: –î–∏–∞–ø–∞–∑–æ–Ω —Ç–µ–º (min, max)
            method: –ú–µ—Ç–æ–¥ ('lda' –∏–ª–∏ 'nmf')
            
        Returns:
            –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º
        """
        print(f"üîç –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ —Ç–µ–º –¥–ª—è {method}...")
        
        min_topics, max_topics = n_range
        
        # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –±–µ—Ä–µ–º —Å—Ä–µ–¥–Ω–µ–µ
        # –í –ø–æ–ª–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å perplexity –∏–ª–∏ coherence score
        optimal = (min_topics + max_topics) // 2
        
        print(f"‚úì –í—ã–±—Ä–∞–Ω–æ {optimal} —Ç–µ–º")
        return optimal
    
    def get_topic_top_words(
        self,
        topic_id: int,
        top_n: int = 10
    ) -> List[Tuple[str, float]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ç–µ–º—ã"""
        return get_topic_top_words(self.model, self.feature_names, topic_id, top_n)
    
    def get_all_topics(self, top_n: int = 10) -> Dict[int, Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Å–µ—Ö —Ç–µ–º–∞—Ö"""
        return get_all_topics(self.model, self.feature_names, self.n_topics, top_n)
    
    def assign_topics(
        self,
        texts: List[str],
        threshold: float = 0.3
    ) -> List[Tuple[int, float]]:
        """
        –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç —Ç–µ–º—ã —Ç–µ–∫—Å—Ç–∞–º (hard clustering)
        
        Args:
            texts: –¢–µ–∫—Å—Ç—ã
            threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–µ–º—ã
            
        Returns:
            –°–ø–∏—Å–æ–∫ (topic_id, probability)
        """
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç—ã –Ω–æ–≤—ã–µ, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º –∏—Ö
        if self.doc_topic_matrix is None or len(texts) != len(self.doc_topic_matrix):
            doc_term_matrix = self.vectorizer.transform(texts)
            doc_topics = self.model.transform(doc_term_matrix)
        else:
            doc_topics = self.doc_topic_matrix
        
        results = []
        
        for doc_topic in doc_topics:
            # –ù–∞—Ö–æ–¥–∏–º —Ç–µ–º—É —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
            max_topic_id = doc_topic.argmax()
            max_prob = doc_topic[max_topic_id]
            
            if max_prob >= threshold:
                results.append((int(max_topic_id), float(max_prob)))
            else:
                results.append((-1, 0.0))  # –ù–µ—Ç —á–µ—Ç–∫–æ–π —Ç–µ–º—ã
        
        return results
    
    def assign_topics_soft(
        self,
        texts: List[str],
        top_k: int = 3,
        min_probability: float = 0.1
    ) -> List[List[Tuple[int, float]]]:
        """
        –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç —Ç–µ–º—ã —Ç–µ–∫—Å—Ç–∞–º (soft clustering) - –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–º –Ω–∞ –∑–∞–ø—Ä–æ—Å
        
        Args:
            texts: –¢–µ–∫—Å—Ç—ã
            top_k: –ú–∞–∫—Å–∏–º—É–º —Ç–µ–º –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç
            min_probability: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è —Ç–µ–º—ã
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ [(topic_id, probability), ...]
        """
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç—ã –Ω–æ–≤—ã–µ, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º –∏—Ö
        if self.doc_topic_matrix is None or len(texts) != len(self.doc_topic_matrix):
            doc_term_matrix = self.vectorizer.transform(texts)
            doc_topics = self.model.transform(doc_term_matrix)
        else:
            doc_topics = self.doc_topic_matrix
        
        results = []
        
        for doc_topic in doc_topics:
            # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-k —Ç–µ–º —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞
            topic_probs = [
                (topic_id, prob)
                for topic_id, prob in enumerate(doc_topic)
                if prob >= min_probability
            ]
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            topic_probs = sorted(topic_probs, key=lambda x: x[1], reverse=True)[:top_k]
            
            if not topic_probs:
                # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–µ–º –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞, –±–µ—Ä–µ–º —Å–∞–º—É—é –≤–µ—Ä–æ—è—Ç–Ω—É—é
                max_topic_id = doc_topic.argmax()
                topic_probs = [(int(max_topic_id), float(doc_topic[max_topic_id]))]
            
            results.append([(int(tid), float(prob)) for tid, prob in topic_probs])
        
        return results
    
    def add_topics_to_dataframe(
        self,
        df: pd.DataFrame,
        text_column: str = 'lemmatized',
        use_soft_clustering: bool = False
    ) -> pd.DataFrame:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ–º—ã –≤ DataFrame
        
        Args:
            df: DataFrame
            text_column: –ö–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º
            use_soft_clustering: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å soft clustering (–Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–º)
            
        Returns:
            DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏
        """
        print("üîÑ –ü—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏–µ —Ç–µ–º –∑–∞–ø—Ä–æ—Å–∞–º...")
        
        texts = df[text_column].tolist()
        
        if use_soft_clustering:
            # Soft clustering - –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–º –Ω–∞ –∑–∞–ø—Ä–æ—Å
            topic_assignments = self.assign_topics_soft(texts, top_k=3, min_probability=0.15)
            
            # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞ (—Å–∞–º–∞—è –≤–µ—Ä–æ—è—Ç–Ω–∞—è)
            df['topic_id'] = [t[0][0] if t else -1 for t in topic_assignments]
            df['topic_probability'] = [t[0][1] if t else 0.0 for t in topic_assignments]
            
            # –í—Å–µ —Ç–µ–º—ã –∫–∞–∫ —Å–ø–∏—Å–æ–∫
            df['all_topics'] = [
                [(tid, round(prob, 3)) for tid, prob in topics]
                for topics in topic_assignments
            ]
            
            # –í—Å–µ —Ç–µ–º—ã –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞ (–¥–ª—è CSV)
            df['all_topics_str'] = [
                ', '.join([f"T{tid}({prob:.2f})" for tid, prob in topics])
                for topics in topic_assignments
            ]
        else:
            # Hard clustering - –æ–¥–Ω–∞ —Ç–µ–º–∞
            topic_assignments = self.assign_topics(texts)
            
            df['topic_id'] = [t[0] for t in topic_assignments]
            df['topic_probability'] = [t[1] for t in topic_assignments]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–µ–º
        topics_info = self.get_all_topics()
        df['topic_name'] = df['topic_id'].map(
            lambda x: topics_info.get(x, {}).get('topic_name', 'No Topic')
        )
        
        clustering_type = "soft" if use_soft_clustering else "hard"
        print(f"‚úì –¢–µ–º—ã –ø—Ä–∏—Å–≤–æ–µ–Ω—ã ({clustering_type} clustering)")
        return df
    
    def get_topic_distribution(self, texts: List[str]) -> Dict[int, int]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º"""
        topic_assignments = self.assign_topics(texts)
        return get_topic_distribution(topic_assignments)

