"""
SOFT-–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è URL –≤ SERP
"""
from typing import List, Dict, Set, Tuple
import pandas as pd
from collections import defaultdict


class SERPClusterer:
    """
    –ú—è–≥–∫–∞—è (SOFT) –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–æ–≤–æ–π –≤—ã–¥–∞—á–∏
    
    –ü—Ä–∏–Ω—Ü–∏–ø: –µ—Å–ª–∏ –¥–≤–∞ –∑–∞–ø—Ä–æ—Å–∞ –∏–º–µ—é—Ç N+ –æ–±—â–∏—Ö URL –≤ —Ç–æ–ø-30, –æ–Ω–∏ –≤ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
    """
    
    def __init__(self, min_common_urls: int = 7, top_positions: int = 30, max_cluster_size: int = 100, strict_mode: bool = False):
        """
        Args:
            min_common_urls: –ú–∏–Ω–∏–º—É–º –æ–±—â–∏—Ö URL –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 7)
            top_positions: –ì–ª—É–±–∏–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞ - —Å–∫–æ–ª—å–∫–æ –ø–æ–∑–∏—Ü–∏–π —É—á–∏—Ç—ã–≤–∞—Ç—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 30)
            max_cluster_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 100)
            strict_mode: –°—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º - —Ç—Ä–µ–±–æ–≤–∞—Ç—å —Å—Ö–æ–∂–µ—Å—Ç–∏ –í–°–ï–• –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False)
        """
        self.min_common_urls = min_common_urls
        self.top_positions = top_positions
        self.max_cluster_size = max_cluster_size
        self.strict_mode = strict_mode
        self.clusters = {}  # query -> cluster_id
        self.cluster_queries = defaultdict(list)  # cluster_id -> [queries]
        
    def _split_large_cluster(self, queries: List[str], query_urls: Dict[str, Set[str]]) -> List[List[str]]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π –∫–ª–∞—Å—Ç–µ—Ä –Ω–∞ –ø–æ–¥–∫–ª–∞—Å—Ç–µ—Ä—ã
        
        Args:
            queries: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –±–æ–ª—å—à–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
            query_urls: –°–ª–æ–≤–∞—Ä—å query -> set(urls)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø–æ–¥–∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–≥–ª–æ–º–µ—Ä–∞—Ç–∏–≤–Ω—É—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ) –∏–ª–∏ –∞–ª—Ñ–∞–≤–∏—Ç—É
        sorted_queries = sorted(queries)
        
        subclusters = []
        current_subcluster = [sorted_queries[0]]
        
        for query in sorted_queries[1:]:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —Å —Ç–µ–∫—É—â–∏–º –ø–æ–¥–∫–ª–∞—Å—Ç–µ—Ä–æ–º
            # –¢—Ä–µ–±—É–µ–º —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ö–æ—Ç—è –±—ã —Å –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º –∏–∑ –ø–æ–¥–∫–ª–∞—Å—Ç–µ—Ä–∞
            has_similarity = False
            for member in current_subcluster:
                common = query_urls[query] & query_urls[member]
                if len(common) >= self.min_common_urls:
                    has_similarity = True
                    break
            
            if has_similarity and len(current_subcluster) < self.max_cluster_size:
                current_subcluster.append(query)
            else:
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π –ø–æ–¥–∫–ª–∞—Å—Ç–µ—Ä
                if len(current_subcluster) > 0:
                    subclusters.append(current_subcluster)
                current_subcluster = [query]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ–¥–∫–ª–∞—Å—Ç–µ—Ä
        if len(current_subcluster) > 0:
            subclusters.append(current_subcluster)
        
        return subclusters
    
    def extract_serp_urls(self, serp_data: any) -> Set[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ SERP –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            serp_data: SERP –¥–∞–Ω–Ω—ã–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ø–∏—Å–æ–∫ URL/—Å—Ç—Ä–æ–∫)
            
        Returns:
            –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö URL –∏–∑ —Ç–æ–ø-N –ø–æ–∑–∏—Ü–∏–π
        """
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ—Ç—É/NaN
        if serp_data is None:
            return set()
        
        # –î–ª—è —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ–º pd.isna
        if not isinstance(serp_data, (list, tuple)):
            if pd.isna(serp_data):
                return set()
            # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ —Å–ø–∏—Å–æ–∫ –∏ –Ω–µ NaN, –Ω–æ –∏ –Ω–µ —Å—Ç—Ä–æ–∫–∞ - –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if not isinstance(serp_data, str):
                return set()
        
        # –î–ª—è —Å–ø–∏—Å–∫–æ–≤ –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É
        if isinstance(serp_data, (list, tuple)) and len(serp_data) == 0:
            return set()
        
        # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ URL (—Å—Ç—Ä–æ–∫)
        if isinstance(serp_data, list):
            urls = []
            for url in serp_data[:self.top_positions]:
                if url and isinstance(url, str):
                    norm_url = self._normalize_url(url)
                    if norm_url:
                        urls.append(norm_url)
            return set(urls)
        
        # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ —Å –¥–æ–º–µ–Ω–∞–º–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é
        if isinstance(serp_data, str):
            urls = [self._normalize_url(d.strip()) for d in serp_data.split(',')]
            return set(urls[:self.top_positions])
        
        return set()
    
    def _normalize_url(self, url: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç URL (—É–±–∏—Ä–∞–µ—Ç –ø—Ä–æ—Ç–æ–∫–æ–ª, www, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—É—Ç—å)"""
        if not url:
            return ""
        
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª
        url = url.replace('https://', '').replace('http://', '')
        
        # –ë–µ—Ä—ë–º –¥–æ –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–æ–±–µ–ª–∞ (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
        url = url.split(' ')[0]
        
        # –£–±–∏—Ä–∞–µ–º www
        url = url.replace('www.', '')
        
        # –£–±–∏—Ä–∞–µ–º trailing slash
        if url.endswith('/'):
            url = url[:-1]
        
        return url.lower()
    
    def calculate_similarity(self, urls1: Set[str], urls2: Set[str]) -> float:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –º–µ–∂–¥—É –¥–≤—É–º—è –Ω–∞–±–æ—Ä–∞–º–∏ URL
        
        Args:
            urls1: –ü–µ—Ä–≤—ã–π –Ω–∞–±–æ—Ä URL
            urls2: –í—Ç–æ—Ä–æ–π –Ω–∞–±–æ—Ä URL
            
        Returns:
            –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –æ—Ç 0 –¥–æ 1 (–ø—Ä–æ—Ü–µ–Ω—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è)
        """
        if not urls1 or not urls2:
            return 0.0
        
        common = len(urls1 & urls2)  # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
        total = len(urls1 | urls2)   # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
        
        return common / total if total > 0 else 0.0
    
    def cluster_by_serp(
        self,
        df: pd.DataFrame,
        serp_column: str = 'serp_main_pages'
    ) -> pd.DataFrame:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç SOFT-–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ SERP
        
        Args:
            df: DataFrame —Å –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∏ SERP –¥–∞–Ω–Ω—ã–º–∏
            serp_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å SERP –¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏ semantic_cluster_id –∏ cluster_name
        """
        print(f"üîÑ SOFT-–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ SERP (–ø–æ—Ä–æ–≥: {self.min_common_urls} –æ–±—â–∏—Ö URL –∏–∑ —Ç–æ–ø-{self.top_positions})...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ SERP –¥–∞–Ω–Ω—ã—Ö
        if serp_column not in df.columns:
            print(f"‚ö†Ô∏è  –ö–æ–ª–æ–Ω–∫–∞ '{serp_column}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é.")
            df['semantic_cluster_id'] = -1
            df['cluster_name'] = df['keyword']
            return df
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∏–∑ SERP –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        print("  üìä –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL –∏–∑ SERP...")
        query_urls = {}
        for idx, row in df.iterrows():
            query = row['keyword']
            serp_data = row[serp_column]
            urls = self.extract_serp_urls(serp_data)
            query_urls[query] = urls
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã –±–µ–∑ SERP –¥–∞–Ω–Ω—ã—Ö
        queries_with_serp = [q for q, urls in query_urls.items() if len(urls) > 0]
        print(f"  ‚úì –ó–∞–ø—Ä–æ—Å–æ–≤ —Å SERP –¥–∞–Ω–Ω—ã–º–∏: {len(queries_with_serp)} –∏–∑ {len(df)}")
        
        if len(queries_with_serp) == 0:
            print("‚ö†Ô∏è  –ù–µ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ —Å SERP –¥–∞–Ω–Ω—ã–º–∏. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞.")
            df['semantic_cluster_id'] = -1
            df['cluster_name'] = df['keyword']
            return df
        
        # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ —Å—Ö–æ–∂–µ—Å—Ç–∏ —á–µ—Ä–µ–∑ –ò–ù–í–ï–†–¢–ò–†–û–í–ê–ù–ù–´–ô –ò–ù–î–ï–ö–° (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤)
        print("  üîó –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏...")
        
        # –®–∞–≥ 1: –°—Ç—Ä–æ–∏–º –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å (URL -> —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤)
        url_to_queries = defaultdict(set)
        
        for query in queries_with_serp:
            for url in list(query_urls[query])[:self.top_positions]:  # –¢–æ–ª—å–∫–æ —Ç–æ–ø-N URL
                url_to_queries[url].add(query)
        
        # –®–∞–≥ 2: –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ —Å–≤—è–∑–µ–π —Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        graph = defaultdict(set)
        
        for query1 in queries_with_serp:
            # –ù–∞—Ö–æ–¥–∏–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ - –∑–∞–ø—Ä–æ—Å—ã —Å –æ–±—â–∏–º–∏ URL
            candidates = set()
            for url in list(query_urls[query1])[:self.top_positions]:
                candidates.update(url_to_queries[url])
            
            # –£–±–∏—Ä–∞–µ–º —Å–∞–º –∑–∞–ø—Ä–æ—Å –∏–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            candidates.discard(query1)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —Ç–æ–ª—å–∫–æ —Å –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º–∏
            for query2 in candidates:
                if query1 < query2:  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è (A-B –∏ B-A)
                    common_urls = query_urls[query1] & query_urls[query2]
                    
                    if len(common_urls) >= self.min_common_urls:
                        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–±—Ä–æ –≤ –≥—Ä–∞—Ñ (–¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ)
                        graph[query1].add(query2)
                        graph[query2].add(query1)
        
        # –®–∞–≥ 2: –ü–æ–∏—Å–∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —Å–≤—è–∑–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π DFS
        visited = set()
        cluster_id = 0
        
        def dfs_iterative(start_node):
            """–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥ –≤ –≥–ª—É–±–∏–Ω—É –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–≤—è–∑–Ω–æ—Å—Ç–∏"""
            component = []
            stack = [start_node]
            
            while stack:
                node = stack.pop()
                
                if node in visited:
                    continue
                
                visited.add(node)
                component.append(node)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ—Ö —Å–æ—Å–µ–¥–µ–π –≤ —Å—Ç–µ–∫
                for neighbor in graph.get(node, []):
                    if neighbor not in visited:
                        stack.append(neighbor)
            
            return component
        
        # –®–∞–≥ 3: –û–±—Ö–æ–¥–∏–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –Ω–∞—Ö–æ–¥–∏–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        for query in queries_with_serp:
            if query not in visited:
                # –ù–æ–≤–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ = –Ω–æ–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä
                component = dfs_iterative(query)
                
                # –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π - —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ –Ω–∞ –ø–æ–¥–∫–ª–∞—Å—Ç–µ—Ä—ã
                if len(component) > self.max_cluster_size:
                    # –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–æ–π –∫–ª–∞—Å—Ç–µ—Ä –Ω–∞ –ø–æ–¥–∫–ª–∞—Å—Ç–µ—Ä—ã –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏
                    subclusters = self._split_large_cluster(component, query_urls)
                    for subcluster in subclusters:
                        for member in subcluster:
                            self.clusters[member] = cluster_id
                            self.cluster_queries[cluster_id].append(member)
                        cluster_id += 1
                else:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä –∫–∞–∫ –µ—Å—Ç—å
                    for member in component:
                        self.clusters[member] = cluster_id
                        self.cluster_queries[cluster_id].append(member)
                    cluster_id += 1
        
        # –ó–∞–ø—Ä–æ—Å—ã –±–µ–∑ SERP ‚Üí –∫–∞–∂–¥—ã–π –ø–æ–ª—É—á–∞–µ—Ç —Å–≤–æ–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
        for idx, row in df.iterrows():
            query = row['keyword']
            if query not in self.clusters:
                self.clusters[query] = cluster_id
                self.cluster_queries[cluster_id].append(query)
                cluster_id += 1
        
        total_clusters = len(self.cluster_queries)
        print(f"  ‚úì –°–æ–∑–¥–∞–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {total_clusters}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ DataFrame
        df['semantic_cluster_id'] = df['keyword'].map(self.clusters)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è –∫–ª–∞—Å—Ç–µ—Ä–∞ (–∑–∞–ø—Ä–æ—Å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å—é)
        cluster_names = {}
        for cid, queries in self.cluster_queries.items():
            cluster_df = df[df['keyword'].isin(queries)]
            if len(cluster_df) > 0:
                main_query = cluster_df.nlargest(1, 'frequency_world')['keyword'].iloc[0]
                cluster_names[cid] = main_query
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –∏–º—è
        df['cluster_name'] = df['semantic_cluster_id'].map(
            lambda x: cluster_names.get(x, '')
        )
        
        # –î–ª—è –ø—É—Å—Ç—ã—Ö –∏–º—ë–Ω –±–µ—Ä—ë–º —Å–∞–º –∑–∞–ø—Ä–æ—Å
        df.loc[df['cluster_name'] == '', 'cluster_name'] = df.loc[df['cluster_name'] == '', 'keyword']
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        multi_query_clusters = sum(1 for queries in self.cluster_queries.values() if len(queries) > 1)
        single_query_clusters = sum(1 for queries in self.cluster_queries.values() if len(queries) == 1)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–∞–∑–º–µ—Ä–∞–º –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        cluster_sizes = [len(queries) for queries in self.cluster_queries.values()]
        max_size = max(cluster_sizes) if cluster_sizes else 0
        avg_size = sum(cluster_sizes) / len(cluster_sizes) if cluster_sizes else 0
        
        print(f"  ‚úì –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–æ: {len(df)} –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ {total_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
        print(f"  ‚Ä¢ –ö–ª–∞—Å—Ç–µ—Ä—ã —Å 2+ –∑–∞–ø—Ä–æ—Å–∞–º–∏: {multi_query_clusters}")
        print(f"  ‚Ä¢ –û–¥–∏–Ω–æ—á–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã: {single_query_clusters}")
        print(f"  ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞: {max_size} –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞: {avg_size:.1f} –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
        large_clusters = sum(1 for size in cluster_sizes if size > 100)
        if large_clusters > 0:
            print(f"  ‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {large_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å >100 –∑–∞–ø—Ä–æ—Å–∞–º–∏!")
            print(f"     –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å --serp-similarity-threshold (—Ç–µ–∫—É—â–∏–π: {self.min_common_urls})")
        
        return df
    
    def get_cluster_info(self, cluster_id: int, df: pd.DataFrame, serp_column: str = 'serp_main_pages') -> Dict:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–ª–∞—Å—Ç–µ—Ä–µ
        
        Args:
            cluster_id: ID –∫–ª–∞—Å—Ç–µ—Ä–∞
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            serp_column: –ö–æ–ª–æ–Ω–∫–∞ —Å SERP –¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–ª–∞—Å—Ç–µ—Ä–µ
        """
        cluster_df = df[df['semantic_cluster_id'] == cluster_id]
        
        if len(cluster_df) == 0:
            return {}
        
        # –°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ URL
        all_urls = []
        url_sets = []
        
        if serp_column in df.columns:
            for _, row in cluster_df.iterrows():
                urls = self.extract_serp_urls(row[serp_column])
                if urls:
                    url_sets.append(urls)
                    all_urls.extend(list(urls))
        
        # –û–±—â–∏–µ URL (–ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤)
        common_urls = set.intersection(*url_sets) if url_sets else set()
        
        # –¢–æ–ø –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö URL
        from collections import Counter
        popular_urls = []
        if all_urls:
            counter = Counter(all_urls)
            # –¢–æ–ø-10 –∏–ª–∏ –≤—Å–µ, —á—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –±–æ–ª—å—à–µ —á–µ–º –≤ 1 –∑–∞–ø—Ä–æ—Å–µ
            limit = 10
            min_count = 2 if len(cluster_df) > 1 else 1
            
            popular_urls = [
                {'url': url, 'count': count} 
                for url, count in counter.most_common(limit)
                if count >= min_count
            ]
        
        result = {
            'cluster_id': cluster_id,
            'size': len(cluster_df),
            'queries': cluster_df['keyword'].tolist(),
            'common_urls': list(common_urls),
            'popular_urls': popular_urls
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'frequency_world' in cluster_df.columns:
            result['total_frequency'] = cluster_df['frequency_world'].sum()
            result['main_query'] = cluster_df.nlargest(1, 'frequency_world')['keyword'].iloc[0]
            
        if 'serp_docs_count' in cluster_df.columns:
            result['avg_serp_docs'] = cluster_df['serp_docs_count'].mean()
            
        return result

