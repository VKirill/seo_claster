"""–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""

from typing import Dict, List, Optional
import pandas as pd

from .cluster_builder import build_initial_clusters, filter_and_number_clusters
from .cluster_processor import strengthen_cluster_links, redistribute_orphans
from .cluster_exporter import (
    get_cluster_stats,
    add_to_dataframe,
    get_cluster_details,
    export_clusters
)


class WordMatchClusterer:
    """
    –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ç–æ—Ä –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º —Å–ª–æ–≤
    
    –ê–ª–≥–æ—Ä–∏—Ç–º –∞–Ω–∞–ª–æ–≥–∏—á–µ–Ω KeyCollector:
    - –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Å–ª–æ–≤ –º–µ–∂–¥—É —Ñ—Ä–∞–∑–∞–º–∏
    - –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç —Ñ—Ä–∞–∑—ã —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
    - –ü–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ñ—Ä–∞–∑—ã –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏ –ø—Ä–∏ —É—Å–∏–ª–µ–Ω–∏–∏ —Å–≤—è–∑–µ–π
    """
    
    def __init__(
        self,
        min_match_strength: int = 2,
        min_group_size: int = 2,
        strengthen_links: bool = True,
        exclude_stopwords: bool = True,
        use_lemmatization: bool = True
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ç–æ—Ä–∞
        
        Args:
            min_match_strength: –°–∏–ª–∞ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ (–º–∏–Ω–∏–º—É–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å–ª–æ–≤)
            min_group_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã
            strengthen_links: –£—Å–∏–ª–∏–≤–∞—Ç—å —Å–≤—è–∑–∏ –≤ –≥—Ä—É–ø–ø–∞—Ö (–ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
            exclude_stopwords: –ò—Å–∫–ª—é—á–∞—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ –ø–æ–¥—Å—á–µ—Ç–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            use_lemmatization: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–µ–º–º—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        """
        self.min_match_strength = min_match_strength
        self.min_group_size = min_group_size
        self.strengthen_links = strengthen_links
        self.exclude_stopwords = exclude_stopwords
        self.use_lemmatization = use_lemmatization
        
        self.clusters = []
        self.query_to_cluster = {}
    
    def cluster_queries(
        self,
        queries: List[str],
        frequencies: Optional[Dict[str, int]] = None
    ) -> List[Dict]:
        """
        –ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º —Å–ª–æ–≤
        
        Args:
            queries: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
            frequencies: –°–ª–æ–≤–∞—Ä—å {query: frequency} –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        print(f"üîÑ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º —Å–ª–æ–≤...")
        print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: —Å–∏–ª–∞={self.min_match_strength}, –º–∏–Ω.—Ä–∞–∑–º–µ—Ä={self.min_group_size}")
        
        # –≠—Ç–∞–ø 1: –ü–µ—Ä–≤–∏—á–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
        clusters = build_initial_clusters(
            queries,
            frequencies,
            self.min_match_strength,
            self.exclude_stopwords
        )
        print(f"   –≠—Ç–∞–ø 1: —Å–æ–∑–¥–∞–Ω–æ {len(clusters)} –ø–µ—Ä–≤–∏—á–Ω—ã—Ö –≥—Ä—É–ø–ø")
        
        # –≠—Ç–∞–ø 2: –£—Å–∏–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
        if self.strengthen_links:
            print(f"   –≠—Ç–∞–ø 2: —É—Å–∏–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π...")
            clusters = strengthen_cluster_links(clusters, self.exclude_stopwords)
        
        # –≠—Ç–∞–ø 3: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É
        valid_clusters, orphan_queries = filter_and_number_clusters(
            clusters,
            self.min_group_size
        )
        print(f"   –≠—Ç–∞–ø 3: –æ—Å—Ç–∞–ª–æ—Å—å {len(valid_clusters)} –≥—Ä—É–ø–ø >= {self.min_group_size} —Ñ—Ä–∞–∑")
        
        # –≠—Ç–∞–ø 4: –ü–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö —Ñ—Ä–∞–∑
        if orphan_queries:
            print(f"   –≠—Ç–∞–ø 4: –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {len(orphan_queries)} –æ–¥–∏–Ω–æ—á–Ω—ã—Ö —Ñ—Ä–∞–∑...")
            redistributed = redistribute_orphans(
                orphan_queries,
                valid_clusters,
                self.min_match_strength,
                self.exclude_stopwords
            )
            print(f"   –ü–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–æ: {redistributed}/{len(orphan_queries)}")
        
        self.clusters = valid_clusters
        
        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ query -> cluster_id
        self.query_to_cluster = {}
        for cluster in self.clusters:
            for query in cluster['queries']:
                self.query_to_cluster[query] = cluster['cluster_id']
        
        print(f"‚úì –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(self.clusters)} –≥—Ä—É–ø–ø")
        return self.clusters
    
    def get_cluster_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º"""
        return get_cluster_stats(self.clusters, self.query_to_cluster)
    
    def add_to_dataframe(
        self,
        df: pd.DataFrame,
        query_column: str = 'keyword',
        cluster_column: str = 'word_match_cluster_id',
        cluster_name_column: str = 'word_match_cluster_name'
    ) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –≤ DataFrame"""
        return add_to_dataframe(
            df,
            self.clusters,
            self.query_to_cluster,
            query_column,
            cluster_column,
            cluster_name_column
        )
    
    def get_cluster_details(self, cluster_id: int) -> Optional[Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
        return get_cluster_details(self.clusters, cluster_id)
    
    def export_clusters(self) -> pd.DataFrame:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã –≤ DataFrame"""
        return export_clusters(self.clusters)

