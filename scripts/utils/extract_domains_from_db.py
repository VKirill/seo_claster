"""
–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–º–µ–Ω–æ–≤ –∏–∑ SERP –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ –∏–Ω—Ç–µ–Ω—Ç—É.

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—É—â—É—é –ë–î –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–æ–ø-–¥–æ–º–µ–Ω—ã –¥–ª—è –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö
–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.
"""
import sqlite3
from collections import Counter
from urllib.parse import urlparse
import re


def extract_domain(url: str) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–º–µ–Ω –∏–∑ URL.
    
    –£–¥–∞–ª—è–µ—Ç www. –ø—Ä–µ—Ñ–∏–∫—Å, –æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–¥–¥–æ–º–µ–Ω—ã (market.yandex.ru)
    """
    try:
        parsed = urlparse(url)
        domain = parsed.netloc or parsed.path
        
        # –£–±–∏—Ä–∞–µ–º www. –ø—Ä–µ—Ñ–∏–∫—Å
        domain = re.sub(r'^www\.', '', domain)
        
        return domain.lower()
    except:
        return ""


def analyze_serp_domains(db_path: str = 'output/serp_data.db'):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–æ–º–µ–Ω—ã –≤ SERP –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
    
    print("=" * 80)
    print("üîç –ê–ù–ê–õ–ò–ó –î–û–ú–ï–ù–û–í –ò–ó SERP –ë–î")
    print("=" * 80)
    print()
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–º–µ–Ω—ã –∏–∑ serp_documents —Å —Ñ–ª–∞–≥–æ–º is_commercial
        cursor.execute("""
            SELECT domain, is_commercial, COUNT(*) as cnt
            FROM serp_documents
            WHERE domain IS NOT NULL AND domain != ''
            GROUP BY domain, is_commercial
        """)
        
        rows = cursor.fetchall()
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(rows)}")
        print()
        
        # –°—á–µ—Ç—á–∏–∫–∏ –¥–æ–º–µ–Ω–æ–≤
        commercial_domains = Counter()
        informational_domains = Counter()
        all_domains = Counter()
        
        for domain, is_commercial, count in rows:
            domain = domain.lower()
            all_domains[domain] += count
            
            if is_commercial == 1 or is_commercial == True:
                commercial_domains[domain] += count
            else:
                informational_domains[domain] += count
        
        conn.close()
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print(f"üåê –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤: {len(all_domains)}")
        print(f"üõí –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {sum(commercial_domains.values())}")
        print(f"üìö –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {sum(informational_domains.values())}")
        print()
        
        # –¢–æ–ø-30 –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –¥–æ–º–µ–Ω–æ–≤
        print("üõí –¢–û–ü-30 –ö–û–ú–ú–ï–†–ß–ï–°–ö–ò–• –î–û–ú–ï–ù–û–í:")
        print("-" * 80)
        for domain, count in commercial_domains.most_common(30):
            print(f"  {domain:40s} ({count} —Ä–∞–∑)")
        
        print()
        
        # –¢–æ–ø-30 –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
        print("üìö –¢–û–ü-30 –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–´–• –î–û–ú–ï–ù–û–í:")
        print("-" * 80)
        for domain, count in informational_domains.most_common(30):
            print(f"  {domain:40s} ({count} —Ä–∞–∑)")
        
        print()
        print("=" * 80)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        return {
            'commercial': commercial_domains.most_common(50),
            'informational': informational_domains.most_common(50),
            'all': all_domains.most_common(100)
        }
        
    except sqlite3.Error as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –ë–î: {e}")
        return None
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return None


def save_domains_to_files(domains_data: dict):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–æ–º–µ–Ω—ã –≤ txt —Ñ–∞–π–ª—ã.
    
    –§–∏–ª—å—Ç—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ –∫—Ä—É–ø–Ω—ã–µ –¥–æ–º–µ–Ω—ã (–º–∞—Å—Ç–æ–¥–æ–Ω—Ç–æ–≤), —É–±–∏—Ä–∞–µ—Ç www.
    """
    if not domains_data:
        print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –¥–ª—è "–º–∞—Å—Ç–æ–¥–æ–Ω—Ç–∞"
    MIN_OCCURRENCES = 50
    
    print()
    print("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –î–û–ú–ï–ù–û–í –í –§–ê–ô–õ–´ (—Ç–æ–ª—å–∫–æ –º–∞—Å—Ç–æ–¥–æ–Ω—Ç—ã)...")
    print("-" * 80)
    
    # –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –¥–æ–º–µ–Ω—ã - —Ñ–∏–ª—å—Ç—Ä—É–µ–º
    commercial_filtered = [
        (domain, count) for domain, count in domains_data['commercial']
        if count >= MIN_OCCURRENCES
    ]
    
    commercial_file = 'keywords_settings/commercial_domains.txt'
    with open(commercial_file, 'w', encoding='utf-8') as f:
        f.write("# –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –¥–æ–º–µ–Ω—ã (–º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å—ã, –º–∞–≥–∞–∑–∏–Ω—ã, –∫—Ä—É–ø–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã)\n")
        f.write("# –¢–æ–ª—å–∫–æ –º–∞—Å—Ç–æ–¥–æ–Ω—Ç—ã (>50 —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –≤ SERP)\n")
        f.write("# –î–æ–±–∞–≤–ª—è–π—Ç–µ –ø–æ –æ–¥–Ω–æ–º—É –¥–æ–º–µ–Ω—É –Ω–∞ —Å—Ç—Ä–æ–∫—É\n")
        f.write("# –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å #\n")
        f.write("# –§–æ—Ä–º–∞—Ç: domain.ru (–±–µ–∑ www, —Å –ø–æ–¥–¥–æ–º–µ–Ω–∞–º–∏ –µ—Å–ª–∏ –≤–∞–∂–Ω—ã)\n\n")
        
        for domain, count in commercial_filtered:
            # –£–±–∏—Ä–∞–µ–º www. –µ—Å–ª–∏ –µ—Å—Ç—å
            domain_clean = re.sub(r'^www\.', '', domain)
            f.write(f"{domain_clean}\n")
    
    print(f"‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(commercial_filtered)} –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –¥–æ–º–µ–Ω–æ–≤ (>{MIN_OCCURRENCES} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π): {commercial_file}")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–æ–º–µ–Ω—ã - —Ñ–∏–ª—å—Ç—Ä—É–µ–º
    info_filtered = [
        (domain, count) for domain, count in domains_data['informational']
        if count >= MIN_OCCURRENCES
    ]
    
    info_file = 'keywords_settings/informational_domains.txt'
    with open(info_file, 'w', encoding='utf-8') as f:
        f.write("# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–æ–º–µ–Ω—ã (–≤–∏–∫–∏–ø–µ–¥–∏—è, –±–ª–æ–≥–∏, —Ñ–æ—Ä—É–º—ã, –≤–∏–¥–µ–æ)\n")
        f.write("# –¢–æ–ª—å–∫–æ –º–∞—Å—Ç–æ–¥–æ–Ω—Ç—ã (>50 —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –≤ SERP)\n")
        f.write("# –î–æ–±–∞–≤–ª—è–π—Ç–µ –ø–æ –æ–¥–Ω–æ–º—É –¥–æ–º–µ–Ω—É –Ω–∞ —Å—Ç—Ä–æ–∫—É\n")
        f.write("# –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å #\n")
        f.write("# –§–æ—Ä–º–∞—Ç: domain.ru (–±–µ–∑ www, —Å –ø–æ–¥–¥–æ–º–µ–Ω–∞–º–∏ –µ—Å–ª–∏ –≤–∞–∂–Ω—ã)\n\n")
        
        for domain, count in info_filtered:
            # –£–±–∏—Ä–∞–µ–º www. –µ—Å–ª–∏ –µ—Å—Ç—å
            domain_clean = re.sub(r'^www\.', '', domain)
            f.write(f"{domain_clean}\n")
    
    print(f"‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(info_filtered)} –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤ (>{MIN_OCCURRENCES} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π): {info_file}")
    
    # –í—Å–µ –¥–æ–º–µ–Ω—ã (–¥–ª—è —Å–ø—Ä–∞–≤–∫–∏)
    all_file = 'keywords_settings/all_domains_stats.txt'
    with open(all_file, 'w', encoding='utf-8') as f:
        f.write("# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—Å–µ—Ö –¥–æ–º–µ–Ω–æ–≤ –∏–∑ SERP –ë–î\n")
        f.write("# –§–æ—Ä–º–∞—Ç: –¥–æ–º–µ–Ω (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_—É–ø–æ–º–∏–Ω–∞–Ω–∏–π)\n\n")
        
        for domain, count in domains_data['all']:
            f.write(f"{domain} ({count})\n")
    
    print(f"‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—Å–µ—Ö –¥–æ–º–µ–Ω–æ–≤: {all_file}")
    print()
    print("=" * 80)


if __name__ == "__main__":
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ë–î
    domains_data = analyze_serp_domains()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª—ã
    if domains_data:
        save_domains_to_files(domains_data)
        
        print()
        print("‚úÖ –ì–û–¢–û–í–û!")
        print()
        print("üìù –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        print("   1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã –≤ keywords_settings/")
        print("   2. –û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ —Å–ø–∏—Å–∫–∏ –¥–æ–º–µ–Ω–æ–≤ –≤—Ä—É—á–Ω—É—é –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏")
        print("   3. –î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–∏ –¥–æ–º–µ–Ω—ã")
        print("   4. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑ - –¥–æ–º–µ–Ω—ã –±—É–¥—É—Ç —É—á–∏—Ç—ã–≤–∞—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")

