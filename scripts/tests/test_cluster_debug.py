"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤.

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ—á–µ–º—É –¥–≤–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ø–∞–ª–∏ –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä:
- "–∫—Ä–∞—Ç–∫–∞—è –±–∏–æ–≥—Ä–∞—Ñ–∏—è –Ω–∏–∫–æ–ª–∞–π —á—É–¥–æ—Ç–≤–æ—Ä–µ—Ü"
- "—á–∞—Å—Ç—å –º–æ—â–µ–π –Ω–∏–∫–æ–ª–∞—è —á—É–¥–æ—Ç–≤–æ—Ä—Ü–∞"

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/tests/test_cluster_debug.py
"""

import json
import os
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

import pandas as pd
from seo_analyzer.core.cache.master_query_db import MasterQueryDatabase
from seo_analyzer.clustering.serp_advanced_clusterer import AdvancedSERPClusterer
from seo_analyzer.core.helpers import load_all_data
from seo_analyzer.core.serp_config import SERP_CONFIG
from seo_analyzer.core.query_groups import QueryGroupManager

# –ü—Ä–æ–±—É–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å config_local
try:
    import config_local
except ImportError:
    config_local = None


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∞"""
    
    # –ó–∞–ø—Ä–æ—Å—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    query1 = "–∫—Ä–∞—Ç–∫–∞—è –±–∏–æ–≥—Ä–∞—Ñ–∏—è –Ω–∏–∫–æ–ª–∞–π —á—É–¥–æ—Ç–≤–æ—Ä–µ—Ü"
    query2 = "—á–∞—Å—Ç—å –º–æ—â–µ–π –Ω–∏–∫–æ–ª–∞—è —á—É–¥–æ—Ç–≤–æ—Ä—Ü–∞"
    group_name = "–Ω–∏–∫–æ–ª–∞–π_—á—É–¥–æ—Ç–≤–æ—Ä–µ—Ü"
    clustering_threshold = 7
    max_cluster_size = 0
    
    print("=" * 80)
    print("DIAGNOSTIKA KLUSTERIZACII")
    print("=" * 80)
    print(f"\n–ì—Ä—É–ø–ø–∞: {group_name}")
    print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: min_common_urls={clustering_threshold}, max_cluster_size={max_cluster_size}")
    print(f"–ó–∞–ø—Ä–æ—Å 1: '{query1}'")
    print(f"–ó–∞–ø—Ä–æ—Å 2: '{query2}'")
    print()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ output —Ñ–∞–π–ª–æ–≤
    print("=" * 80)
    print("PROVERKA REZULTATOV IZ OUTPUT FAILOV")
    print("=" * 80)
    
    group_manager = QueryGroupManager()
    group_manager.discover_groups()
    group = group_manager.get_group(group_name)
    
    if not group:
        print(f"WARNING: Gruppa '{group_name}' ne naydena cherez QueryGroupManager")
        print("  Prodolzhaem s Master DB...")
        group = None
    
    # –ò—â–µ–º CSV —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    use_output_file = False
    csv_path = None
    
    if group:
        file_suffix = f"_{clustering_threshold}_{max_cluster_size}" if max_cluster_size else f"_{clustering_threshold}"
        csv_path = group.output_dir / f"seo_analysis_full{file_suffix}.csv"
        
        if not csv_path.exists():
            print(f"WARNING: Fail rezultatov ne nayden: {csv_path}")
            print(f"  Zapustite snachala: python main.py {group_name} {clustering_threshold} {max_cluster_size}")
            print("\n  Prodolzhaem s analizom cherez Master DB...")
        else:
            print(f"OK: Nayden fail rezultatov: {csv_path.name}")
            use_output_file = True
    else:
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—É—Ç—å
        output_dir = Path("output") / "groups" / group_name
        file_suffix = f"_{clustering_threshold}_{max_cluster_size}" if max_cluster_size else f"_{clustering_threshold}"
        csv_path = output_dir / f"seo_analysis_full{file_suffix}.csv"
        
        if csv_path.exists():
            print(f"OK: Nayden fail rezultatov: {csv_path.name}")
            use_output_file = True
        else:
            print(f"WARNING: Fail rezultatov ne nayden: {csv_path}")
            print("  Prodolzhaem s analizom cherez Master DB...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print("\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    master_db = MasterQueryDatabase()
    
    if not master_db.group_exists(group_name):
        print(f"‚ùå –ì—Ä—É–ø–ø–∞ '{group_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ Master DB")
        return
    
    df = master_db.load_queries(group_name, include_serp_urls=True)
    
    if df is None or df.empty:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä—É–ø–ø—ã '{group_name}'")
        return
    
    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ Master DB")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
    if query1 not in df['keyword'].values:
        print(f"‚ùå –ó–∞–ø—Ä–æ—Å '{query1}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö")
        return
    
    if query2 not in df['keyword'].values:
        print(f"‚ùå –ó–∞–ø—Ä–æ—Å '{query2}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö")
        return
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å output —Ñ–∞–π–ª - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç—Ç—É–¥–∞
    if use_output_file:
        try:
            output_df = pd.read_csv(csv_path, encoding='utf-8-sig', low_memory=False)
            
            if query1 in output_df['keyword'].values and query2 in output_df['keyword'].values:
                row1_output = output_df[output_df['keyword'] == query1].iloc[0]
                row2_output = output_df[output_df['keyword'] == query2].iloc[0]
                
                cluster_id1 = row1_output.get('semantic_cluster_id', -1)
                cluster_id2 = row2_output.get('semantic_cluster_id', -1)
                cluster_name1 = row1_output.get('cluster_name', 'N/A')
                cluster_name2 = row2_output.get('cluster_name', 'N/A')
                
                print(f"\nüìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ output —Ñ–∞–π–ª–∞:")
                print(f"  –ó–∞–ø—Ä–æ—Å 1: –∫–ª–∞—Å—Ç–µ—Ä ID = {cluster_id1}, –∏–º—è = '{cluster_name1}'")
                print(f"  –ó–∞–ø—Ä–æ—Å 2: –∫–ª–∞—Å—Ç–µ—Ä ID = {cluster_id2}, –∏–º—è = '{cluster_name2}'")
                
                if cluster_id1 == cluster_id2 and cluster_id1 != -1:
                    print(f"\n  ‚ö†Ô∏è  –û–ë–ê –ó–ê–ü–†–û–°–ê –í –û–î–ù–û–ú –ö–õ–ê–°–¢–ï–†–ï!")
                    
                    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –≤ —ç—Ç–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
                    cluster_queries = output_df[output_df['semantic_cluster_id'] == cluster_id1]['keyword'].tolist()
                    print(f"\n  –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ: {len(cluster_queries)}")
                    print(f"  –ü–µ—Ä–≤—ã–µ 15 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ:")
                    for i, q in enumerate(cluster_queries[:15], 1):
                        marker = " <--" if q in [query1, query2] else ""
                        print(f"    {i}. {q}{marker}")
                    
                    if len(cluster_queries) > 15:
                        print(f"    ... –∏ –µ—â–µ {len(cluster_queries) - 15} –∑–∞–ø—Ä–æ—Å–æ–≤")
                else:
                    print(f"\n  ‚úì –ó–∞–ø—Ä–æ—Å—ã –≤ —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö")
                    print(f"\n  (–í–æ–∑–º–æ–∂–Ω–æ, –ø—Ä–æ–±–ª–µ–º–∞ –±—ã–ª–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –∏–ª–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å)")
                    return
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è output —Ñ–∞–π–ª–∞: {e}")
            print("   –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –∞–Ω–∞–ª–∏–∑–æ–º —á–µ—Ä–µ–∑ Master DB...")
    
    print()
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º
    row1 = df[df['keyword'] == query1].iloc[0]
    row2 = df[df['keyword'] == query2].iloc[0]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º SERP –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    print("=" * 80)
    print("–ü–†–û–í–ï–†–ö–ê –ò –ó–ê–ì–†–£–ó–ö–ê SERP –î–ê–ù–ù–´–•")
    print("=" * 80)
    
    # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á
    api_key = None
    if config_local and hasattr(config_local, 'XMLSTOCK_API_KEY'):
        api_key = config_local.XMLSTOCK_API_KEY
    if not api_key:
        api_key = os.getenv('XMLSTOCK_API_KEY')
    
    if not api_key:
        print("\n‚ö†Ô∏è  API –∫–ª—é—á xmlstock –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —á–µ—Ä–µ–∑:")
        print("   1. config_local.py: XMLSTOCK_API_KEY = 'user:key'")
        print("   2. –ü–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è: XMLSTOCK_API_KEY=user:key")
    else:
        print(f"\n‚úì API –∫–ª—é—á –Ω–∞–π–¥–µ–Ω: {api_key[:20]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ SERP –¥–∞–Ω–Ω—ã—Ö
        def has_serp_data(serp_data):
            if serp_data is None:
                return False
            if isinstance(serp_data, str):
                try:
                    parsed = json.loads(serp_data)
                    return isinstance(parsed, list) and len(parsed) > 0
                except:
                    return False
            return isinstance(serp_data, list) and len(serp_data) > 0
        
        need_load = []
        serp_data1 = row1.get('serp_top_urls')
        serp_data2 = row2.get('serp_top_urls')
        
        if not has_serp_data(serp_data1):
            print(f"\n‚ö†Ô∏è  –£ –∑–∞–ø—Ä–æ—Å–∞ '{query1}' –Ω–µ—Ç SERP –¥–∞–Ω–Ω—ã—Ö")
            need_load.append(query1)
        
        if not has_serp_data(serp_data2):
            print(f"‚ö†Ô∏è  –£ –∑–∞–ø—Ä–æ—Å–∞ '{query2}' –Ω–µ—Ç SERP –¥–∞–Ω–Ω—ã—Ö")
            need_load.append(query2)
        
        if need_load:
            print(f"\nüîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º SERP –¥–∞–Ω–Ω—ã–µ –¥–ª—è {len(need_load)} –∑–∞–ø—Ä–æ—Å–æ–≤...")
            
            from seo_analyzer.analysis.serp.analyzer import SERPAnalyzer
            
            serp_analyzer = SERPAnalyzer(
                api_key=api_key,
                lr=SERP_CONFIG['api']['lr'],
                max_retries=SERP_CONFIG['api']['max_retries'],
                retry_delay=SERP_CONFIG['api']['retry_delay'],
                timeout=SERP_CONFIG['api']['timeout'],
                query_group=group_name,
                max_concurrent=SERP_CONFIG['api']['max_concurrent'],
                use_master_db=True,
                use_batch_async=True
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            serp_results = await serp_analyzer.analyze_queries_batch(need_load)
            
            print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(serp_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º DataFrame –∏–∑ Master DB
            print("üîÑ –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Master DB...")
            df = master_db.load_queries(group_name, include_serp_urls=True)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏
            row1 = df[df['keyword'] == query1].iloc[0]
            row2 = df[df['keyword'] == query2].iloc[0]
            
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
            await serp_analyzer.close()
        else:
            print("\n‚úì –£ –æ–±–æ–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –µ—Å—Ç—å SERP –¥–∞–Ω–Ω—ã–µ")
    
    print()
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º SERP URL
    print("=" * 80)
    print("–ê–ù–ê–õ–ò–ó SERP –î–ê–ù–ù–´–•")
    print("=" * 80)
    
    # –°–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è URL
    clusterer = AdvancedSERPClusterer(
        min_common_urls=clustering_threshold,
        top_positions=30,
        mode='balanced'
    )
    
    # –ü–∞—Ä—Å–∏–º serp_top_urls (–º–æ–∂–µ—Ç –±—ã—Ç—å JSON —Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ —É–∂–µ —Å–ø–∏—Å–æ–∫)
    def parse_serp_urls(serp_data):
        """–ü–∞—Ä—Å–∏—Ç serp_top_urls –∏–∑ JSON —Ñ–æ—Ä–º–∞—Ç–∞"""
        if serp_data is None:
            return []
        
        # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
        if isinstance(serp_data, list):
            urls = []
            for item in serp_data:
                if isinstance(item, dict):
                    url = item.get('url', '')
                elif isinstance(item, str):
                    url = item
                else:
                    continue
                if url:
                    urls.append(url)
            return urls
        
        # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ - –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
        if isinstance(serp_data, str):
            try:
                parsed = json.loads(serp_data)
                if isinstance(parsed, list):
                    urls = []
                    for item in parsed:
                        if isinstance(item, dict):
                            url = item.get('url', '')
                        elif isinstance(item, str):
                            url = item
                        else:
                            continue
                        if url:
                            urls.append(url)
                    return urls
            except (json.JSONDecodeError, TypeError):
                pass
        
        return []
    
    raw_urls1 = parse_serp_urls(row1.get('serp_top_urls'))
    raw_urls2 = parse_serp_urls(row2.get('serp_top_urls'))
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL —á–µ—Ä–µ–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ç–æ—Ä
    serp_urls1 = [clusterer._normalize_url(url) for url in raw_urls1 if url]
    serp_urls2 = [clusterer._normalize_url(url) for url in raw_urls2 if url]
    
    print(f"\n–ó–∞–ø—Ä–æ—Å 1: '{query1}'")
    print(f"  –ù–∞–π–¥–µ–Ω–æ URL: {len(serp_urls1)}")
    if serp_urls1:
        print(f"  –ü–µ—Ä–≤—ã–µ 10 URL:")
        for i, url in enumerate(serp_urls1[:10], 1):
            print(f"    {i}. {url}")
    else:
        print(f"  ‚ö†Ô∏è  –ù–µ—Ç SERP –¥–∞–Ω–Ω—ã—Ö!")
    
    print(f"\n–ó–∞–ø—Ä–æ—Å 2: '{query2}'")
    print(f"  –ù–∞–π–¥–µ–Ω–æ URL: {len(serp_urls2)}")
    if serp_urls2:
        print(f"  –ü–µ—Ä–≤—ã–µ 10 URL:")
        for i, url in enumerate(serp_urls2[:10], 1):
            print(f"    {i}. {url}")
    else:
        print(f"  ‚ö†Ô∏è  –ù–µ—Ç SERP –¥–∞–Ω–Ω—ã—Ö!")
    
    print()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ —É –æ–±–æ–∏—Ö)
    print("=" * 80)
    print("–ê–ù–ê–õ–ò–ó –°–•–û–ñ–ï–°–¢–ò URL")
    print("=" * 80)
    
    common_count = 0
    weighted_score = 0.0
    common_urls = set()
    
    if serp_urls1 and serp_urls2:
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
        common_count, weighted_score = clusterer.calculate_weighted_similarity(
            serp_urls1,
            serp_urls2
        )
        
        print(f"\n–û–±—â–∏–µ URL: {common_count}")
        print(f"–í–∑–≤–µ—à–µ–Ω–Ω—ã–π score: {weighted_score:.2f}")
        print(f"–ü–æ—Ä–æ–≥ (min_common_urls): {clustering_threshold}")
        print()
        
        # –ù–∞—Ö–æ–¥–∏–º –æ–±—â–∏–µ URL
        urls1_set = set(serp_urls1[:30])
        urls2_set = set(serp_urls2[:30])
        common_urls = urls1_set & urls2_set
        
        print(f"–û–±—â–∏–µ URL ({len(common_urls)}):")
        if common_urls:
            for i, url in enumerate(sorted(common_urls), 1):
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏–∏ –≤ –æ–±–æ–∏—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö
                pos1 = serp_urls1.index(url) + 1 if url in serp_urls1 else None
                pos2 = serp_urls2.index(url) + 1 if url in serp_urls2 else None
                
                print(f"  {i}. {url}")
                print(f"     –ü–æ–∑–∏—Ü–∏—è –≤ –∑–∞–ø—Ä–æ—Å–µ 1: {pos1}")
                print(f"     –ü–æ–∑–∏—Ü–∏—è –≤ –∑–∞–ø—Ä–æ—Å–µ 2: {pos2}")
        else:
            print("  (–Ω–µ—Ç –æ–±—â–∏—Ö URL)")
        
        print(f"\n–í—ã–≤–æ–¥:")
        if common_count >= clustering_threshold:
            print(f"  ‚úÖ –ü–æ—Ä–æ–≥ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω ({common_count} >= {clustering_threshold})")
            print(f"  ‚Üí –ó–∞–ø—Ä–æ—Å—ã –î–û–õ–ñ–ù–´ –±—ã—Ç—å –≤ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ")
        else:
            print(f"  ‚ùå –ü–æ—Ä–æ–≥ –ù–ï –ø—Ä–µ–æ–¥–æ–ª–µ–Ω ({common_count} < {clustering_threshold})")
            print(f"  ‚Üí –ó–∞–ø—Ä–æ—Å—ã –ù–ï –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ")
            print(f"  ‚Üí –í–æ–∑–º–æ–∂–Ω–æ, –æ–Ω–∏ –æ–±—ä–µ–¥–∏–Ω–∏–ª–∏—Å—å —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω—É—é —Å–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏")
    else:
        print("\n‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ SERP –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä—è–º–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:")
        print(f"  –ó–∞–ø—Ä–æ—Å 1: {len(serp_urls1)} URL")
        print(f"  –ó–∞–ø—Ä–æ—Å 2: {len(serp_urls2)} URL")
        print("\n  ‚ö†Ô∏è  –ï—Å–ª–∏ —É –∑–∞–ø—Ä–æ—Å–∞ –Ω–µ—Ç SERP –¥–∞–Ω–Ω—ã—Ö, –æ–Ω –º–æ–∂–µ—Ç –ø–æ–ø–∞—Å—Ç—å –≤ –∫–ª–∞—Å—Ç–µ—Ä")
        print("     —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω—É—é —Å–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏!")
    
    print()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    print("=" * 80)
    print("–¢–ï–°–¢ –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò (–Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö)")
    print("=" * 80)
    
    print(f"\n–ó–∞–ø—É—Å–∫–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –Ω–∞ –≤—Å–µ—Ö {len(df)} –∑–∞–ø—Ä–æ—Å–∞—Ö...")
    print("(—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è)")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º serp_top_urls –≤ serp_urls (—Å–ø–∏—Å–æ–∫ URL) –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ç–æ—Ä–∞
    def convert_serp_urls(row):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç serp_top_urls –≤ —Å–ø–∏—Å–æ–∫ URL"""
        serp_data = row.get('serp_top_urls')
        if serp_data is None:
            return []
        
        # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
        if isinstance(serp_data, list):
            urls = []
            for item in serp_data:
                if isinstance(item, dict):
                    url = item.get('url', '')
                elif isinstance(item, str):
                    url = item
                else:
                    continue
                if url:
                    urls.append(url)
            return urls
        
        # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ - –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
        if isinstance(serp_data, str):
            try:
                parsed = json.loads(serp_data)
                if isinstance(parsed, list):
                    urls = []
                    for item in parsed:
                        if isinstance(item, dict):
                            url = item.get('url', '')
                        elif isinstance(item, str):
                            url = item
                        else:
                            continue
                        if url:
                            urls.append(url)
                    return urls
            except (json.JSONDecodeError, TypeError):
                pass
        
        return []
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é –∫–æ –≤—Å–µ–º –¥–∞–Ω–Ω—ã–º
    df['serp_urls'] = df.apply(lambda row: convert_serp_urls(row), axis=1)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
    clustered_df = await clusterer.cluster_by_serp(
        df,
        serp_column='serp_urls'
    )
    
    # –ù–∞—Ö–æ–¥–∏–º –∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è –Ω–∞—à–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    cluster_row1 = clustered_df[clustered_df['keyword'] == query1]
    cluster_row2 = clustered_df[clustered_df['keyword'] == query2]
    
    if cluster_row1.empty:
        print(f"‚ùå –ó–∞–ø—Ä–æ—Å '{query1}' –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ—Å–ª–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
        return
    
    if cluster_row2.empty:
        print(f"‚ùå –ó–∞–ø—Ä–æ—Å '{query2}' –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ—Å–ª–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
        return
    
    cluster_id1 = cluster_row1['semantic_cluster_id'].iloc[0]
    cluster_id2 = cluster_row2['semantic_cluster_id'].iloc[0]
    cluster_name1 = cluster_row1['cluster_name'].iloc[0]
    cluster_name2 = cluster_row2['cluster_name'].iloc[0]
    
    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏:")
    print(f"  –ó–∞–ø—Ä–æ—Å 1: '{query1}'")
    print(f"    ‚Üí –ö–ª–∞—Å—Ç–µ—Ä ID: {cluster_id1}")
    print(f"    ‚Üí –ò–º—è –∫–ª–∞—Å—Ç–µ—Ä–∞: '{cluster_name1}'")
    
    print(f"\n  –ó–∞–ø—Ä–æ—Å 2: '{query2}'")
    print(f"    ‚Üí –ö–ª–∞—Å—Ç–µ—Ä ID: {cluster_id2}")
    print(f"    ‚Üí –ò–º—è –∫–ª–∞—Å—Ç–µ—Ä–∞: '{cluster_name2}'")
    
    if cluster_id1 == cluster_id2:
        print(f"\n  ‚ö†Ô∏è  –û–ë–ê –ó–ê–ü–†–û–°–ê –í –û–î–ù–û–ú –ö–õ–ê–°–¢–ï–†–ï!")
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –≤ —ç—Ç–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
        cluster_queries = clustered_df[clustered_df['semantic_cluster_id'] == cluster_id1]['keyword'].tolist()
        print(f"\n  –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ: {len(cluster_queries)}")
        print(f"  –ü–µ—Ä–≤—ã–µ 15 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ:")
        for i, q in enumerate(cluster_queries[:15], 1):
            marker = " <--" if q in [query1, query2] else ""
            print(f"    {i}. {q}{marker}")
        
        if len(cluster_queries) > 15:
            print(f"    ... –∏ –µ—â–µ {len(cluster_queries) - 15} –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        print(f"\n  –ü—Ä–∏—á–∏–Ω–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è:")
        if serp_urls1 and serp_urls2:
            print(f"    - –û–±—â–∏—Ö URL: {common_count} (–ø–æ—Ä–æ–≥: {clustering_threshold})")
            print(f"    - –í–∑–≤–µ—à–µ–Ω–Ω—ã–π score: {weighted_score:.2f} (–ø–æ—Ä–æ–≥: {clustering_threshold})")
            if common_count >= clustering_threshold:
                print(f"    - ‚úÖ –ü–æ—Ä–æ–≥ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω ({common_count} >= {clustering_threshold})")
            else:
                print(f"    - ‚ùå –ü–æ—Ä–æ–≥ –ù–ï –ø—Ä–µ–æ–¥–æ–ª–µ–Ω ({common_count} < {clustering_threshold})")
                print(f"    - ‚ö†Ô∏è  –ó–∞–ø—Ä–æ—Å—ã –æ–±—ä–µ–¥–∏–Ω–∏–ª–∏—Å—å —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω—É—é —Å–≤—è–∑—å!")
                print(f"    - –í–æ–∑–º–æ–∂–Ω–æ, –µ—Å—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å, –∫–æ—Ç–æ—Ä—ã–π —Å–≤—è–∑–∞–Ω —Å –æ–±–æ–∏–º–∏")
        else:
            print(f"    - ‚ö†Ô∏è  –£ –æ–¥–Ω–æ–≥–æ –∏–∑ –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–µ—Ç SERP –¥–∞–Ω–Ω—ã—Ö!")
            print(f"    - –ó–∞–ø—Ä–æ—Å 1 –∏–º–µ–µ—Ç {len(serp_urls1)} URL")
            print(f"    - –ó–∞–ø—Ä–æ—Å 2 –∏–º–µ–µ—Ç {len(serp_urls2)} URL")
            print(f"    - –ó–∞–ø—Ä–æ—Å—ã –æ–±—ä–µ–¥–∏–Ω–∏–ª–∏—Å—å —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω—É—é —Å–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏")
    else:
        print(f"\n  ‚úì –ó–∞–ø—Ä–æ—Å—ã –≤ —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö")
        print(f"\n  (–í–æ–∑–º–æ–∂–Ω–æ, –ø—Ä–æ–±–ª–µ–º–∞ –±—ã–ª–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –∏–ª–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å)")
    
    print()
    print("=" * 80)
    print("–î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
    print("=" * 80)


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
