"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–ø–æ–ª–Ω—è–µ–º–æ—Å—Ç–∏ –ø–æ–ª–µ–π –≤ –ë–î
–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–π CSV —Å 10 –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç pipeline
"""

import asyncio
import pandas as pd
import sqlite3
from pathlib import Path
import sys
import os
import io

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º UTF-8 –¥–ª—è –≤—ã–≤–æ–¥–∞
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from pipeline.analyzer import SEOAnalyzer
from seo_analyzer.core.cache.master_query_db import MasterQueryDatabase


def create_test_csv(num_queries: int = 10) -> Path:
    """–°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π CSV —Ñ–∞–π–ª —Å –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
    test_queries = [
        '–∫—É–ø–∏—Ç—å —Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫ –º–æ—Å–∫–≤–∞',
        '—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫ —Ü–µ–Ω–∞',
        '—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫ –æ—Ç–∑—ã–≤—ã',
        '—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫ –Ω–µ–¥–æ—Ä–æ–≥–æ',
        '–≥–¥–µ –∫—É–ø–∏—Ç—å —Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫',
        '—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫ –¥–≤—É—Ö–∫–∞–º–µ—Ä–Ω—ã–π',
        '—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫ lg –æ—Ç–∑—ã–≤—ã',
        '—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫ –∏–Ω–¥–µ–∑–∏—Ç —Ü–µ–Ω–∞',
        '—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫ —Å –º–æ—Ä–æ–∑–∏–ª–∫–æ–π',
        '—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫ –¥–ª—è –¥–æ–º–∞'
    ]
    
    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    test_queries = test_queries[:num_queries]
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    df = pd.DataFrame({
        '–ó–∞–ø—Ä–æ—Å': test_queries,
        'frequency_world': [1200, 800, 500, 450, 350, 600, 400, 300, 550, 250],
        'frequency_exact': [950, 650, 400, 350, 280, 480, 320, 240, 440, 200]
    })
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ semantika/test_db_fields.csv
    test_dir = Path('semantika')
    test_dir.mkdir(exist_ok=True)
    
    test_file = test_dir / 'test_db_fields.csv'
    df.to_csv(test_file, index=False, encoding='utf-8-sig', sep=';')
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π CSV: {test_file}")
    print(f"   –ó–∞–ø—Ä–æ—Å–æ–≤: {len(test_queries)}")
    
    return test_file


async def run_test_analysis(test_file: Path):
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º —Ñ–∞–π–ª–µ"""
    print("\n" + "=" * 80)
    print("üöÄ –ó–ê–ü–£–°–ö –¢–ï–°–¢–û–í–û–ì–û –ê–ù–ê–õ–ò–ó–ê")
    print("=" * 80)
    
    # –°–æ–∑–¥–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    class Args:
        def __init__(self):
            self.input_file = str(test_file)
            self.group = 'test_db_fields'
            self.skip_embeddings = True
            self.skip_graph = True
            self.skip_topics = True
            self.skip_hierarchical = True
            self.skip_forms = True
            self.skip_yandex_direct = True
            self.xmlstock_api_key = None
            self.serp_batch_async = True
            self.enable_graph = False  # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            
            # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å API –∫–ª—é—á –∏–∑ config_local
            try:
                import config_local
                self.xmlstock_api_key = getattr(config_local, 'XMLSTOCK_API_KEY', None)
            except ImportError:
                pass
            
            # –ï—Å–ª–∏ –Ω–µ—Ç –≤ config_local - –ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
            if not self.xmlstock_api_key:
                self.xmlstock_api_key = os.getenv('XMLSTOCK_API_KEY')
    
    args = Args()
    
    # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    analyzer = SEOAnalyzer(args)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º pipeline
    try:
        await analyzer.run()
        print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        return True
    except Exception as e:
        print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")
        print("   –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –ë–î...")
        import traceback
        traceback.print_exc()
        
        # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤—Ä—É—á–Ω—É—é, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å –≤ DataFrame
        if hasattr(analyzer, 'df') and analyzer.df is not None and len(analyzer.df) > 0:
            print("\nüíæ –ü–æ–ø—ã—Ç–∫–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤—Ä—É—á–Ω—É—é...")
            try:
                from seo_analyzer.core.cache.master_query_db import MasterQueryDatabase
                master_db = MasterQueryDatabase()
                
                group_name = 'test_db_fields'
                csv_path = test_file
                
                master_db.save_queries(
                    group_name=group_name,
                    df=analyzer.df,
                    csv_path=csv_path,
                    csv_hash=None
                )
                print("‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤—Ä—É—á–Ω—É—é!")
            except Exception as save_error:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {save_error}")
        
        return True  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º True —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ë–î


def check_db_fields(group_name: str = 'test_db_fields'):
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–∞–ø–æ–ª–Ω—è–µ–º–æ—Å—Ç—å –ø–æ–ª–µ–π –≤ –ë–î"""
    print("\n" + "=" * 80)
    print("üìä –ü–†–û–í–ï–†–ö–ê –ó–ê–ü–û–õ–ù–Ø–ï–ú–û–°–¢–ò –ü–û–õ–ï–ô –í –ë–î")
    print("=" * 80)
    
    master_db = MasterQueryDatabase()
    db_path = master_db.db_path
    
    if not db_path.exists():
        print(f"‚ùå –ë–î –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {db_path}")
        return
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≥—Ä—É–ø–ø—ã
    cursor.execute('''
        SELECT COUNT(*) FROM master_queries WHERE group_name = ?
    ''', (group_name,))
    
    count = cursor.fetchone()[0]
    
    if count == 0:
        print(f"‚ùå –ì—Ä—É–ø–ø–∞ '{group_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ë–î")
        conn.close()
        return
    
    print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –≥—Ä—É–ø–ø–µ '{group_name}': {count}\n")
    
    # –°–ø–∏—Å–æ–∫ –ø–æ–ª–µ–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    fields_to_check = {
        '–ì–µ–æ–ª–æ–∫–∞—Ü–∏—è': [
            'geo_type',
            'geo_country',
            'geo_city',
        ],
        '–ò–Ω—Ç–µ–Ω—Ç': [
            'main_intent',
        ],
        'SERP –±–∞–∑–æ–≤—ã–µ': [
            'serp_req_id',
            'serp_status',
            'serp_found_docs',
            'serp_titles_with_keyword',
        ],
        'SERP offer info': [
            'serp_docs_with_offers',
            'serp_total_docs',
            'serp_offer_ratio',
            'serp_avg_price',
            'serp_min_price',
            'serp_max_price',
            'serp_median_price',
            'serp_offers_count',
            'serp_offers_with_discount',
            'serp_avg_discount_percent',
        ]
    }
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥–æ–µ –ø–æ–ª–µ
    results = {}
    
    for category, fields in fields_to_check.items():
        print(f"\nüìã {category}:")
        results[category] = {}
        
        for field in fields:
            cursor.execute(f'''
                SELECT 
                    COUNT(*) as total,
                    COUNT({field}) as filled,
                    COUNT(CASE WHEN {field} IS NOT NULL AND {field} != '' AND {field} != 0 THEN 1 END) as non_empty
                FROM master_queries
                WHERE group_name = ?
            ''', (group_name,))
            
            row = cursor.fetchone()
            total, filled, non_empty = row
            
            percentage = (non_empty / total * 100) if total > 0 else 0
            
            status = "‚úÖ" if non_empty > 0 else "‚ùå"
            results[category][field] = {
                'total': total,
                'filled': filled,
                'non_empty': non_empty,
                'percentage': percentage
            }
            
            print(f"  {status} {field:35} {non_empty:3}/{total:3} ({percentage:5.1f}%)")
    
    # –í—ã–≤–æ–¥–∏–º –ø—Ä–∏–º–µ—Ä—ã –∑–∞–ø–∏—Å–µ–π
    print("\n" + "=" * 80)
    print("üìù –ü–†–ò–ú–ï–†–´ –ó–ê–ü–ò–°–ï–ô:")
    print("=" * 80)
    
    cursor.execute('''
        SELECT 
            keyword,
            main_intent,
            geo_city,
            geo_country,
            serp_req_id,
            serp_status,
            serp_found_docs,
            serp_avg_price,
            serp_offer_ratio
        FROM master_queries
        WHERE group_name = ?
        LIMIT 5
    ''', (group_name,))
    
    rows = cursor.fetchall()
    
    if rows:
        print(f"\n{'–ó–∞–ø—Ä–æ—Å':<30} {'–ò–Ω—Ç–µ–Ω—Ç':<15} {'–ì–æ—Ä–æ–¥':<15} {'–°—Ç—Ä–∞–Ω–∞':<10} {'req_id':<12} {'–°—Ç–∞—Ç—É—Å':<12} {'–î–æ–∫–æ–≤':<8} {'–¶–µ–Ω–∞':<10} {'Offer%':<8}")
        print("-" * 130)
        
        for row in rows:
            keyword, intent, city, country, req_id, status, docs, price, ratio = row
            keyword = (keyword[:27] + '...') if keyword and len(keyword) > 30 else (keyword or '')
            intent = intent or ''
            city = city or ''
            country = country or ''
            req_id = (req_id[:10] + '..') if req_id and len(req_id) > 12 else (req_id or '')
            status = status or ''
            docs = str(docs) if docs else ''
            price = f"{price:.0f}" if price else ''
            ratio = f"{ratio:.1%}" if ratio else ''
            
            print(f"{keyword:<30} {intent:<15} {city:<15} {country:<10} {req_id:<12} {status:<12} {docs:<8} {price:<10} {ratio:<8}")
    
    conn.close()
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "=" * 80)
    print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print("=" * 80)
    
    total_fields = sum(len(fields) for fields in fields_to_check.values())
    filled_fields = sum(
        sum(1 for field_data in category_data.values() if field_data['non_empty'] > 0)
        for category_data in results.values()
    )
    
    print(f"\n‚úÖ –ó–∞–ø–æ–ª–Ω–µ–Ω–æ –ø–æ–ª–µ–π: {filled_fields}/{total_fields} ({filled_fields/total_fields*100:.1f}%)")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è
    critical_fields = ['main_intent', 'geo_city', 'serp_found_docs', 'serp_avg_price']
    critical_filled = sum(
        1 for category_data in results.values()
        for field, field_data in category_data.items()
        if field in critical_fields and field_data['non_empty'] > 0
    )
    
    print(f"‚úÖ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω—ã: {critical_filled}/{len(critical_fields)}")
    
    if critical_filled == len(critical_fields):
        print("\nüéâ –í–°–ï –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–û–õ–Ø –ó–ê–ü–û–õ–ù–ï–ù–´!")
    else:
        print(f"\n‚ö†Ô∏è  –ù–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–µ–π: {len(critical_fields) - critical_filled}")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 80)
    print("–¢–ï–°–¢ –ó–ê–ü–û–õ–ù–Ø–ï–ú–û–°–¢–ò –ü–û–õ–ï–ô –í –ë–î")
    print("=" * 80)
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π CSV
    test_file = create_test_csv(num_queries=10)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
    success = await run_test_analysis(test_file)
    
    if success:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–ø–æ–ª–Ω—è–µ–º–æ—Å—Ç—å –ø–æ–ª–µ–π
        check_db_fields('test_db_fields')
    else:
        print("\n‚ùå –ê–Ω–∞–ª–∏–∑ –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω, –ø—Ä–æ–≤–µ—Ä–∫–∞ –ë–î –ø—Ä–æ–ø—É—â–µ–Ω–∞")
    
    print("\n" + "=" * 80)
    print("–¢–ï–°–¢ –ó–ê–í–ï–†–®–ï–ù")
    print("=" * 80)


if __name__ == "__main__":
    asyncio.run(main())

