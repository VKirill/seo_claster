"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω–æ–≥–æ –∑–∞–º—ã–∫–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ–ø–∞–¥–∞–µ—Ç –ª–∏ "—Å–∫—É–¥ –æ–±–æ–∏" –≤ –∫–ª–∞—Å—Ç–µ—Ä —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω—ã–µ —Å–≤—è–∑–∏
"""

import json
import sqlite3
from pathlib import Path
from typing import List, Dict, Set

DB_PATH = Path("output/master_queries.db")
GROUP_NAME = "—Å–∫—É–¥"
MIN_COMMON_URLS = 7

QUERIES = [
    "—Å–∏—Å—Ç–µ–º–∞ —Å–∫—É–¥",
    "–∫–æ–Ω—Ç—Ä–æ–ª—å –¥–æ—Å—Ç—É–ø–∞ —Å–∫—É–¥",
    "—Å–∫—É–¥ —Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è",
    "—Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–æ—Å—Ç—É–ø–∞ —Å–∫—É–¥",
    "—Å–∫—É–¥ —Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–æ–º",
    "–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ —Å–∫—É–¥",
    "—Å–∫—É–¥ –≤ –æ—Ñ–∏—Å",
    "—Å–∫—É–¥ –æ—Ö—Ä–∞–Ω–Ω–∞—è —Å–∏–≥–Ω–∞–ª–∏–∑–∞—Ü–∏—è",
    "—Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Å–∫—É–¥",
    "—Å–∫—É–¥ –Ω–∞–∫–ª–∞–¥–Ω–æ–π",
    "—Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å–∫—É–¥",
    "—Å–∏—Å—Ç–µ–º–∞ —Å–∫—É–¥ –¥–ª—è –¥–≤–µ—Ä–µ–π",
    "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∫—É–¥",
    "–æ—Å–Ω–∞—â–µ–Ω–∏–µ —Å–∫—É–¥",
    "—Å–∏—Å—Ç–µ–º–∞ —Å–∫—É–¥ –≤ –æ—Ñ–∏—Å",
    "—Å–∫—É–¥ —Å–æ–≤–∞",
    "—Å—Ç–∞–Ω—Ü–∏—è —Å–∫—É–¥",
    "—Å–∏—Å—Ç–µ–º–∞ —É—á–µ—Ç–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è —Å–∫—É–¥",
    "—Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è —É—á–µ—Ç–∞ –¥–æ—Å—Ç—É–ø–∞ —Å–∫—É–¥",
    "—Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è —É–¥–∞–ª–µ–Ω–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ —Å–∫—É–¥",
    "—Å–∏—Å—Ç–µ–º—ã —Å–∫—É–¥ –¥–ª—è –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–π",
    "—Ä–æ—Å—Ç–∞–± —Å–∫—É–¥",
    "—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞ —Å–∫—É–¥",
    "—Ä–µ–¥ —Å–∫—É–¥",
    "—Å–∫—É–¥ –æ–±–æ–∏",
    "—Å–∫—É–¥ —Ä–∏–º",
]


def normalize_url(url: str) -> str:
    if not url:
        return ""
    url = url.replace("https://", "").replace("http://", "")
    url = url.replace("www.", "")
    url = url.split("?")[0].split("#")[0]
    return url.rstrip("/").lower()


def extract_urls_from_json(serp_top_urls_json: str) -> List[str]:
    if not serp_top_urls_json:
        return []
    try:
        data = json.loads(serp_top_urls_json)
        if isinstance(data, list):
            urls = []
            for item in data:
                if isinstance(item, dict):
                    url = item.get('url', '')
                elif isinstance(item, str):
                    url = item
                else:
                    continue
                if url:
                    urls.append(normalize_url(url))
            return urls
        return []
    except (json.JSONDecodeError, TypeError):
        return []


def get_query_urls(db_path: Path, group_name: str, query: str) -> List[str]:
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute('''
        SELECT serp_top_urls
        FROM master_queries
        WHERE group_name = ? AND keyword = ?
    ''', (group_name, query))
    row = cursor.fetchone()
    conn.close()
    if row and row[0]:
        return extract_urls_from_json(row[0])
    return []


def calculate_url_overlap(urls1: List[str], urls2: List[str], top_n: int = 20) -> int:
    set1 = set(urls1[:top_n])
    set2 = set(urls2[:top_n])
    return len(set1 & set2)


def build_similarity_graph(query_urls_dict: Dict[str, List[str]], min_common: int) -> Dict[str, Set[str]]:
    """–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
    graph = {query: set() for query in query_urls_dict.keys()}
    
    queries = list(query_urls_dict.keys())
    for i, query1 in enumerate(queries):
        for query2 in queries[i+1:]:
            overlap = calculate_url_overlap(
                query_urls_dict[query1],
                query_urls_dict[query2]
            )
            if overlap >= min_common:
                graph[query1].add(query2)
                graph[query2].add(query1)
    
    return graph


def find_connected_component(graph: Dict[str, Set[str]], start_query: str) -> Set[str]:
    """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å start_query —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω–æ–µ –∑–∞–º—ã–∫–∞–Ω–∏–µ"""
    visited = set()
    stack = [start_query]
    component = set()
    
    while stack:
        query = stack.pop()
        if query in visited:
            continue
        visited.add(query)
        component.add(query)
        
        for neighbor in graph.get(query, set()):
            if neighbor not in visited:
                stack.append(neighbor)
    
    return component


def main():
    print("=" * 80)
    print("–ü–†–û–í–ï–†–ö–ê –¢–†–ê–ù–ó–ò–¢–ò–í–ù–û–ì–û –ó–ê–ú–´–ö–ê–ù–ò–Ø –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò")
    print("=" * 80)
    print(f"\n–ü–æ—Ä–æ–≥: {MIN_COMMON_URLS} –æ–±—â–∏—Ö URL")
    print()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º URL
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ URL...")
    query_urls_dict = {}
    for query in QUERIES:
        urls = get_query_urls(DB_PATH, GROUP_NAME, query)
        if urls:
            query_urls_dict[query] = urls
    
    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(query_urls_dict)} –∑–∞–ø—Ä–æ—Å–æ–≤\n")
    
    # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ —Å–≤—è–∑–µ–π
    print("üîó –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å–≤—è–∑–µ–π...")
    graph = build_similarity_graph(query_urls_dict, MIN_COMMON_URLS)
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–≤—è–∑–∏
    total_edges = sum(len(neighbors) for neighbors in graph.values()) // 2
    print(f"‚úì –°–æ–∑–¥–∞–Ω–æ —Å–≤—è–∑–µ–π: {total_edges}")
    
    # –ù–∞—Ö–æ–¥–∏–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –¥–ª—è "—Å–∫—É–¥ –æ–±–æ–∏"
    target_query = "—Å–∫—É–¥ –æ–±–æ–∏"
    if target_query not in query_urls_dict:
        print(f"\n‚ùå –ó–∞–ø—Ä–æ—Å '{target_query}' –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    
    component = find_connected_component(graph, target_query)
    
    print(f"\n{'=' * 80}")
    print(f"–†–ï–ó–£–õ–¨–¢–ê–¢: –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –¥–ª—è '{target_query}'")
    print("=" * 80)
    print(f"\n‚úÖ –ó–∞–ø—Ä–æ—Å–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ: {len(component)}")
    print(f"\n–°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ:")
    for i, query in enumerate(sorted(component), 1):
        print(f"  {i}. {query}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤—Å–µ –ª–∏ –∑–∞–ø—Ä–æ—Å—ã –∏–∑ —Å–ø–∏—Å–∫–∞ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
    all_in_cluster = set(QUERIES).issubset(component)
    print(f"\n{'=' * 80}")
    print("–ê–ù–ê–õ–ò–ó")
    print("=" * 80)
    
    if all_in_cluster:
        print(f"\n‚úÖ –í–°–ï –∑–∞–ø—Ä–æ—Å—ã –∏–∑ —Å–ø–∏—Å–∫–∞ –ø–æ–ø–∞–¥–∞—é—Ç –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä!")
        print(f"   –≠—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—Ç, –ø–æ—á–µ–º—É '{target_query}' –ø–æ–ø–∞–ª –≤ –∫–ª–∞—Å—Ç–µ—Ä —Å–æ –≤—Å–µ–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏.")
    else:
        missing = set(QUERIES) - component
        print(f"\n‚ö†Ô∏è  –ù–ï –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ:")
        print(f"   –í –∫–ª–∞—Å—Ç–µ—Ä–µ: {len(component)} –∏–∑ {len(QUERIES)}")
        print(f"   –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç: {len(missing)}")
        for q in missing:
            print(f"     - {q}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä—è–º—ã–µ —Å–≤—è–∑–∏ "—Å–∫—É–¥ –æ–±–æ–∏"
    print(f"\n{'=' * 80}")
    print(f"–ü–†–Ø–ú–´–ï –°–í–Ø–ó–ò '{target_query}' (>= {MIN_COMMON_URLS} –æ–±—â–∏—Ö URL)")
    print("=" * 80)
    direct_links = graph.get(target_query, set())
    if direct_links:
        print(f"\n‚úÖ –ü—Ä—è–º—ã—Ö —Å–≤—è–∑–µ–π: {len(direct_links)}")
        for link in sorted(direct_links):
            overlap = calculate_url_overlap(
                query_urls_dict[target_query],
                query_urls_dict[link]
            )
            print(f"   ‚Ä¢ {link} ({overlap} –æ–±—â–∏—Ö URL)")
    else:
        print(f"\n‚ùå –ù–µ—Ç –ø—Ä—è–º—ã—Ö —Å–≤—è–∑–µ–π —Å –ø–æ—Ä–æ–≥–æ–º {MIN_COMMON_URLS}")


if __name__ == "__main__":
    main()

