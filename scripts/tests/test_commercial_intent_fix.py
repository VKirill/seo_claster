"""
–¢–µ—Å—Ç –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ–Ω—Ç–æ–≤

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∑–∞–ø—Ä–æ—Å—ã —Å –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ (–∫—É–ø–∏—Ç—å, –∑–∞–∫–∞–∑–∞—Ç—å, —Ü–µ–Ω–∞ –∏ —Ç.–¥.)
–ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –∫–∞–∫ commercial, –¥–∞–∂–µ –µ—Å–ª–∏ SERP –¥–∞–Ω–Ω—ã–µ –≥–æ–≤–æ—Ä—è—Ç –∏–Ω–∞—á–µ.
"""

import re
import pandas as pd
from pathlib import Path
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from seo_analyzer.classification.intent_classifier import IntentClassifier
from seo_analyzer.core.helpers import KEYWORD_GROUP_DIR, GEO_DICTIONARIES, KEYWORD_DICTIONARIES
from pathlib import Path


def load_text_file(file_path: Path) -> set:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å–æ —Å–ª–æ–≤–∞–º–∏"""
    if not file_path.exists():
        return set()
    
    with open(file_path, 'r', encoding='utf-8') as f:
        words = {line.strip().lower() for line in f if line.strip()}
    
    return words


def load_keyword_dicts() -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Å–ª–æ–≤–∞—Ä–∏ –∏–∑ keyword_group"""
    result = {}
    
    for dict_name, dict_info in KEYWORD_DICTIONARIES.items():
        file_path = KEYWORD_GROUP_DIR / dict_info["file"]
        words = load_text_file(file_path)
        result[dict_name] = {
            "words": words,
            "weight": dict_info["weight"],
            "flag": dict_info["flag"],
        }
    
    return result


def load_geo_dicts() -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —Å–ª–æ–≤–∞—Ä–∏"""
    result = {}
    
    for geo_name, filename in GEO_DICTIONARIES.items():
        file_path = KEYWORD_GROUP_DIR / filename
        result[geo_name] = load_text_file(file_path)
    
    return result


def test_commercial_intent_detection():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ–Ω—Ç–∞"""
    
    print("=" * 80)
    print("–¢–ï–°–¢: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ–Ω—Ç–∞")
    print("=" * 80)
    print()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏
    keyword_dicts = load_keyword_dicts()
    geo_dicts = load_geo_dicts()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    classifier = IntentClassifier(keyword_dicts, geo_dicts)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
    test_queries = [
        "—Å–∫—É–¥ –∫—É–ø–∏—Ç—å",
        "–∑–∞–∫–∞–∑–∞—Ç—å —Å–∫—É–¥",
        "—Å–∏—Å—Ç–µ–º–∞ —Å–∫—É–¥ –∫—É–ø–∏—Ç—å",
        "–ø—Ä–æ–¥–∞–∂–∞ —Å–∫—É–¥",
        "–∫–∞—Ä—Ç–∞ —Å–∫—É–¥ –∫—É–ø–∏—Ç—å",
        "–∫–∞—Ä—Ç–æ—á–∫–∏ –¥–æ—Å—Ç—É–ø–∞ —Å–∫—É–¥ –∫—É–ø–∏—Ç—å",
        "—Å–∫—É–¥ –∫—É–ø–∏—Ç—å –∫–æ–º–ø–ª–µ–∫—Ç",
        "—Å–∏—Å—Ç–µ–º–∞ —Å–∫—É–¥ –∫–æ–º–ø–ª–µ–∫—Ç –∫—É–ø–∏—Ç—å",
        "–∫—É–ø–∏—Ç—å —Å–∫—É–¥ —Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è",
        "–∫–æ–Ω—Ç—Ä–æ–ª—å –¥–æ—Å—Ç—É–ø–∞ —Å–∫—É–¥ –∫—É–ø–∏—Ç—å",
        "–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä —Å–∫—É–¥ –∫—É–ø–∏—Ç—å",
        "–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –¥–æ—Å—Ç—É–ø–∞ —Å–∫—É–¥ —Ü–µ–Ω–∞",
        "—Å–∫—É–¥ —Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫—É–ø–∏—Ç—å",
        "–ø—Ä–æ–¥–∞–∂–∞ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è —Å–∫—É–¥",
        "–∫–∞—Ä—Ç—ã –¥–ª—è —Å–∫—É–¥ –∫—É–ø–∏—Ç—å",
        "–∫–∞—Ä—Ç—ã –¥–æ—Å—Ç—É–ø–∞ –¥–ª—è —Å–∫—É–¥ –∫—É–ø–∏—Ç—å",
        "–∫—É–ø–∏—Ç—å —Å–∫—É–¥ —Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–æ–º",
        "—Å–∫—É–¥ —Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–æ–º —Ü–µ–Ω–∞",
        "–∑–∞–º–æ–∫ —Å–∫—É–¥ –∫—É–ø–∏—Ç—å",
        "—Å–∫—É–¥ –∑–∞–º–æ–∫ –Ω–∞ –¥–≤–µ—Ä—å –∫—É–ø–∏—Ç—å",
    ]
    
    print("–¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã:")
    print("-" * 80)
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ —Å–ª–æ–≤–∞
    commercial_keywords = keyword_dicts.get('commercial', {}).get('words', set())
    
    # –°–æ–∑–¥–∞–µ–º pattern –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ü–µ–ª—ã—Ö —Å–ª–æ–≤
    commercial_patterns = [
        re.compile(r'\b' + re.escape(word.lower()) + r'\b', re.IGNORECASE)
        for word in commercial_keywords
    ]
    
    results = []
    errors = []
    
    for query in test_queries:
        main_intent, scores, flags = classifier.classify_intent(query)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ —Å–ª–æ–≤–æ (–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞)
        has_commercial_word = any(pattern.search(query.lower()) for pattern in commercial_patterns)
        
        result = {
            'query': query,
            'intent': main_intent,
            'has_commercial_word': has_commercial_word,
            'commercial_score': scores.get('commercial', 0),
            'informational_score': scores.get('informational', 0),
        }
        results.append(result)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å
        is_commercial = main_intent in ['commercial', 'commercial_geo']
        
        if has_commercial_word and not is_commercial:
            errors.append(f"‚ùå '{query}' - –∏–Ω—Ç–µ–Ω—Ç {main_intent}, –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å commercial")
            print(f"‚ùå '{query}'")
            print(f"   –ò–Ω—Ç–µ–Ω—Ç: {main_intent} (–æ–∂–∏–¥–∞–µ—Ç—Å—è: commercial)")
            print(f"   Scores: commercial={scores.get('commercial', 0):.2f}, info={scores.get('informational', 0):.2f}")
        else:
            print(f"‚úÖ '{query}' ‚Üí {main_intent}")
    
    print()
    print("=" * 80)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print("=" * 80)
    
    total = len(test_queries)
    correct = total - len(errors)
    
    print(f"–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {total}")
    print(f"–ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö: {correct}")
    print(f"–û—à–∏–±–æ–∫: {len(errors)}")
    print()
    
    if errors:
        print("–û–®–ò–ë–ö–ò:")
        for error in errors:
            print(f"  {error}")
        print()
        return False
    else:
        print("‚úÖ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´!")
        print()
        return True


def test_commercial_word_matching():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤ (—Ü–µ–ª—ã–µ —Å–ª–æ–≤–∞ vs –ø–æ–¥—Å—Ç—Ä–æ–∫–∏)"""
    
    print("=" * 80)
    print("–¢–ï–°–¢: –ü–æ–∏—Å–∫ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤ (—Ü–µ–ª—ã–µ —Å–ª–æ–≤–∞ vs –ø–æ–¥—Å—Ç—Ä–æ–∫–∏)")
    print("=" * 80)
    print()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏
    keyword_dicts = load_keyword_dicts()
    commercial_keywords = keyword_dicts.get('commercial', {}).get('words', set())
    
    # –°–æ–∑–¥–∞–µ–º patterns
    commercial_patterns = [
        re.compile(r'\b' + re.escape(word.lower()) + r'\b', re.IGNORECASE)
        for word in commercial_keywords
    ]
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏
    test_cases = [
        ("–∫—É–ø–∏—Ç—å —Å–∫—É–¥", True, "–¥–æ–ª–∂–µ–Ω –Ω–∞–π—Ç–∏ '–∫—É–ø–∏—Ç—å'"),
        ("—Å–∫—É–ø–∏—Ç—å –º–µ—Ç–∞–ª–ª", False, "–ù–ï –¥–æ–ª–∂–µ–Ω –Ω–∞–π—Ç–∏ '–∫—É–ø–∏—Ç—å' –≤ '—Å–∫—É–ø–∏—Ç—å'"),
        ("–≤—ã–∫—É–ø –∞–≤—Ç–æ", False, "–ù–ï –¥–æ–ª–∂–µ–Ω –Ω–∞–π—Ç–∏ '–∫—É–ø–∏—Ç—å' –≤ '–≤—ã–∫—É–ø'"),
        ("—Ü–µ–Ω–∞ —Ç–æ–≤–∞—Ä–∞", True, "–¥–æ–ª–∂–µ–Ω –Ω–∞–π—Ç–∏ '—Ü–µ–Ω–∞'"),
        ("–æ—Ü–µ–Ω–∫–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏", False, "–ù–ï –¥–æ–ª–∂–µ–Ω –Ω–∞–π—Ç–∏ '—Ü–µ–Ω–∞' –≤ '–æ—Ü–µ–Ω–∫–∞'"),
        ("–∑–∞–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç–∞–≤–∫—É", True, "–¥–æ–ª–∂–µ–Ω –Ω–∞–π—Ç–∏ '–∑–∞–∫–∞–∑–∞—Ç—å'"),
        ("–ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä", False, "–ù–ï –¥–æ–ª–∂–µ–Ω –Ω–∞–π—Ç–∏ '–∑–∞–∫–∞–∑–∞—Ç—å' –≤ '–ø–æ–∫–∞–∑–∞—Ç—å'"),
    ]
    
    print("–ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ —Å–ª–æ–≤:")
    print("-" * 80)
    
    errors = []
    
    for query, should_match, description in test_cases:
        has_match = any(pattern.search(query.lower()) for pattern in commercial_patterns)
        
        if has_match == should_match:
            print(f"‚úÖ '{query}' - {description}")
        else:
            error = f"‚ùå '{query}' - {description} (–ø–æ–ª—É—á–∏–ª–∏: {has_match})"
            print(error)
            errors.append(error)
    
    print()
    print("=" * 80)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print("=" * 80)
    
    total = len(test_cases)
    correct = total - len(errors)
    
    print(f"–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {total}")
    print(f"–ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö: {correct}")
    print(f"–û—à–∏–±–æ–∫: {len(errors)}")
    print()
    
    if errors:
        print("–û–®–ò–ë–ö–ò:")
        for error in errors:
            print(f"  {error}")
        print()
        return False
    else:
        print("‚úÖ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´!")
        print()
        return True


if __name__ == "__main__":
    print()
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ö–û–ú–ú–ï–†–ß–ï–°–ö–ò–• –ò–ù–¢–ï–ù–¢–û–í")
    print()
    
    # –¢–µ—Å—Ç 1: –ü–æ–∏—Å–∫ —Å–ª–æ–≤
    test1_passed = test_commercial_word_matching()
    
    # –¢–µ—Å—Ç 2: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Ç–æ–≤
    test2_passed = test_commercial_intent_detection()
    
    # –ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    print()
    print("=" * 80)
    print("–ò–¢–û–ì–û–í–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢")
    print("=" * 80)
    
    if test1_passed and test2_passed:
        print("‚úÖ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û!")
        sys.exit(0)
    else:
        print("‚ùå –ù–ï–ö–û–¢–û–†–´–ï –¢–ï–°–¢–´ –ù–ï –ü–†–û–®–õ–ò")
        if not test1_passed:
            print("  - –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤: –ü–†–û–í–ê–õ–ï–ù")
        if not test2_passed:
            print("  - –¢–µ—Å—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–Ω—Ç–µ–Ω—Ç–æ–≤: –ü–†–û–í–ê–õ–ï–ù")
        sys.exit(1)

