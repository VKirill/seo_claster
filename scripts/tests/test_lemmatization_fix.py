"""
–¢–µ—Å—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ normalized –∏ lemmatized
"""

import pandas as pd
from seo_analyzer.core.normalizer import QueryNormalizer


def test_normalizer():
    """–¢–µ—Å—Ç –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    
    print("=" * 80)
    print("–¢–ï–°–¢: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ normalized –∏ lemmatized")
    print("=" * 80)
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π DataFrame –±–µ–∑ –∫–æ–ª–æ–Ω–æ–∫
    test_queries = [
        "—Å–∫—É–¥ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –∫—É–ø–∏—Ç—å",
        "—Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–æ—Å—Ç—É–ø–∞ —Ü–µ–Ω–∞",
        "—Ç—É—Ä–Ω–∏–∫–µ—Ç –¥–ª—è –æ—Ñ–∏—Å–∞",
        "—Å—á–∏—Ç—ã–≤–∞—Ç–µ–ª—å –∫–∞—Ä—Ç rfid",
        "–¥–æ–º–æ—Ñ–æ–Ω —Å –∫–∞–º–µ—Ä–æ–π"
    ]
    
    df = pd.DataFrame({
        'keyword': test_queries,
        'frequency_world': [100, 200, 150, 80, 300],
        'frequency_exact': [50, 100, 75, 40, 150]
    })
    
    print(f"\n1. –ò—Å—Ö–æ–¥–Ω—ã–π DataFrame:")
    print(f"   –ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
    print(f"   –ó–∞–ø—Ä–æ—Å–æ–≤: {len(df)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–æ–ª–æ–Ω–æ–∫ –Ω–µ—Ç
    assert 'normalized' not in df.columns, "–ö–æ–ª–æ–Ω–∫–∞ normalized —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!"
    assert 'lemmatized' not in df.columns, "–ö–æ–ª–æ–Ω–∫–∞ lemmatized —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!"
    print("   ‚úì –ö–æ–ª–æ–Ω–∫–∏ normalized –∏ lemmatized –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ —á–µ—Ä–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä
    print(f"\n2. –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ —á–µ—Ä–µ–∑ QueryNormalizer...")
    normalizer = QueryNormalizer()
    normalized_results = normalizer.normalize_batch(df['keyword'].tolist())
    
    df['normalized'] = [r['normalized'] for r in normalized_results]
    df['lemmatized'] = [r['lemmatized'] for r in normalized_results]
    
    print(f"   ‚úì –ö–æ–ª–æ–Ω–∫–∏ —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ")
    print(f"   –ö–æ–ª–æ–Ω–∫–∏ —Ç–µ–ø–µ—Ä—å: {list(df.columns)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∏ —Å–æ–∑–¥–∞–Ω—ã
    assert 'normalized' in df.columns, "–ö–æ–ª–æ–Ω–∫–∞ normalized –Ω–µ —Å–æ–∑–¥–∞–Ω–∞!"
    assert 'lemmatized' in df.columns, "–ö–æ–ª–æ–Ω–∫–∞ lemmatized –Ω–µ —Å–æ–∑–¥–∞–Ω–∞!"
    assert len(df['normalized']) == len(df), "–†–∞–∑–º–µ—Ä –∫–æ–ª–æ–Ω–∫–∏ normalized –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç!"
    assert len(df['lemmatized']) == len(df), "–†–∞–∑–º–µ—Ä –∫–æ–ª–æ–Ω–∫–∏ lemmatized –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç!"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã
    print(f"\n3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
    for idx, row in df.head(3).iterrows():
        print(f"\n   –û—Ä–∏–≥–∏–Ω–∞–ª:     '{row['keyword']}'")
        print(f"   Normalized:   '{row['normalized']}'")
        print(f"   Lemmatized:   '{row['lemmatized']}'")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
    assert df['normalized'].iloc[0] != "", "Normalized –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—É—Å—Ç—ã–º!"
    assert df['lemmatized'].iloc[0] != "", "Lemmatized –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—É—Å—Ç—ã–º!"
    
    print(f"\n{'=' * 80}")
    print("‚úÖ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û!")
    print("=" * 80)
    print("\nüìù –í—ã–≤–æ–¥:")
    print("   ‚Ä¢ QueryNormalizer —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    print("   ‚Ä¢ –ö–æ–ª–æ–Ω–∫–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
    print("   ‚Ä¢ –î–∞–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ")
    print("   ‚Ä¢ –ü—Ä–æ–±–ª–µ–º–∞ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏ —Ä–µ—à–µ–Ω–∞!")
    print()


if __name__ == '__main__':
    try:
        test_normalizer()
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()

